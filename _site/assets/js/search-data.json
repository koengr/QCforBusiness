{"0": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Introduction to Quantum Computing for Business",
    "content": "Introduction to Quantum Computing for Business . This open-access book contains everything you should know about quantum computers, without going into tedious technical details. It answers questions such as: . | What are the applications of quantum computers and quantum networks? . | In what year will we have large-scale quantum computers? . | What are the consequences for cybersecurity ? . | What strategic actions should organisations take? . | What is the status of todayâ€™s hardware? . | . As the first generation of quantum computers is on the horizon, understanding their impact is more important than ever. Luckily, you donâ€™t need a physics degree to understand their functionality - just like you donâ€™t need to know how a transistor works to thrive in conventional IT. This book, written by Koen Groenland, is a gentle and business-oriented introduction to the opportunities and threats of quantum technologies. It equips you with the necessary knowledge to join cutting-edge discussions and to make strategic decisions. Start reading ðŸ“– Prefer to read a printed version? A paperback edition will be released in Q1 2025. ",
    "url": "/",
    
    "relUrl": "/"
  },"1": {
    "doc": "The applications: what problems will we solve?",
    "title": "The applications: what problems will we solve with quantum computers?",
    "content": "Reading time: 27 minutes . Contents . | What applications offer a quantum speedup? | How can we compare different types of speedups?Â  | Where is the killer application? | Further reading | . At a glance The most important application areas are . | the simulation of chemistry and materials, . | cracking cryptography, . | using quantum networks to distribute cryptographic keys, and . | solving large-scale optimisation and AI problems.Â  . | . Getting utility out of a quantum computer is not straightforward. It requires an algorithm that beats all other known methods (even those that run on very fast classical computers), and it must tackle a problem with real-world relevance. Especially in optimisation and AI, we have not found a convincing â€˜killer applicationâ€™ yet. In the previous chapter, we saw that quantum computers have relatively low clock speeds, but they happen to solve some problems more efficiently, that is, in fewer steps. Therefore, the most important question in this field is: what advantage do quantum computers have on which problems? . The Quantum Algorithm Zoo1 lists pretty much all known quantum algorithms. It has grown into an impressive list that cites over 400 papers. Unfortunately, upon closer inspection, itâ€™s hard to extract precisely the useful business applications, for a few reasons. Some algorithms solve highly artificial problems for which no real business use cases are known. Others may make unrealistic assumptions or may only offer a speedup when a problem grows to outrageously large problems (that we never encounter in the real world). Nevertheless, itâ€™s definitely recommended to scroll through. For this book, we take a different approach. We focus specifically on algorithms with plausible business applications. To assess their advantage, we split our main question into two parts: . | What applications offer a quantum speedup? . | How large is this speedup in practice? . | . ",
    "url": "/essentials/applications-overview/#the-applications-what-problems-will-we-solve-with-quantum-computers",
    
    "relUrl": "/essentials/applications-overview/#the-applications-what-problems-will-we-solve-with-quantum-computers"
  },"2": {
    "doc": "The applications: what problems will we solve?",
    "title": "What applications offer a quantum speedup?",
    "content": "We foresee four major families of use cases where quantum computing can make a real impact on society. We briefly discuss each of them here and link to a later chapter that discusses each application in more depth.Â  . 1. Simulation of other quantum systems: molecules, materials, and chemical processes . Most materials can be accurately simulated on classical computers. However, in some specific situations, the locations of atoms and electrons become notoriously hard to describe, sometimes requiring quantum mechanics to make useful predictions. Such problems are the prototypical examples of where a quantum computer can offer a great advantage. Realistic applications could be in designing new chemical processes (leading to cheaper and more energy-efficient factories), estimating the effects of new medicine, or working towards materials with desirable properties (like superconductors or semiconductors). Of course, scientists will also be excited to simulate the physics that occur in exotic circumstances, like at the Large Hadron Collider or in black holes. Simulation is, however, not a silver bullet, and quantum computers will not be spitting out recipes for new pharmaceuticals by themselves. Breakthroughs in chemistry and material science will still require a mix of theory, lab testing, computation, and, most of all, the hard work of smart scientists and engineers. From this perspective, quantum computers have the potential to become a valued new tool for R&amp;D departments. Read more: What are the main applications in chemistry and material science? . See also: . | Startup PhaseCraft studies the famous Fermi-Hubbard model using a quantum computer . | Startup Zapata reduces the runtime and error rate of famous chemistry algorithm . | IBM and Daimler research next-gen batteries . | Roche started a project to find medicines for Alzheimerâ€™s . | An overview of various simulation software packages for quantum computers . | . 2. Cracking a certain type of cryptography . The security of todayâ€™s internet communication relies heavily on a cryptographic protocol invented by Rivest, Shamir and Adleman (RSA) in the late 70s. The protocol helps distribute secret encryption keys (so that nobody else can read messages in transit) and guarantees the origin of files and webpages (so that you know that the latest Windows update actually came from Microsoft, and not from some evil cybercriminal). RSA works thanks to an ingenious mathematical trick: honest users can set up their encryption using relatively few computational steps, whereas â€˜spyingâ€™ on others would require one to solve an extremely hard problem. For the RSA cryptosystem, that problem isÂ prime factorisation,Â where the goal is to decompose a very large number (for illustration purposes, letâ€™s think of 15) into its prime factors (here: 3 and 5). As far as we know, for sufficiently large numbers, this task takes such an incredibly long time that nobody would ever succeed in breaking a relevant code â€“ at least on a classical computer. This all changed in 1994 when computer scientist Peter Shor discovered that quantum computers happen to be quite good at factoring. The quantum algorithm by Shor can crack RSA (and also its cousin calledÂ elliptic curve cryptography, abbreviated ECC) in a relatively efficient way using a quantum computer. To be more concrete, according toÂ a recentÂ paper, a plausible quantum computer could factor the required 2048-bit number in roughly 8 hours (and using approximately 20 million imperfect qubits). Note that future breakthroughs will likely further reduce the stated time and qubit requirements. Luckily, not all cryptography is broken as easily by a quantum computer. RSA and ECC fall in the category ofÂ public key cryptography,Â which delivers a certain range of functionalities. A different class of protocols isÂ symmetric key cryptography,Â which is reasonably safe against quantum computers but doesnâ€™t provide the same rich functionality asÂ public keyÂ crypto. The most sensible approach is replacing RSA and ECC with so-calledÂ post-quantum cryptographyÂ (PQC): public key cryptosystems resilient to attackers with a large-scale quantum computer. Interestingly, PQC doesÂ notÂ require honest users (thatâ€™s you) to have a quantum computer: it will work perfectly fine on todayâ€™s PCs, laptops and servers. Read more: How will quantum computers impact cybersecurity?Â  . See also: . | (Youtube, technical) MinutePhysics explains Shorâ€™s algorithm.Â  . | Nature feature article:Â The race to save the Internet from quantum hackers . | . At the time of writing, a complex migration lies ahead of pretty much every large organisation in the world, which comes in addition to many existing cybersecurity threats. The foundations have been laid: thanks to the American National Institute of Standards and Technology (NIST), cryptographers from around the globe came together to select the best quantum-safe alternatives, culminating in the publication of the first standards in August 2024. These are the new algorithms that the vast majority of users will adopt. Unfortunately, many governments and enterprises run a great amount of legacy software that is hard to update, making this a complex IT migration that can easily take 5-15 years, depending on the organisation. Thereâ€™s a serious threat that quantum computers will be able to run Shorâ€™s algorithm within such a timeframe, so organisations are encouraged to start migrating as early as possible.Â  . A new type of cryptography comes with its own additional risks: the new standards have not yet been tested as thoroughly as the nearly 50-year-old RSA algorithm. Ideally, new implementations will beÂ hybrid, meaning that they combine the security of a conventional and a post-quantum algorithm. On top of that, organisations are encouraged to adoptÂ cryptographic agility, meaning that cryptosystems can be easily changed or updated if the need arises.Â  . Read more: What steps should your organisation take? . 3. Quantum Key Distribution to strengthen cryptography . Out of all the applications for quantum networks, Quantum Key Distribution (QKD) is the one to watch. It allows two parties to generate secure cryptographic keys together, which can then be used for everyday needs like encryption and authentication. It requires a quantum network connection that transports photons in fragile quantum states. Such connections can currently reach a few hundred kilometres, and there is a clear roadmap to expand to a much wider internet. The most likely usage will be as an â€˜add-onâ€™ for high-security purposes (such as military communication or data exchange between data centres), in addition to standard post-quantum cryptography.Â  . Unfortunately, we often see media articles suggesting that QKD is a solution to the threat of Shorâ€™s algorithm and that it would form an â€˜unbreakable internetâ€™. Both claims are highly inaccurate. Firstly, QKD does not offer the wide range of functionality that public key cryptography offers, so it is not a complete replacement for the cryptosystems broken by Shor. Secondly, there will almost certainly be ways to hack a QKD system (just like with any other security system). Then why bother with QKD? The advantage of QKD is based on one important selling point: contrary to most other forms of cryptography, it does not rely on assumptions about the computational power of a hacker. This can be an essential factor when someone is highly paranoid about their cryptography or when data has to remain confidential for an extremely long period of time.Â  . At this time, pretty much every national security agency discourages the use of QKD simply because the available products are far from mature (and because PQC should be prioritised). It is unclear how successful QKD could be in the futureâ€”we will discuss this in-depth in another chapter. We firmly warn that other security products with the word â€˜quantumâ€™ in the name do not necessarily offer protection against Shorâ€™s algorithm. In particular, quantum random number generators (QRNGs) are sometimes promoted as a saviour against the quantum threat, which is nonsense. These devices serve a completely different purpose: they compete with existing hardware to generate unpredictable secret keys, which find a use (for example) inÂ hardware security modulesÂ in data centres.Â  . Read more: What are the use cases of quantum networks? . See also:Â  . | A short video explainer about how QKD works. | The NCSC states that it â€˜does not endorse the use of QKD for any government or military applicationsâ€™ . | The French ANSSI, German BSI, Dutch NLNCSA and Swedish SNCSA published a critical position paper on QKD in 2024.Â  . | Samsung builds QRNGs into certain phones on the South Korean market.Â  . | . 4. Optimisation and machine-learning . This is the part where most enterprises get excited. Can we combine the success of artificial intelligence (AI) and machine learning with the radically new capabilities of quantum computers? Can we create a superpowered version of ChatGPT or DALL-E, or at least speed up the demanding training process? . In this section, weâ€™ll take a closer look at the known applications for quantum computers on â€˜non-quantum problemsâ€™ other than cryptography. We focus specifically on the harder optimisation problems that currently take up large amounts of classical resources. Under the hood, all such applications are based on concrete mathematical problems such as binary optimisation, differential equations, classification, optimal planning, and so forth. For conciseness, we will use the word â€˜optimisationâ€™ as a catch-all term for all these problems, including things like machine learning and AI. Unfortunately, the amount of value that â€˜quantumâ€™ can add to optimisation tasks is very much a disputed topic. The situation here is very subtle: there exist many promising quantum algorithms, but as weâ€™ll see, each comes with important caveats that might limit their practical usefulness. To start, we can classify the known algorithms into the following three categories. Rigorous but slow algorithms . Many quantum optimisation algorithms have a well-provenÂ quantum speedup:Â there is no dispute that these requireÂ fewerÂ computational stepsÂ than any classical algorithm. For instance, a famous quantum algorithm invented byÂ Lov GroverÂ (with extensions byÂ DÃ¼rr and HÃ¸yer) finds the maximum of a function in fewer stepsÂ than conventional brute-force search. Similarly, quantum speedups were found for popular computational methods such asÂ backtracking,Â gradient descent, linear programming,Â lasso, andÂ for solving differential equations.Â  . The main question is whether this also means that the quantum computer requires lessÂ time! All of the above optimisation algorithms offer a so-calledÂ polynomial speedupÂ (in the case of Grover, this is sometimes further specified to be aÂ quadratic speedup). As we will soon see, it is not entirely clear if these speedups are enough to compensate for the slowness of a realistic quantum computer â€“ at least in the foreseeable future.Â  . Heuristic algorithms . Some algorithms claim much larger speedups, but there is no undisputed evidence to back this up. Often, these algorithms are tested on small datasets using the limited quantum computers available today â€“ which are still so tiny that not much can be concluded about larger-scale problems. Nonetheless, these â€˜high risk, high rewardâ€™ approaches typically make the bold claims that receive media attention. The most noteworthy variants are: . | Variational quantum circuitsÂ (VQC) are relatively short quantum programs that a classical computer can incrementally change. In jargon, these are quantum circuits that rely on a set of free parameters. The classical computer will run these programs many times, trying different parameters until the quantum program behaves as desired (for example, it might output very good train schedules or accurately describe a complex molecule). The philosophy is that we squeeze as much as possible out of small quantum computers with short-lived qubits: the (fast) classical computer takes care of most of the computation, whereas the quantum computer runs just long enough to sprinkle some quantum magic into the solution. Although its usefulness is disputed, this algorithm is highly flexible, leading to quantum variants of classifiers, neural networks, and support vector machines. Variants of this algorithm may be found under different names, such as Quantum Approximate optimisation Algorithm (QAOA), Variational Quantum Eigensolver (VQE), and quantum neural networks.Â  . | Quantum annealingÂ solves a particular subclass of optimisation problems. Instead of using the conventional â€˜quantum gatesâ€™, it uses the native physical forces that act on a set of qubits in a more analog way. Annealing itself is a mature classical algorithm. The advantage of a â€˜quantumâ€™ approach is not immediately apparent, although there are claims that hard-to-find solutions are more easily reached thanks to â€˜quantum fluctuationsâ€™ or â€˜tunnellingâ€™. Quantum annealing was popularised by the Canadian companyÂ D-Wave, which builds dedicated hardware with up to 5000 qubits and offers a cloud service that handles relatively large optimisation problems.Â  . | . Fast algorithms in search of a use case . Finally, there exist algorithms with large speedups, for which we are still looking for applications with any scientific or economic relevance. These are classic cases of solutions in search of a problem. The most notable example is the quantum algorithm that solves systems of linear equations2 with an exponential advantage. This problem is ubiquitous in engineering and optimisation, but unfortunately, there are so many caveats that no convincing practical uses have been found3. Recently, much attention has gone to the algorithm for topological data analysisÂ (a method to assess certain global features of a dataset), which promises an exponential advantage under certain assumptions. Again, scientists are still searching for a convincing application. Similarly, a quantum version of a classical machine learning algorithm called Support Vector Machines was found to have an exponential advantage over classical methods4. Unfortunately, this only works with a very specific dataset based on the factoring problem that Shorâ€™s algorithm is well known for. No rigorous advantage is known for more general datasets. A fourth class: quantum-inspired algorithms . Some impressive speedups that were recently found have been â€˜dequantisedâ€™: these algorithms were found to work on classical computers too! Thereâ€™s a beautiful story behind this process, where Ewin Tang, an undergraduate student at the time, made one of the most unexpected algorithmic breakthroughs of the decade. A great report by Robert Davis can be found on Medium5.Â  . Whatâ€™s left? . Unfortunately, a quantum optimisation algorithm with undisputed economic value does not yet exist; all of them come with serious caveats. This perspective is perhaps a bit disappointing, especially in a context where quantum computing is often presented as a disruptive innovation. Our main takeaway is that quantum optimisation (especially quantum machine learning!) is rather over-hyped.Â  . That doesnâ€™t mean that thereâ€™s no hope for quantum optimisation. Firstly, there are good reasons to believe that new algorithms and applications will be found. Secondly, the usefulness of the â€˜slowerâ€™ quantum optimisation algorithms ultimately depends on the speed of a future quantum computer compared to the speed of a future classical computer. To better understand the differences in computational speeds, we will need to quantify the amount of â€˜quantum advantageâ€™ that different algorithms have. Read more: Optimisation and AI: what are companies doing today? . Â  . ",
    "url": "/essentials/applications-overview/#what-applications-offer-a-quantum-speedup",
    
    "relUrl": "/essentials/applications-overview/#what-applications-offer-a-quantum-speedup"
  },"3": {
    "doc": "The applications: what problems will we solve?",
    "title": "How can we compare different types of speedups?Â ",
    "content": "When looking at the applications of quantum computers, one should always keep in mind: Are these actual improvements over our current state-of-the-art? Anyone can claim that their algorithmÂ canÂ solve a problem, but what we really care about is whether it solves itÂ faster. Classical computers are already extremely fast, so quantum algorithms should offer a substantial speedup before they become competitive.Â  . The fairest way to compare algorithms is by running them on actual hardware in a setting similar to how you would use the algorithm in practice. In the future, we expect such benchmarks to be the main tool to compare quantum and classical approaches. However, mature quantum hardware is not available yet, so we resort to a more theoretical comparison tool: the asymptotic runtime of an algorithm. What does asymptotic runtime mean? An important figure of merit of an algorithm is its so-called asymptotic complexity, which describes how much longer a computation takes as the problem becomes â€˜biggerâ€™ or more complicated. The term â€˜asymptoticâ€™ refers to the problemâ€™s size, which gets (asymptotically) larger, theoretically all the way to infinity. Size turns out to be a very relevant parameter. For example, computing 54 x 12 is much easier than 231.423 x 971.321, even though in technical jargon, they are instances of the same problem of â€˜multiplicationâ€™, and weâ€™d use the very sameÂ long multiplication algorithm that we learned in elementary school to tackle them. Similarly, creating a work schedule for a team of 5 is simpler than dealing with 10,000 employees. We typically use the letter \\(n\\) to denote the problem size. You can see \\(n\\) as the number of digits in a multiplication (like 2 or 6 above) or the number of employees involved in a schedule.Â  . For some very hard problems, the time to solution takes the form of an exponential, something like \\(T\\ \\sim\\ 2^{n}\\) or \\(T\\ \\sim\\ 10^{n}\\), where \\(T\\) is the number of steps (or time) taken6. Exponential scaling is typically a bad thing, as such functionsss becomes incredibly large even for moderate values of \\(n\\). For example, brute-force guessing a pin code of \\(n\\) digits takes roughly \\(T\\ \\sim\\ 10^{n}\\). There are also problems for which the number of steps scales like a polynomial, such as \\(T\\ \\sim\\ n^{3}\\) or \\(T\\ \\sim\\ n\\). Polynomials grow much slower than exponentials, allowing use to solve large problems in a reasonable amount of time. Whenever a new algorithm can bring an exponential scaling down to a polynomial, we may call this an â€˜exponential speedupâ€™. Such speedups are a computer scientistâ€™s dream because they have a tremendous impact on practical runtimes.Â For example, quantum computers can factor large numbers in time roughly \\(T\\ \\sim\\ n^{3}\\) (thanks toÂ Shorâ€™s algorithm7), whereas the best classical algorithm requires close to exponentially many steps8. Often, we deal with â€˜merelyâ€™ aÂ polynomial speedup, which happens when we obtain a smaller polynomial (for example, going from \\(T\\ \\sim\\ n^{2}\\)Â towards \\(T\\ \\sim\\ n\\) or perhaps even a â€˜smallerâ€™ exponential function (like \\(T\\ \\sim\\ 2^{n}\\)Â towards \\(T\\ \\sim\\ 2^{n/2}\\)). Reducing the exponent by a factor of two (like \\(n^{2}\\ \\rightarrow n\\)) is also sometimes called aÂ quadratic speedup, which is precisely what Groverâ€™s algorithm gives us. See also: . | At a more coarse level, we can define differentÂ complexity classes like P, NP and BQP.Â  | . Here is a rough overview of quantum speedups as we understand them today, categorised by their type of asymptotic speedup: . Â  . ðŸŸ¢Â  Â The â€˜exponentialâ€™ box is the most interesting one, featuring applications where quantum computers seem to have a groundbreaking advantage over classical computers. It containsÂ Shorâ€™s algorithmÂ for factoring, explaining the towering advantage that quantum computers have in codebreaking. We also believe it contains some applications inÂ chemistry and material science, especially those relating to dynamics (studying how molecules and materials change over time).Â  . ðŸŸ¡Â  Â TheÂ â€™polynomialâ€™Â box is still interesting, but its applicability is unclear. Recall that a quantum computer would need much more timeÂ per stepÂ â€“ and on top of that, it will have considerable overhead due toÂ error correction. Does a polynomial reduction in the number of steps overcome this slowness? According to aÂ recent paper, small polynomial speedups (as achieved byÂ Groverâ€™s algorithm) will not cut it, at least not in the foreseeable future.Â  . ðŸ”´Â  Â For some computations, a quantum computer offersÂ no speedup.Â Examples include sorting a list or loading large amounts of data.Â  . If this were the complete story, then most people would agree that quantum computing is a bit disappointing. It would be a niche product for hackers and a tiny community of physicists and chemists who study quantum mechanics itself.Â  . âšªÂ  Â Luckily, there is yet another category: many of the most exciting claims come from theÂ heuristicÂ algorithms. This term is used when an algorithm might give a suboptimal solution (which could still be useful) or when we cannot rigorously quantify the runtime. Such algorithms are quite common on classical computers: neural networks fall in this category, and these caused a significant revolution in AI. Unfortunately, it is unclear what the impact of currently known heuristic quantum algorithms will be.Â  . In summary, the potential for a useful quantum speedup varies greatly across applications, and so does their range of applicability. The case of factoring has a clear and convincing speedup but provides little value except to those with malicious intent. In contrast, machine learning and optimisation do tackle a broad palette of relevant problems, but the speed advantage of a quantum computer remains uncertain in this field. The applications of chemistry and material science fall somewhere in the middle, with some relevant areas ofÂ applicability and concrete indications of a practical speed advantage. ",
    "url": "/essentials/applications-overview/#how-can-we-compare-different-types-of-speedups",
    
    "relUrl": "/essentials/applications-overview/#how-can-we-compare-different-types-of-speedups"
  },"4": {
    "doc": "The applications: what problems will we solve?",
    "title": "Where is the killer application?",
    "content": "Is there hope that weâ€™ll find new quantum algorithms with a large commercial or societal value? For a quantum algorithm to be truly impactful, we require two properties:Â  . | [Useful] The algorithm solves a problem with real-world significance (for example, because organisations can work more efficiently or because it helps answer a scientific question). | [Better/faster] Using this particular algorithm is the most sensible* choice from a technical perspective**, even when compared to all other possible methods. | . Iâ€™d propose the term quantum utility for a situation where both properties are convincingly satisfied. The precise definition can be a bit finicky, so before we start searching for utility, we need to get some technical details out of the way. * What is â€˜sensibleâ€™ (2) depends strongly on the context of the real-world problem (1). In most cases, we care about how fast a problem is solved, but one should also take into account the total cost of developing the software, the cost of leasing the hardware, the energy consumption, the probability of errors, and so forth. For example, a high-frequency trader might be happy with a 2% faster algorithm even if the costs are sky-high and thereâ€™s a decent chance of failure, whereas a hospital could dismiss a 200x faster quantum approach if the costs donâ€™t outweigh the benefits. Indeed, what is â€˜sensibleâ€™ is highly subjective. In practice, we can relax this requirement somewhat and focus primarily on speed, which is a sufficiently complex figure of merit. Ideally, the quantum algorithm should enjoy anÂ exponentialÂ speedup or at least a large polynomial speedup.Â  . ** We explicitly look for technical perspectives. Otherwise, one might also say that using a quantum algorithm is commercially the best option because it creates good PR or because it keeps the workforce excited. Then perhaps the first utility has already been reached! However, this is not the computational revolution that weâ€™re looking for, so I explicitly exclude such non-technical reasons in property (2). Similarly, I donâ€™t want to worry too much about legal issues (â€˜it doesnâ€™t comply with regulationsâ€™) because it feels somewhat artificial to dismiss a quantum algorithm for such reasons. Supremacy, advantage, utility . Around 2019 and 2020, the terms quantum supremacy and quantum advantage were popularly used when quantum computers did, for the first time, beat the best supercomputers in terms of speed (property 2)9 10. This involved an algorithm that was cherry-picked to perform well on a relatively small and noisy quantum computer whilst being as challenging as possible for a conventional supercomputer. Quantum advantage was mostly a man-on-the-moon-type scientific achievement, showcasing the rapid progress in hardware engineering and silencing the sceptics who still thought quantum computing wouldnâ€™t work. There was no attempt to have any practical value (1). As a natural next step, the race is on to be the first to run something useful whilst leaving classical supercomputers in the dust. This led IBM to coin the term quantum utility11, which we adapted above. In the following years, we can expect the leading hardware and software manufacturers to maximise the amount of â€˜utilityâ€™ that they could possibly squeeze out of medium-sized quantum computers, whilst competitors will use their best classical simulations to dispute these claims. The first battles have already been fought: in June 2023, IBM claimed to simulate certain material science models better than classically possible12, quickly followed by two scientific responses that showed how easy it was to simulate the same experiment on a laptop13 14. It seems to me that healthy competition is good for the field overall. It should lead to increasingly convincing and rigorous quantum utility, from which the end-users will eventually profit! . In parallel, there is a rapidly expanding number of press releases by startups and enterprises that claim to create business value by solving industrial problems on todayâ€™s hardware, often without sharing many details. These approaches typically start with a relevant problem in mind and hence score well on usefulness (1). However, it is questionable whether quantum algorithms were indeed the best option (2), and most reports Iâ€™ve seen hardly bother to show any argumentation in this direction. Such claims should only be taken seriously if a rigorous benchmark against state-of-the-art classical techniques is included. Do known algorithms provide utility? . With the quantum utility criteria in mind, we can revisit the algorithms that were discussed before. | Â  | (1) Useful | (2) Better than classical | . | Optimisation: Rigorous but slow algorithms | âœ“ | ? | . | Optimisation: Fast algorithms in search of a use casess | ? | âœ“ | . | Optimisation: Heuristic algorithms | âœ“ | ? | . | Simulation of molecules and materials | âœ“ | ? | . | Breaking RSA | âœ“ | âœ“ | . Several algorithms, most notably Groverâ€™s algorithm, have an extensive range of industrial applicability. However, it seems that in practice, other (classical) approaches solve such problems faster. The quadratic speedup will be insufficient in the near term, and itâ€™s unclear if it will be in the future.Â  . Then, we have several exponential speedups, like the algorithm for topological data analysis, for which no practical uses have been found (despite many scientific and industrial efforts).Â  . Most optimistic outlooks focus on heuristic algorithms, for which the speed advantage will become clear with maturing hardware. Even for simulation of molecules and materials, it is hard to pinpoint precisely where we can find utility. Classical computers are already incredibly fast, and excellent classical algorithmic techniques have been developed. Scientist Garnet Chan even givesÂ talks which are suggestively titled â€˜Is There Evidence of Exponential Quantum Advantage in Quantum Chemistry?â€™.Â The case for chemistry and material science is quite subtle, and we discuss this further in the in-depth chapter on quantum simulation. To our best knowledge, codebreaking (Shorâ€™s algorithm) is the only impactful algorithm that has little competition from classical methods. Hopefully, most critical cryptography will be updated well before a quantum computer arrives, making large-scale deployment of Shorâ€™s algorithm relatively uninteresting. Either way, the application of codebreaking is not quite the positive innovation that quantum enthusiasts are looking for. Â  . Could the nature of quantum mechanics be such that exponential speedups are only found in codebreaking, chemistry, and a bunch of highly artificial toy problems, but nowhere else in the broad spectrum of practically relevant challenges? Most people would argue that such a scenario is unlikely. There are still good hopes that either some of the caveats with existing algorithms will be addressed or that new breakthrough algorithms will be discovered. How optimistic you are about quantum computing should depend on (at least) the following questions: . -Â Â Â Â Â Â Â  How impactful will heuristic and to-be-discovered algorithms be compared to classical algorithms? In other words, what is the algorithmic potential of quantum computing? . -Â Â Â Â Â Â Â  How will quantum hardware develop relative to classical hardware? . Ultimately, the commercial success of quantum computers depends strongly on these questions. If we allow ourselves to do some more hypothetical dreaming, I picture that the following future scenarios could be possible, on a spectrum of optimism versus pessimism: . Starting on the pessimistic side, if one believes that optimisation algorithms turn out to be lacklustre, then quantum computing might remain a niche for academics. However, depending on the utility of more widely applicable algorithms, it could be that quantum computers will be installed in special-purpose computing facilities or, even more optimistically, that they become increasingly common additions to data centres (much like GPUs today). Where would you place yourself in this figure? . ",
    "url": "/essentials/applications-overview/#where-is-the-killer-application",
    
    "relUrl": "/essentials/applications-overview/#where-is-the-killer-application"
  },"5": {
    "doc": "The applications: what problems will we solve?",
    "title": "Further reading",
    "content": ". | â€˜The Potential Impact of Quantum Computers on Societyâ€˜15 (Ronald de Wolf, 2017) is an accessible overview of known algorithms, together with an assessment of how we can ensure a mostly positive net effect on society. | â€˜Quantum algorithms: an overviewâ€˜16 (Ashley Montanaro, 2016) is a more technical overview paper that describes a selection of impactful algorithms in greater detail. | Professor Scott Aaronson warns us to â€˜Read the fine printâ€™ of optimisation algorithms.Â [Appeared inÂ Nature Physics, with paywall]Â  . | Professor Sanker Das Sarma warns of hype within the field of quantum optimisation and machine learning. | A quantitative analysis of Groverâ€™s runtime compared to todayâ€™s supercomputers.Â  . | (Scientific paper) Amazon researchers lay out a comprehensive list of end-to-end complexities of nearly every known quantum algorithm. | . | Jordan, S. (2024) Quantum Algorithm Zoo. Available at: https://quantumalgorithmzoo.org/ (Accessed: 27 September 2024).Â &#8617; . | Harrow, Aram W; Hassidim, Avinatan; Lloyd, Seth (2008). â€˜Quantum algorithm for linear systems of equationsâ€™. Physical Review Letters. 103 (15) 150502. https://doi.org/10.1103/PhysRevLett.103.150502Â &#8617; . | Aaronson, Scott. â€˜Read the Fine Print.â€™ Nature Physics 11, no. 4 (April 2015): 291â€“93. https://doi.org/10.1038/nphys3272. Open access: https://www.scottaaronson.com/papers/qml.pdf.Â &#8617; . | Liu, Yunchao, Srinivasan Arunachalam, and Kristan Temme. â€˜A Rigorous and Robust Quantum Speed-up in Supervised Machine Learning.â€™ Nature Physics 17, no. 9 (September 2021): 1013â€“17. https://doi.org/10.1038/s41567-021-01287-z.Â &#8617; . | Qiskit. â€˜How Ewin Tangâ€™s Dequantized Algorithms Are Helping Quantum Algorithm Researchers.â€™ Qiskit (blog), March 15, 2023. https://medium.com/qiskit/how-ewin-tangs-dequantized-algorithms-are-helping-quantum-algorithm-researchers-3821d3e29c65.Â &#8617; . | With the symbol \\(\\sim\\) we mean â€˜roughly proportional toâ€™. It essentially allows us to write down an approximation of a function, making them easier to read, throwing away some details are not important here.Â &#8617; . | Â You may find even sources stating that Shorâ€™s algorithm takes a time proportional to n2Â log(n). Such scaling is theoretically possible but relies onÂ asymptotic optimisationsÂ that are unlikely to be used in practice.Â &#8617; . | Technically, the best algorithms for factoring, like the general number field sieve, have a scaling behavior that lies between polynomial and exponential. Hence, the speedup of Shorâ€™s algorithm is technically a bit less than â€˜exponentialâ€™ â€“ a more correct term would be â€˜superpolynomialâ€™. Still, this book (and many other sources) continue to use the term â€˜exponential speedupâ€™ to emphasize the enormous scaling advantage over polynomial speedups.Â &#8617; . | Han-Sen Zhong et al., Quantum computational advantage using photons. Science 370, 1460-1463 (2020). https://doi.org/10.1126/science.abe8770Â &#8617; . | Arute, Frank, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C. Bardin, Rami Barends, Rupak Biswas, et al. â€˜Quantum Supremacy Using a Programmable Superconducting Processorâ€™. Nature 574, no. 7779 (October 2019): 505â€“10. https://doi.org/10.1038/s41586-019-1666-5.Â &#8617; . | Technically, IBM has a subtly different interpretation. In a blog post (see https://www.ibm.com/quantum/blog/what-is-quantum-utlity), they define â€˜utilityâ€™ as: â€˜Quantum computation that provides reliable, accurate solutions to problems that are beyond the reach of brute force classical computing methods, and which are otherwise only accessible to classical approximation methods.â€™ In other words: a quantum computer doesnâ€™t have to outperform any classical algorithm, it merely has to compete with the silly approach of brute-force search â€“ which is almost never the best algorithm in practise. This definition seems heavily focused on claiming utility as soon as possible. Nevertheless, if we look at the big picture, we seem to have a similar notion of â€˜advantage for end-usersâ€™ in mind, so Iâ€™m happy to adopt the term â€˜utilityâ€™ anyway.Â &#8617; . | Kim, Youngseok, Andrew Eddins, Sajant Anand, Ken Xuan Wei, Ewout van den Berg, Sami Rosenblatt, Hasan Nayfeh, et al. â€˜Evidence for the Utility of Quantum Computing before Fault Toleranceâ€™. Nature 618, no. 7965 (June 2023): 500â€“505. https://doi.org/10.1038/s41586-023-06096-3.Â &#8617; . | BeguÅ¡iÄ‡, Tomislav, and Garnet Kin-Lic Chan. â€˜Fast Classical Simulation of Evidence for the Utility of Quantum Computing before Fault Toleranceâ€™. arXiv, 28 June 2023. https://doi.org/10.48550/arXiv.2306.16372.Â &#8617; . | Tindall, Joseph, Matthew Fishman, E. Miles Stoudenmire, and Dries Sels. â€˜Efficient Tensor Network Simulation of IBMâ€™s Eagle Kicked Ising Experimentâ€™. PRX Quantum 5, no. 1 (23 January 2024): 010308. https://doi.org/10.1103/PRXQuantum.5.010308.Â &#8617; . | de Wolf, R. (2017) â€˜The potential impact of quantum computers on societyâ€™, Ethics and Information Technology, 19(4), pp. 271â€“276.https://doi.org/10.1007/s10676-017-9439-z. (Open access: https://arxiv.org/abs/1712.05380)Â &#8617; . | Montanaro, A. (2016) â€˜Quantum algorithms: an overviewâ€™, npj Quantum Information, 2(1), pp. 1â€“8. https://doi.org/10.1038/npjqi.2015.23.Â &#8617; . | . ",
    "url": "/essentials/applications-overview/#further-reading",
    
    "relUrl": "/essentials/applications-overview/#further-reading"
  },"6": {
    "doc": "The applications: what problems will we solve?",
    "title": "The applications: what problems will we solve?",
    "content": " ",
    "url": "/essentials/applications-overview/",
    
    "relUrl": "/essentials/applications-overview/"
  },"7": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "The background: why are we so enthusiastic about quantum technology?",
    "content": "Reading time: 13 minutes . Contents . | What is quantum technology? | The importance of high-performance computing | Why can quantum computers have an advantage?Â  | From algorithm to software | Further reading | . At a glance Quantum technology is an umbrella term for devices that exploit quantum phenomena like superposition and entanglement. The most notable innovations are expected in computers, networks, sensors, and simulators. Quantum computers can have a speed advantage thanks to their ability to run quantum algorithms, which solve specific problems in much fewer steps than conventional methods. However, quantum computers are expected to have relatively low clock speeds, so the algorithmic advantage must be significant before a practical speedup manifests itself. ",
    "url": "/essentials/background/#the-background-why-are-we-so-enthusiastic-about-quantum-technology",
    
    "relUrl": "/essentials/background/#the-background-why-are-we-so-enthusiastic-about-quantum-technology"
  },"8": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "What is quantum technology?",
    "content": "Quantum physics, the rules that dictate the behaviour of the tiniest particles, has already proven itself as an invaluable basis for new technologies. Without this scientific theory, many invaluable tools like LED lighting, MRI scanners and solar cells may not have been invented. And itâ€™s still relevant to push the limits of innovation, withÂ nano-size vehicles that consist of just a few atoms orÂ ever-smaller transistors on computer chips on the horizon. Just ahead of us is a new paradigm, which weâ€™ll call quantum technology. The distinguishing factor is that it goes beyond merely building stuff from small particles. Quantum technology is about devices that perform certain processes in a fundamentally different way. That is, the data (or operations) we work with can have special properties unique to quantum physics, such as superposition and entanglement. In our jargon, we will refer to â€˜classicalâ€™ technology for devices that donâ€™t carefully exploit the possibilities of quantum physics â€“ they are based on â€˜classicalâ€™ physics that weâ€™re used to from high school. Your laptop and phone are examples of classical computers, and theyâ€™re connected to the classical internet. The internal transistors and electrical circuits might be so tiny that quantum physics is relevant there, but the fundamental point is that the information that they process is purely classical. Whereas classical computers work with â€˜bitsâ€™, quantum technology will need a different type of information carrier that itself can be controlled at a quantum-mechanical level. Weâ€™ll call these objects â€˜quantum bitsâ€™, or shortly â€˜qubitsâ€™. Within the field of quantum technology, we distinguish four categories: . | Quantum computers are devices that use quantum physics to perform automatic calculations to solve a problem. Computing is considered the most impactful application for most organisations, and therefore, itâ€™s the main focus of this book.Â  . | Quantum networks are connections between quantum devices over whichÂ qubits (or similar forms of quantum data)Â can be transmitted. The most relevant use case is to strengthen cryptography used by classical computers, but there are many more applications. | Quantum sensors are devices that exploit the effects of quantum physics to accurately measure certain quantities, such as a magnetic field or the strength of the earthâ€™s gravity. Quantum clocks also fall into this category.Â  . | Quantum simulators are devices similar to quantum computers, except that they specialise in solving a limited set of problems. Typically, they are built to reproduce the behaviour of atoms and electrons in a specific molecule or a piece of material, allowing us to measure properties like energies and reaction rates. | . Each of these categories accomplishes a different goal or functionality. For now, weâ€™ll remain agnostic about how they are built â€“ it will be a task for hardware engineers to figure out how our desired functionality is best implemented. Since all of these have to deal with quantum-mechanical processes under the hood, it is not uncommon that they use similar building blocks. In this book, we mainly focus onÂ computersÂ andÂ networks because these seem to have the biggest impact on typical (business) users. ",
    "url": "/essentials/background/#what-is-quantum-technology",
    
    "relUrl": "/essentials/background/#what-is-quantum-technology"
  },"9": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "The importance of high-performance computing",
    "content": "The abundance of cheap computational power has given humanity incredible wealth. We automated the most tedious tasks to free up time for leisure and to solve other urgent problems. It allowed us to scale factories, supply chains and logistics to unprecedented sizes, allowing us to transport resources around the globe at minimal costs. Thanks to computer-aided design, the performance of computer chips, aeroplane wings, heart monitors, and LCD screens has improved with every generation. Today, our computers are already incredibly fast. In fact, for many applications, there is little economic gain in making these computers even faster. Decades-old machines can successfully oversee factory operations, and writing a text document or scheduling a meeting with eight busy colleagues is not limited by the speed of your computer in any way. However, this book specifically is about the applications where we are still hungry for more computational power. For example, by feeding more data into weather models, our forecasts can become more accurate. If staff rostering would take less time, we could take more last-minute changes into account. Accurate predictions of drug reactivity in the human body could save on costly medical trials and reduce the time to market. Machine learning models like ChatGPT are still demanding more training hours to produce more sophisticated results. It should be clear that weâ€™re not talking about computations that happen on your laptop. Weâ€™re thinking of problems where somehow thereâ€™s value in investing in the fastest possible computers on earth. This is the domain of high-performance computing (HPC), colloquially called supercomputers. Merely looking at the market, there seems to be incredible value in computing stuff: companies and academics spend tens of billions of dollars on them1, and hardware suppliers like Nvidia have rapidly grown to become among the most valuable companies on earth. We should keep a close eye on this field because the kinds of problems that are now being crunched in HPC are likely the ones where radically new computational tools can have the biggest commercial impact. ",
    "url": "/essentials/background/#the-importance-of-high-performance-computing",
    
    "relUrl": "/essentials/background/#the-importance-of-high-performance-computing"
  },"10": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "Why can quantum computers have an advantage?Â ",
    "content": "A naive view of quantum computers is that theyâ€™re simplyÂ fasterÂ than their conventional cousins. Or perhaps one may naively point at Mooreâ€™s Law: with transistors reaching atomic scales, we run into quantum effects, so quantum physics may help us make better chips. However, none of these are our core motivations for looking at quantum computers.Â  . When we talk about a computerâ€™s speed, most people will refer to its clock speed: the number of basic computational steps that a single processor core can complete in one second. Unfortunately,Â it seems unlikely that quantum computers will catch up with classical machines in terms of raw clock speed any time soon, partially because the speed of a modern CPU is already spectacular. A modern desktop processor, or even the one in your phone, works at a rate of several GHz, that is, several billions of steps per second. In each of these steps, a broad palette of operations can be applied to astronomically large numbers â€“ modern chips work with 64-bit values, meaning that numbers up to 18,446,744,073,709,551,615 can be processed. Each of these elementary steps can be something like addition, multiplication, a comparison, etcetera, and we have powerful tools to weave these basic operations together to form efficient software. Now, quantum computers are supposed to be even faster, right? Well, itâ€™s not hard to find backing for that claim:Â  . News headers by Techradar2 and IFLScience3. You may be disappointed to hear that quantum computers, at this moment, cannot even add or multiply numbers of more than 3 or 4 bits. And even if they could, their rate of operation would by no means reach several GHz, but more likely several MHz (a few million operations per second) at best. In other words, theyâ€™re more than a thousand timesÂ slower.Â To make things worse, the information in quantum computers is extremely fragile and needs to be constantly checked and corrected using so-calledÂ error correction.Â This is a form of overhead that could make quantum computers another several orders of magnitude slower. Even in the far future, when quantum computers are more mature and more reliable, we still expect them to be much slower than the classical chips at that time.Â  . How does this rhyme with the news about ever-faster quantum computers? And why are we still interested in these slow machines? As we claimed before, we hope to do certain computations in aÂ fundamentally different way. Letâ€™s look at a beautiful analogy that Andy Matuschak and Michael Nielsen bring up in their online course Quantum.country.Â  . Imagine that youâ€™d like to travel from Morocco to Spain, which are separated by a small piece of sea called the Strait of Gibraltar. If your technology does not allow you to cross the sea (say, you have access to a car but not a boat), then youâ€™d need to take a large detour, all the way through North Africa, past the Arabian Peninsula, and through Europe, before you can reach your destination. This represents the steps taken by a classical computer. In the same analogy, a quantum computer grants you the ability to traverse both land and sea (much like a hovercraft) so that you can take a much more direct route. The beauty of quantum computation is that we have a fundamentally different way to travel (do computations), which can sometimes bring us to our destination using a shorter route (doing fewer computational steps). Even with a much slower vehicle (computer), one may arrive at the destination sooner. In fact, the quantum advantage often grows as problems become larger and more complicated. The analogy also shows that quantum computers do not always have an advantage: you would not want to travel from Amsterdam to Berlin by hovercraft. Unfortunately, in many cases, we donâ€™t yet know what the fastest means of transportation is. It is still an active area of research to completely map out the landscape over which quantum and classical computers can travel and to determine which problems allow a speedup, and which donâ€™t. For this reason, we donâ€™t expect that classical computers will be replaced any time soon. Instead, classical and quantum processors will live side by side, and programmers will pick whichever tool is better suited to solve a certain problem. The situation could be similar to how we use graphical processing units (GPUs) today, which offer tremendous speedups for the training of artificial intelligence models but are not made to replace regular classical processors (CPUs). Perhaps we should even give quantum computers a similar name, like â€˜QPUâ€™ for Quantum Processing Unit. In the analogy with the Strait of Gibraltar, the precise route that you travel denotes the chosenÂ algorithm.Â In the field of computer science, an algorithm is aÂ step-by-step list of instructionsÂ that describes how a computational problem should be solved. TheÂ â€˜stepsâ€™Â here should be sufficiently simple so that it is completely unambiguous how to do them. They could be operations such as adding, multiplying, or comparing two numbers. Needless the say, the fewer steps the algorithm requires, the better. By exploiting quantum mechanics, a quantum computer introduces new basic steps that are impossible to perform on a classical computer. For example, the previous chapter introduced quantum logic gates that generalise operations like AND and OR. Using these building blocks, we can formulate quantum algorithms that take much fewer steps than the best classical algorithm ever could! . In the end, the time needed to solve a problem can be very roughly calculated as: \\(\\text{ }{\\text{Time to solve a problem} = \\text{time per step}\\ \\times \\text{number of steps required}}\\) . The â€˜time per stepâ€™ is a property of the hardware that you use. Clearly, a faster CPU will lead to faster solutions. The â€˜number of steps requiredâ€™ is dictated by the algorithm. The latter is precisely how quantum computers can offer spectacular speedups. As long as the improvement in the â€˜number of steps requiredâ€™ compensates for the disadvantage in â€˜time per stepâ€™, a quantum computer can help us solve problems in less time! . A recurring theme in this book is the search for industrially relevant quantum algorithms. This turns out to be more challenging than it seems at first sight. Quantum algorithms are built on deep and complex mathematics, rely on counter-intuitive quantum phenomena, and require inventive new methods to tackle a problem. Simple tweaks to existing classical algorithms are rarely sufficient. In fact, for most problems, no quantum speedups have been identified yet at all, despite the best attempts by scientists worldwide. We might go as far as to say that, even if we had a large-scale quantum computer today, its value would be limited. For this reason, the ongoing development of novel algorithms is exceedingly important. ",
    "url": "/essentials/background/#why-can-quantum-computers-have-an-advantage",
    
    "relUrl": "/essentials/background/#why-can-quantum-computers-have-an-advantage"
  },"11": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "From algorithm to software",
    "content": "In the end, simply finding a good algorithm is not enough: it has to be turned into software, a piece of language that explicitly tells a computer how to execute the step-by-step instructions. The difference between â€˜algorithmsâ€™ and â€˜softwareâ€™ is subtle. An algorithm is a purely mathematical description that describes precisely how numbers should be manipulated. It could tell which two numbers must be multiplied, what function must be evaluated, or how an image must be transformed. However, different computers can use different types of processors and memory, and an algorithm does not describe how these operations are done on a specific computer. This is where software comes into play. It describes precisely what hardware operation must be called, where each number is stored in memory, and how an image is represented in binary. As an analogy, you may think of the algorithm as a recipe to bake the perfect chocolate cookie. The algorithm should unambiguously describe what should happen to the ingredients: in what order they should be mixed, how long they should be heated at what temperature, etcetera. However, to build a factory that produces these cookies, you need to be even more specific: Where is the sugar stored? Out of what pipe does the dough flow? How are cookies laid next to each other in the oven? . Fundamentally, core scientific breakthroughs come from finding new algorithms. Once a new algorithm is found, it can be re-used many different times on any capable machine (assuming a good software developer will turn it into appropriate code!). In this book, we care less about quantum software and more about quantum algorithms. Firstly, the algorithms tell us precisely the functionality that quantum computers can offer. Moreover, we donâ€™t yet know how a mature quantum computer will be programmed or how quantum hardware and software will change in the following years. On the other hand, once a new algorithm is found, it can be cherished forever. Now that we have come to appreciate algorithms, it is natural to ask which quantum algorithms we know of. What problems do quantum computers solve well? And how do these algorithms compare to their classical equivalents? This will be the topic of the next chapter. ",
    "url": "/essentials/background/#from-algorithm-to-software",
    
    "relUrl": "/essentials/background/#from-algorithm-to-software"
  },"12": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "Further reading",
    "content": ". | The Map of Quantum Computing (Youtube)Â  â€“ A 30-minute overview video by Domain of Science that forms a great supplement to this book.Â  . | Chris Ferrieâ€™s book â€˜What You Shouldnâ€™t Know About Quantum Computersâ€™ debunks several myths about quantum computers, presented in a very accessible way. | Are you looking for a much more extensive source that covers pretty much everything there is to know about quantum computers? French consultant Olivier Ezratti maintains a 1500+ page book, â€˜Understanding Quantum Technologiesâ€˜. | . | See e.g. https://www.marketsandmarkets.com/Market-Reports/Quantum-High-Performance-Computing-Market-631.html and https://www.mordorintelligence.com/industry-reports/cloud-high-performance-computing-hpc-market.Â &#8617; . | WyciÅ›lik-Wilson, S.E. (2019) Google creates quantum chip millions of times faster than the fastest supercomputer, TechRadar. Available at: https://www.techradar.com/news/google-creates-quantum-chip-millions-of-times-faster-than-the-fastest-supercomputer (Accessed: 29 September 2024).Â &#8617; . | Dunhill, J. (2021) Chinese Scientists Create Quantum Processor 60,000 Times Faster Than Current Supercomputers, IFLScience. Available at: https://www.iflscience.com/chinese-scientists-create-quantum-processor-60000-times-faster-than-current-supercomputers-61475 (Accessed: 29 September 2024).Â &#8617; . | . ",
    "url": "/essentials/background/#further-reading",
    
    "relUrl": "/essentials/background/#further-reading"
  },"13": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "The background: why are we so enthusiastic about quantum?",
    "content": " ",
    "url": "/essentials/background/",
    
    "relUrl": "/essentials/background/"
  },"14": {
    "doc": "Four myths about quantum computing",
    "title": "Four myths about quantum computing",
    "content": "Reading time: 10 minutes . Contents . | Myth 1: Quantum computers find all solutions at once | Myth 2: Qubits can store much more data than the same number of classical bits. | Myth 3: Entanglement allows you to send information faster than light or to influence objects at a distance | Myth 4: Quantum computers are always ten years away. | Further reading | . This chapter relies on a bit of quantum physics jargon. See the Introduction to the quantum world for a quick introduction. ",
    "url": "/essentials/myths/",
    
    "relUrl": "/essentials/myths/"
  },"15": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 1: Quantum computers find all solutions at once",
    "content": "This myth is likely the most technical, and builds on a misinterpretation of the concept of superposition. A single qubit can be in two states at the same time (0 and 1), two qubits can represent four states (00, 01, 10, 11), and three qubits are potentially in eight unique configurations simultaneously. As we increase the number of qubits, this number of coexisting states scales exponentially! . This means that a mere 1000 qubits can effectively â€˜storeâ€™ \\(2^{1000}\\) unique values, all at the same time. Thatâ€™s an incomprehensibly large number, much more than there are atoms in the visible universe. Even the fastest computers in the world couldnâ€™t loop through all these states in a lifetime. Each of these states can be interpreted like a file on a computer, be it an Excel spreadsheet, a web page, a CAD drawing, or whatever kind of data we choose to work with. A smart computer scientist can also devise a way to make 1000 bits represent â€˜solutionsâ€™ to some problem. For example, imagine that we want to find an optimal aeroplane wing that generates incredible lift while requiring as few materials as possible. Using quantum superposition, we might represent \\(2^{1000}\\) such wings all at once. We picked the example of aeroplane wings because simulating their aerodynamic properties requires a pretty hefty computation. Letâ€™s assume that we have written such a computer program that accurately simulates any wing, and call that program \\(f\\). It will output 1 if the wing works well (according to whatever metric), and 0 otherwise. Surely, the program takes a very large number of computation steps, which weâ€™ll call T. The program will need some input, denoted by \\(x\\), which is a 1000-bit description of all the relevant properties of a hypothetical aeroplane wing. In other words, the compute program computes \\(f(x) = 1\\) if \\(x\\) is a fantastic wing, and \\(f(x) = 0\\) if itâ€™s rubbish. Now, a quantum computer should be able to execute any classical function, right? We should be able to run \\(f\\) on a quantum computer, but now we have the unique feature that the 1000-qubit input can represent a humongous number of potential aeroplane wings at the same time! By doing a mere T computational steps, we can check the properties of \\(2^{1000}\\) solutions! . If this actually worked, quantum computers would have an astonishing power. They could straightforwardly find mathematical proofs that humans havenâ€™t been able to solve in centuries, simply by trying all possible proofs in parallel. They would rapidly produce the perfect train and bus schedules, discover new drugs and straightforwardly hack encryption systems. They would solve problems in the complexity class NP, which is widely believed to be impossible with machines in our universe, owing to the famous P â‰  NP conjecture. So, whereâ€™s the catch? For those who read the introduction to quantum physics, we shouldnâ€™t forget about the postulate of quantum measurement. The output of the computation would be a superposition over \\(2^{1000}\\) outcomes. If we want to learn anything about this output, weâ€™d perform a quantum measurement that collapses this superposition. Instead of looking at \\(2^{1000}\\) different solutions simultaneously, we get to see only one outcome â€“ corresponding to the performance of just a random aeroplane wing. In this case, there is no advantage compared to a classical computer because we couldâ€™ve just as well picked a random wing at first, and then spent the same T steps on a (much faster) classical machine. Although this â€˜quantum parallelismâ€™ is too good to be true, quantum computers can use the above idea to some lesser extent. Using Groverâ€™s algorithm, we can find desirable solutions (the \\(x\\) for which \\(f(x) = 1\\)) in roughly the square root of the number of values that \\(x\\) can take. In the above example, the number of required steps is reduced to \\(\\sqrt{2^{1000}}\\ T = 2^{500}\\ T\\). This is an incredible reduction, but weâ€™re still looking at a number of steps larger than the number of atoms in the universe â€“ finding solutions with this brute-force method remains far from efficient. ",
    "url": "/essentials/myths/#myth-1-quantum-computers-find-all-solutions-at-once",
    
    "relUrl": "/essentials/myths/#myth-1-quantum-computers-find-all-solutions-at-once"
  },"16": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 2: Qubits can store much more data than the same number of classical bits.",
    "content": "This myth is very similar to the previous one: canâ€™t N qubits represent \\(2^{N}\\) different numbers at the same time? Or arenâ€™t they perhaps even more powerful, because for each of the \\(2^{N}\\) different numbers, thereâ€™s a complex number, which can have as many decimal digits as we like? . Again, by the rules of quantum measurement, this is too good to be true. Itâ€™s impossible to store much information in a qubit because it collapses to a classical 0 or 1 when we measure it. The problem is really in retrieving the information, where we have very limited capabilities. For the same reason, when sending a classical message over a long distance, thereâ€™s little value in using qubits as information carriers. As a side note, there is a fascinating related protocol called â€˜superdense codingâ€™ that you may like to look up out of theoretical interest. Also, when your data itself represents something quantum (for example, the state of electrons in a molecule), then storing this data in qubits does have a potentially huge advantage. ",
    "url": "/essentials/myths/#myth-2-qubits-can-store-much-more-data-than-the-same-number-of-classical-bits",
    
    "relUrl": "/essentials/myths/#myth-2-qubits-can-store-much-more-data-than-the-same-number-of-classical-bits"
  },"17": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 3: Entanglement allows you to send information faster than light or to influence objects at a distance",
    "content": "Entanglement is an incredibly confusing phenomenon. In particular, our most common interpretation of quantum mechanics states that whenever we measure one qubit, the state of another distant qubit can drastically change. Whilst this picture is helpful for physicists when performing computations, it tricks our intuition. Imagine that, in the faraway future, we want to protect our solar system against an alien invasion. We installed sentinels on faraway outposts, which should signal Earth of any approaching dangers. Alice is one of these noble guards stationed at a remote asteroid in the icy Kuiper Belt. She brought with her a qubit labelled A, which is entangled with another qubit B thatâ€™s safely kept on earth by her colleague Bob. Whilst it takes light signals around 5 hours to travel between them, isnâ€™t there a way for Alice to alarm Bob any faster, possibly by doing some special operations on her qubit? Perhaps she could even give some clues about the type of looming threat? . Unfortunately, Alice cannot remotely change any measurable quantity of Bobâ€™s qubit. Bobâ€™s measurements will always have the same outcome probabilities, no matter what Alice does to her qubit. Using more qubits or employing different quantum objects wonâ€™t help either. Fundamentally, there is no way to signal any information faster than the speed of light. There is a subtle difference between â€˜changing measurable quantitiesâ€™ and â€˜knowing somethingâ€™ about the state of a particle. To illustrate, assume that we start with a particular entangled state: measuring qubits A and B will result either in both qubits being â€˜0â€™ or both qubits being â€˜1â€™, letâ€™s say with 50% probability each. Measuring something like A= â€˜0â€™ and B= â€˜1â€™ is impossible. When Alice measures her qubit and reads the outcome â€˜0â€™, she immediately knows the outcome of a future measurement made by Bob: she knows this will be â€˜0â€™ with 100% probability. However, this knowledge is not accessible to Bob. He doesnâ€™t even know whether Alice measured or not! Even if they agreed in advance that Alice would measure at a set time, Bob doesnâ€™t know her outcome. From his perspective, â€˜0â€™ or â€˜1â€™ are still equally likely. Something interesting happens when Alice sends a message to Bob to inform him that her measurement returned â€˜0â€™. With this updated knowledge, Bob suddenly knows precisely what the state of his qubit is: it must have collapsed to â€˜0â€™, and he can perfectly predict the outcome of a subsequent measurement. In a way, this did indeed change the state of the qubit from Bobâ€™s perspective, but it was only possible after some (classical) communication took place between Alice to Bob, a process that is limited by the speed of light. What is quantum entanglement good for, then? Some potential applications include: . | Creating certifiably secure encryption keys at remote locations. | Creating certifiable randomness. | Forming connections between separate quantum computers, allowing them to send quantum data to each other using teleportation. For this to work, devices also need to transfer some classical data, so qubit transmission is never faster than the speed of light. Teleportation is an intriguing method to scale up quantum computers when a limited number of qubits can fit on a single chip or within a single fridge. | . ",
    "url": "/essentials/myths/#myth-3-entanglement-allows-you-to-send-information-faster-than-light-or-to-influence-objects-at-a-distance",
    
    "relUrl": "/essentials/myths/#myth-3-entanglement-allows-you-to-send-information-faster-than-light-or-to-influence-objects-at-a-distance"
  },"18": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 4: Quantum computers are always ten years away.",
    "content": "This statement is a playful reference to the situation around nuclear fusion, where predictions of its realisation being just 30 years in the future have repeatedly been postponed. Scientists have been working on fusion for decades, but itâ€™s still far from a mature energy source. Similarly, Iâ€™ve heard several overly optimistic claims about quantum computers being made in the past ten years, often claiming that quantum computers are somewhere between three to ten years away. An article by TechCrunch1 boldly paraphrases Dario Gil (IBM) and Chad Rigetti (founder of Rigetti Computing) saying that â€˜the moment that a quantum computer will be able to perform operations better than a classical computer is only three years awayâ€™, whilst this article was published back in 2018. For reference, the 127-qubit Eagle chip was announced by IBM at the end of 2021, but even several years later, itâ€™s still primarily used for testing and education. In 2019, consulting firm Gartner published â€˜The CIOâ€™s Guide to Quantum Computingâ€™, which indicates that 100â€”200 qubits are sufficient for â€˜key potential applicationsâ€™ in chemistry. They also predicted that â€˜by 2023, 20% of organisations will be budgeting for quantum computing projectsâ€™. Clearly, these predictions were overly optimistic. Similarly, Microsoft made claims in 2018 that their cloud platform Azure would feature quantum computing in 5 years2, which is technically true. However, they have repeatedly hinted to do this with fault-tolerant topological qubits, which still remain elusive. Startup PsiQuantum famously claimed to have a million photonic qubits by 20253, and consultants at BCG advised that quantum computers â€˜generate business valueâ€™ in the same year4. Again, it remains to be seen if this holds true. Luckily, if youâ€™re reading this book, you must have noticed that not all experts share the same vision. Most scientists have warned for a long time that quantum computing is a long-term effort. Nevertheless, the thesis that â€˜quantum computing is always X years awayâ€™ is hard to defend, thanks to convincing evidence that we are steadily progressing towards a clear goal. Every year, quantum hardware sees major improvements in the number of qubits, their stability, and the level of control that is demonstrated. Most experts even expect an exponential scaling of the number of qubits, similar to Mooreâ€™s Law, and manufacturers have clear roadmaps that underline these predictions. Moreover, theorists have set clear targets for when the hardware is good enoughâ€”and weâ€™d sooner see the requirements drop with new breakthroughs rather than become more stringent. Building a quantum computer is a long marathon, and itâ€™s impossible to predict when they willÂ become commercially relevant, but the rapid rate of progress is undeniable. ",
    "url": "/essentials/myths/#myth-4-quantum-computers-are-always-ten-years-away",
    
    "relUrl": "/essentials/myths/#myth-4-quantum-computers-are-always-ten-years-away"
  },"19": {
    "doc": "Four myths about quantum computing",
    "title": "Further reading",
    "content": ". | (Youtube) Veritasium explains Entanglement . | (Youtube, technical!) Minute Physics explains Teleportation . | Chris Ferrie debunks more myths in his free book â€˜What you shouldnâ€™t know about Quantum Computersâ€˜ . | Scott Aaronson shares a transcript of a public talk, explaining why he is optimistic about the steady progress towards large-scale quantum computers. | . | Shieber, Jonathan. â€˜The Reality of Quantum Computing Could Be Just Three Years Away.â€™ TechCrunch, September 7, 2018. https://techcrunch.com/2018/09/07/the-reality-of-quantum-computing-could-be-just-three-years-away/.Â &#8617; . | Saran, Cliff. â€˜Microsoft Predicts Five-Year Wait for Quantum Computing in Azure.â€™ ComputerWeekly.com, May 9, 2018. https://www.computerweekly.com/news/252440763/Microsoft-predicts-five-year-wait-for-quantum-computing-in-Azure.Â &#8617; . | Cookson, C. (2021) â€˜PsiQuantum expects commercial quantum computer by 2025â€™, 13 March. Available at: https://www.ft.com/content/a5af3039-abbf-4b25-92e2-c40e5957c8cd (Accessed: 26 September 2024).Â &#8617; . | Matt Langione, Jean-FranÃ§ois Bobier, Zheng Cui, Cassia Naudet-Baulieu, Amit Kumar, and Antoine GourÃ©vitch. â€˜Quantum Computing Is Becoming Business Ready.â€™ BCG Global, April 27, 2023. https://www.bcg.com/publications/2023/enterprise-grade-quantum-computing-almost-ready.Â &#8617; . | . ",
    "url": "/essentials/myths/#further-reading",
    
    "relUrl": "/essentials/myths/#further-reading"
  },"20": {
    "doc": "Preface: why this book?",
    "title": "Preface: why this book?",
    "content": "Â  . Â  . â€˜Quantum computing will change everything,â€™ the man in front of me said. Standing tall and confident, he took another sip of his drink before continuing, â€˜It will be the biggest revolution since the invention of the transistor. Imagine a world where we can cure any disease with personalised medicine. A world where new energy sources will free us from our dependence on fossil fuels. Not to mention thatâ€¦â€™ . â€˜Wellâ€”â€™ I tried to interrupt, but the man passionately rattled on. â€˜It will finally enable us to build general Artificial Intelligence that can take over our tedious everyday jobs, so 95% of our population no longer has to work!â€™ . â€˜You know that quantum computers are still quite some years away, right?â€™, I countered. He leaned in, eyes still gleaming with excitement. â€˜Thatâ€™s what most people think. But the reality is, weâ€™re closer than ever. Quantum supremacy has already been achieved. Google did it in 2019; since then, progress has been exponential. Did you see the presentation by that guy from Goldman Sachs? Their investments already see higher returns than ever since their new Monte Carlo algorithm.â€™ . The above conversation captures a feeling that many seasoned experts in quantum computing will have. A group of enthusiasts presents â€˜quantumâ€™ as a revolutionary technology with unprecedented capabilities. Plentiful reputable sources report how next-generation devices are key in tackling climate change, revolutionising AI, and building unhackable networks. Experts who are actually building quantum computers are much, much more reluctant. At an academic conference, you hear a completely different story. Scientists ridicule the absurd claims that some consultants and startups make. They will point out that the applications of quantum computers are still very much uncertain and that weâ€™re still searching for convincing use cases. The quantum scene seems divided into two distinct worlds. One is the business world, eager to reach out to anybody who will hear them about the game-changing capabilities of quantum computers. The other is a more cautious community of scientists and technical experts who quietly bring quantum computers to reality, sharing their results in specialised papers that require a PhD to understand. I was fascinated by this paradoxical situation. Who is right? How powerful are these quantum computers really, and how do they compare to existing technologies? In what year will we have a large-scale quantum computer, and what will it look like? These are billion-dollar questions, but the answers will vary wildly, depending on who you ask. After searching for these answers for a decade, Iâ€™ve finally found a unique position to answer most of these questions. As a former academic researcher, I acquired a detailed understanding of quantum computers and their algorithms. For the past four years, I haveÂ had the privilege of forming R&amp;D collaborations with startups, enterprises and governmentsÂ while having countless meetings with CEOs, research leads and policymakers. Iâ€™ve seen the perspectives from both worlds and can cut through dishonest and deceptive claims. Additionally, after training many new colleagues and setting up professional learning programs, I developed a good intuition about what newcomers want to know about quantum technology and how to explain it in an accessible way. However, the decisive factor that led me to write this book is my discomfort with other sources. Like many others in this field, Iâ€™m unhappy with the many hyped and unbalanced articles that would otherwise populate the top entries in Google search results (or even the New York Times best-selling books1). There is a clear need for a neutral source of information that others can reference when disagreeing about facts or debunking myths, and Iâ€™m very happy that itâ€™s finally complete. That doesnâ€™t mean that this book contains only confirmed facts â€“ not at all! Writing about a computer of the future comes with mountains of uncertainty. In 2005, nobody could have predicted that a mere five years ahead, everyone would be playing games and consuming the internet on their smartphones. In 2015, nobody could have predicted the impact of Large Language Models like ChatGPT. And indeed, todayâ€™s best predictions of a future quantum revolution wonâ€™t be quite so accurate either. Even worse, experts wildly disagree in several cases. For example, the usefulness of quantum AI and optimisation is vigorously disputed, and the rate at which hardware will progress depends on many yet-to-discover breakthroughs. The best I could do is describe various perspectives on these matters and highlight the best arguments from both sides. My colleagues and I had many discussions and disagreements, without which I wouldnâ€™t have been able to gather the facts and opinions in this book. And it shouldnâ€™t stop there. I keep welcoming criticism, opinions, and feedback about these complex topics, aiming to refine these texts even more in future updates. Even though much is still uncertain, I think that a reliable indication of the prospects of quantum computing is more important than ever. Quantum startups are acquiring huge investments, allowing them to hire managers, software developers, salesmen, and marketers. Governments need informed policymakers, and journalists should cover quantum breakthroughs. Pretty much every organisation that deals with IT will want to keep a close eye on the impact that â€˜quantumâ€™ will have on them. This book is for precisely these people who donâ€™t need to understand all the technical details but still need to talk, read, and write about quantum technologies. We wonâ€™t care so much about the underlying math or physics but rather about the functionality of a quantum computer: the opportunities, the threats, and the concrete actions organisations can take. How should you read this book? I chose to split the content into three parts. The first part contains the essentials that we recommend for everyone to read. This is an incredibly efficient way to learn all the background that you need â€“ you should be ready to understand other sources and have some depth in professional discussions or meetings. To go into more detail, parts two and three contain more information about the (software) applications and the (hardware) devices, respectively. A final fourth part is reserved for additional resources that can be useful or fun when continuing your quantum journey. | Iâ€™m referring to Michio Kakuâ€™s book â€˜Quantum Supremacyâ€™, but before you even consider reading it, you might like to see the book review by a professor in quantum computer science (https://scottaaronson.blog/?p=7321).Â &#8617; . | . ",
    "url": "/essentials/preface/",
    
    "relUrl": "/essentials/preface/"
  },"21": {
    "doc": "An introduction to the quantum world",
    "title": "An introduction to the quantum world",
    "content": "Reading time: 17 minutes . Contents . | What is quantum?Â Â  | Four surprising phenomena | What does a quantum computer look like? | Further reading | . At a glance You donâ€™t need to understand quantum mechanics to understand the functionality of quantum computers. But if you insist, quantum mechanics describes the behaviour of the smallest particles. It leads to many counter-intuitive phenomena: computer memory can store multiple pieces of data at the same time, but upon measurement, nature selects just a single piece (and throws away all the others). If you want to drive a car, do you need to understand how its engine works? Of course, you donâ€™t! In a similar vein, you donâ€™t need to know the details of quantum physics to read the rest of this book. So feel free to skip this chapter. Nevertheless, I know that most people want to have some conceptual intuition about what quantum mechanics really is. It is not natural to leave one of the most used words in this book as an abstract concept, and it might be hard for the human brain to proceed without at least seeing some examples. Here is my best attempt to explain quantum mechanics in accessible terms. Proceed with caution, as things will surely get confusing from here. ",
    "url": "/essentials/quantum/",
    
    "relUrl": "/essentials/quantum/"
  },"22": {
    "doc": "An introduction to the quantum world",
    "title": "What is quantum?Â Â ",
    "content": "Quantum physics or quantum mechanicsÂ is the theory that describes the tiniest particles, like electrons, atoms, and small molecules. The theory is meant to describe the fundamental laws of nature using a set of mathematical equations, allowing us to predict cause and effect at the scale of nanometers. It answers questions like â€˜What happens when I bring two electrons close together?â€™ or â€˜Will these two substances undergo a chemical reaction?â€™. You may contrast quantum mechanics to Newtonâ€™s classical physics that we learned in high school, which works great for objects the size of a building or football but becomes inaccurate at much smaller scales. Quantum is, in a sense, aÂ refinementÂ of classical physics: the theories are effectively identical when applied to a coffee mug, but the more difficult quantum theory is needed to describe very small things.Â  . Some examples of systems where quantum could play a role are: . | Atoms and the electrons that orbit around them. | Flows of electricity in microscopic (nano-scale) wires and chips. | Photons, the particles out of which light is made. | . To proceed, we need some physics jargon. We like to use the word â€˜stateâ€™, which is a complete description of all the physical properties of the world at one instance: the locations of all the different particles, their velocities, how much they rotate, etcetera. Usually, the entire universe is too big to study, so we often simplify our world to just a single isolated particle or to a limited piece of computer memory. Letâ€™s imagine a bare particle in an otherwise empty world. We may be interested in its location, which weâ€™ll call \\(x\\). For example, the world might look something likeÂ the image below, which can be described by a very simple state: \\(x = 5\\) (the ruler is just virtual). In the spirit of computing, we might look at a â€˜bitâ€™ that stores information. You may think of it as a tiny magnet that can either point â€˜upâ€™ (1) or â€˜downâ€™ (0). The state of a piece of memory is easy to describe, simply by stating the bit values one by one. For example: 11010. Importantly, the state of the world can change over time. We will often care about the state of the world at a certain moment, for example, at the beginning of computation or at the end of it. ",
    "url": "/essentials/quantum/#what-is-quantum",
    
    "relUrl": "/essentials/quantum/#what-is-quantum"
  },"23": {
    "doc": "An introduction to the quantum world",
    "title": "Four surprising phenomena",
    "content": "The most iconic quantum phenomenon is superposition. Think about any property that we can (classically) measure, such as the position of a particle or the value of a bit on a hard drive (0 or 1). In quantum mechanics, theÂ state of the world can be such that many different measurement outcomes are somewhat â€˜trueâ€™ at the same time: a particle can be at multiple positions at once, or a bit could be 0 and 1 simultaneously. When we say â€˜at the same timeâ€™ we mean that, to predict any cause and effect, we need to keep track of all these possibilities. To illustrate, I sometimes picture a quantum particle to split into many opaque copies of itself, spread out over space, where the degree of transparency determines how likely the particle is to be found there: the darker, the more likely. How can you possibly describe a state like that? For a single particle, the state is a long list, where for each possible position, we store a number called the amplitude, which is related to how likely the particle is to be found at that location. In other words, the state describes precisely to what extent a particle is at position \\(x = 0\\), to what extent at position \\(x = 1\\), and so forth, for every possible location that the particle can be at. And indeed, this list could be infinitely long! Luckily, when dealing with computers, we work with simpler objects. A quantum bit needs just two amplitudes, which denote the extent to which the bit is â€˜0â€™ or â€˜1â€™, respectively. Because we will talk about quantum-mechanical bits a lot, we will give them a shorter name: qubits. If we have a bunch of qubits together, weâ€™ll call it a quantum memory. To throw in some more examples of weird quantum states, an electron can move at a velocity of 10 m/s and 100 m/s at the same time (which obviously also leads to a superposition in its location). More relevant for us: a quantum memory might store the numbers 5 and 11 â€˜simultaneouslyâ€™ or even 46 different Microsoft Excel spreadsheets â€˜at onceâ€™. These amplitudes feel somewhat analogous to probabilities, which can similarly describe the likelihood that a particle can be found at some location. However, there is a fundamental difference. Probabilities in the classical world help us deal with information we donâ€™t have: surely, the particle is already at some location, but perhaps we just donâ€™t know which location yet. Quantum mechanics is different. Even if we know every tiny detail about the location of a particle, we still need to describe it as a superposition. Fundamentally, the location is not determined yet. Hence, there is literally no better way to describe the particle than by tracking this convoluted superposition. Amplitudes are also more finicky to deal with than probabilities because these numbers can become negative (and for math experts, they can even be complex numbers). The second weird phenomenon is how quantum measurements work. Why do we never observe an electron at two places at the same time? Why do I never find a car both moving and standing still? In quantum mechanics, as soon as we measure the location of a particle, it instantly jumps to just a single location at random â€“ making its location fully determined. Similarly, when we measure a qubit, it jumps to either â€˜0â€™ or â€˜1â€™. When we measure the data in a quantum memory, we may find any one of the 46 spreadsheets that were stored. This means that our world is intrinsically random (and hence, not deterministic!). But this doesnâ€™t imply that we cannot understand it. We can calculate the probabilities of measurement outcomes with incredible precision as long as we know the state before the measurement. It is important to note that we cannot learn anything about the world without measuring â€“ it is our only way to obtain data from the world. Any observation, even a slight peek at our system, is a measurement in quantum mechanics. However, measurements are destructive in the sense that they change the state of the world. We fundamentally cannot â€˜lookâ€™ at a particle without disturbing it. In fact, they delete all the rich data encoded in a superposition! If a particle was initially at position \\(x = 0\\),\\(\\ x = 3\\ \\)and \\(x = 10,\\) all simultaneously, then upon measurement, it jumps to one of these three options. In jargon, we call this instantaneous change a â€˜collapseâ€™. From then onwards, it is 100% at a fixed location: if, at first, we measure the particle to be at \\(x = 3\\), then any subsequent measurement will give the same result, until some other force moves it again. This means that, during a quantum computation, we should carefully choose when we perform any measurements â€“ we cannot just peek at the data at any moment we like, or we risk disturbing a superposition. This also means that a single piece of quantum memory cannot store an immense number of spreadsheets at the same time â€“ at least, you wouldnâ€™t be able to retrieve each of them. To store 15 MB worth of classical data, we need 15 MB worth of qubits. Hence, quantum computers are not particularly useful for storing classical data. The fact that a measurement changes the state of the world poses a serious problem for the engineers who are building quantum computers. No matter what material we construct our qubits from, they will surely interact with other nearby particles, and some of these interactions could act like destructive measurements. We call this effect decoherence, and we will later see that this forms one of the core challenges to large-scale quantum computation. At this point, quantum data doesnâ€™t seem particularly useful. Why would we want to deal with superpositions if they lead to all this uncertainty? Intuitively, think about the advantage of a quantum computer in the following way. Using quantum mechanics, a device can manipulate data in ways that a classical computer could never do. A quantum computer can manipulate the data it stores using so-called quantum gates, or simply â€˜gatesâ€™ for short. These are rapid bursts of some physical forces that change the state of one or more qubits. They can turn a classical-looking state into a quantum superposition or vice versa. They can act like logical operations, like the AND and OR gates that are used in classical electronics, but also like new quantum logic that has no classical counterpart. From a functional perspective, a quantum gate takes one or more qubits as input, changes their internal state, and then outputs the same number of qubits (with their altered states). In other words, the number of physical objects remains unchanged, but the overall state changes. As an example, you may think of our prototypical magnet that was initially pointing â€˜upâ€™, but a quantum gate might flip this to â€˜downâ€™. There are many such gates possible, each having a different effect on their input. We like to give them names in capital letters, such as X, Z, H, and CX. Importantly, a quantum gate is deterministic, meaning that its input-output behaviour is always the same, as opposed to the quantum measurements we saw earlier. The canonical way to describe a quantum computer program is by defining a sequence of quantum gates, where for each gate, we also indicate what qubits are supposed to be the gateâ€™s input. At the end of the computation, we measure all qubits. Below, an example of such a list is given, using the standard Quantum Assembly (QASM) language. Together, these steps can be graphically displayed in a quantum circuit, as shown here on the right. Quantum circuits represent each qubit with a horizontal line and indicate time flowing from left to right. Whenever a box with a letter is displayed over a qubit line, then the corresponding gate should be applied. This isnâ€™t unlike the way we read sheet music! You may notice that sometimes, two or more gates can be performed in parallel as long as they act on different qubits. When we run a circuit on an actual quantum computer, the final measurements lead to probabilistic outcomes. We get to see a bunch of ones and zeroes: one classical bit for each qubit. If the circuit was a good quantum algorithm, then with high probability, these classical bits will tell us the answer we were looking for. But even then, we might need to redo the computation a few times and take (for example) the most common result as our final answer. If you are completely confused at this point, you are not alone. The whole business of quantum superposition and quantum operations is incredibly complex and is not something you could possibly master after reading a few pages. Scientists who have studied the subject for many years are still frequently baffled by deceptive paradoxes and counter-intuitive phenomena. On the other hand, I hope that the functionality of quantum circuits makes some sense: we define a list of instructions and feed them into a machine that can execute them. We donâ€™t have to know precisely whatâ€™s going on under the hood! . There is one remaining quantum phenomenon to cover â€“ one that comes with a mysterious flair surrounding it. Weâ€™re talking about quantum entanglement, which weâ€™ll describe using the following example. Imagine that we have two qubits, which we can transport independently from each other without disturbing the data they store. Together, the bits can represent the states 00, 01, 10 or 11, or any superposition of these. According to quantum mechanics, we can create a very specific state where the pair of qubits is simultaneously 00 and 11. Now, imagine that computer scientist Alice grabs one of the qubits, takes it on her rocket ship, and flies it all the way to dwarf planet Pluto. The other qubit remains on Earth in the hands of physicist Bob. Upon arriving on Pluto, Alice measures her qubit and finds outcome â€˜1â€™. A deep question is: what do we now know about Bobâ€™s qubit? . Since the only possible measurement outcomes were 00 and 11, the other qubit can only be measured as â€˜1â€™ from now onwards. It essentially collapses to be 100% in the state â€˜1â€™. But how could the earth-based qubit possibly know that a measurement occurred on Pluto? What mechanism made it collapse? According to Einsteinâ€™s theory of relativity, information cannot travel faster than the speed of light, which translates into a few hours between Earth and Pluto. Nevertheless, measuring the qubits inÂ two faraway locations will always give a consistent result, even when the two qubits are measured at exactly the same time. This paradox shows once again how confusing quantum mechanics can be. However, the story above is perfectly consistent with both quantum mechanics and the theory of relativity. The core principle is that no information can be sent faster than light between Alice and Bob. For example, can you see why Bob has no way of detecting when Alice performs her measurement just by looking at his entangled qubit? In the most common interpretation of quantum mechanics, the Earth qubit does indeed change its state instantaneously when Alice measures, although there is no way to exploit this effect. Thereâ€™s a fascinating further discussion about the philosophy behind entanglement, but weâ€™ll leave that to other sources. What matters to us is that distant qubits can share specific properties that would be impossible to mimic classically, leading to new functionalities we can exploit. We will discover what these functionalities are in the chapter on quantum networks. So there you have it: four surprising phenomena you may hear frequently in quantum technology conversations. To summarise: . | Superposition: the phenomenon where a qubit is both 0 and 1 at the same time. | Quantum measurement: measuring a quantum memory destroys superposition. The result we obtain is probabilistic. | Quantum gates: deterministic changes to the state of qubits, which generalise classical logic gates like OR, AND, NOT. A list of several quantum gates (together with the qubits they act on) forms a quantum circuit. | Entanglement: Qubits separated over a long distance can still share unique properties. | . ",
    "url": "/essentials/quantum/#four-surprising-phenomena",
    
    "relUrl": "/essentials/quantum/#four-surprising-phenomena"
  },"24": {
    "doc": "An introduction to the quantum world",
    "title": "What does a quantum computer look like?",
    "content": "Most large-scale computing today happens in data centres, where we donâ€™t care much about the specifics of the devices that do our calculations. We also expect that future quantum computers will mostly be tucked away in the â€˜cloudâ€™, making their appearance and inner workings largely irrelevant to most users. However, for this optional chapter, we can take the opportunity to view what todayâ€™s cutting-edge hardware looks like. There are many different ways to build a quantum computer, each working in vastly different ways. Here, we describe the example of so-called superconducting qubits, a relatively mature platform used by companies like IBM, Google and Rigetti and several academic institutes. Research institute QuTech in Delft, The Netherlands, was so kind to provide photos that allow us to look inside their labs. We will see that only a tiny part of the computer is actually â€˜quantumâ€™, whereas most of the machine consists of bulky classical machinery thatâ€™s required to keep the computer working. A quantum chip. Photo credits: Marc Blommaert for QuTech. The real quantum magic happens on a chip, not unlike the computer chips used in your laptop or phone. The qubits are formed by tiny electronic circuits where the flow of electrical current is restricted to just one out of two states: the â€˜bitâ€™ states 0 and 1. Since this is a quantum system, the current can also be in a superposition â€“ picture all the electrons in the wire participating both in flow â€˜0â€™ and flow â€˜1â€™ simultaneously! This only works when the chip is cooled down to unimaginably low temperatures, down to around 10 milliKelvin â€“ a hundredth of a degree above absolute zero. At these temperatures, the electronic circuits become superconducting, such that an initial current can flow indefinitely. This is important because any damping of the current would cause unwanted disturbance to the qubit state. The temperature constraint is why the quantum chip is placed in a massive dilution refrigerator, a cylinder of about half a meter in diameter and over a meter tall, which specialises in keeping the quantum chip cool. In the future, larger quantum computers may need even bigger fridges or have several of these close together. Deeper parts of the fridge have different temperatures, allowing us to cool in stages. An example could be to cool a first environment to 35 Kelvin (-283 Â°Celsius or -396.7 Â°Fahrenheit), followed by subsequent stages to ~3K, 900mK, 100mK, until the final stage of ~10mK is reached. The interior of a dilution fridge, as used for superconducting quantum computers. Photo credits: Marc Blommaert for QuTech. Engineers typically suspend the fridge on the ceiling so that the higher temperatures are on top, and the ultracold quantum chip is placed at the very bottom. The internals are shaped accordingly: several layers of gold disks are suspended below one another, one disk for each temperature zone. A large number of wires run between the disks, transporting signals between the ceiling and the lowermost areas. The whole structure forms the iconic metal chandelier that you often see in images, although it would all be covered by a boring metal case when the fridge is in operation. To make the qubits do something useful, like executing a quantum gate or performing a measurement, we need to send signals into the chip. Just like in classical computers, these signals are technically voltage differences between various wires. Some voltages remain constant over time, others oscillate at microwave frequencies. The wiring itself becomes increasingly challenging for larger quantum computers for two reasons. Firstly, we currently need around 2-4 wires to control a single qubit, which is problematic when we scale to millions of qubits â€“ itâ€™s impossible to connect that many wires to a tiny chip. Weâ€™ll need to find â€˜multiplexingâ€™ solutions, where a single wire can serve multiple qubits at once. Secondly, many wires connect the ultracold chip to other hardware that sits at room temperature, forming a channel for heat and noise to enter. The dilution fridge circumvents this by incrementally cooling and damping the signals as they travel through the different layers of the fridge, but it can only handle so many cables. A stack of classical control electronics used to generate and measure electronic signals. Photo credits: Marc Blommaert for QuTech. Besides the large chandelier, an array of specialised control electronics is needed to produce the necessary electronic pulses and to carefully read out the tiny signals that qubits produce when we measure them. These devices sit in one or multiple electronics racks, each half a meter wide and nearly two meters tall, similar to the ones youâ€™ll find in a typical data centre. Ironically, the actual quantum software can be written on a simple laptop, from where the instructions are passed to the control electronics to run a quantum circuit. The whole situation is reminiscent of computers in the 1940s and 1950s, which similarly occupied a large room and required several engineers for all kinds of laborious manual maintenance tasks. On top of this, the dilution fridges are particularly noisy â€“ to the extent that those who operate them ideally do this from a different room â€“ and are fairly power-hungry. The quantum computer described above consumes around 25 kW, comparable to driving an electric car. To summarise, a quantum computer based on superconducting qubits consists mostly of bulky classical stuff: a large cylindrical fridge and a rack full of classical electronic devices, all of which work together to keep the microscopic qubits in the coldest part of the fridge working. ",
    "url": "/essentials/quantum/#what-does-a-quantum-computer-look-like",
    
    "relUrl": "/essentials/quantum/#what-does-a-quantum-computer-look-like"
  },"25": {
    "doc": "An introduction to the quantum world",
    "title": "Further reading",
    "content": "If youâ€™d like to know more about the physics and math behind qubits, we recommend the following sources: . | Quantum CountryÂ â€“ a great online textbook about Quantum Computing byÂ Andy MatuschakÂ andÂ Michael Nielsen. | QuTech Academyâ€™s School of QuantumÂ explains a broad range of quantum topics using short videos.Â  . | (Youtube) A video tour that looks inside IBMâ€™s superconducting quantum computer. | . ",
    "url": "/essentials/quantum/#further-reading",
    
    "relUrl": "/essentials/quantum/#further-reading"
  },"26": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "Timelines: when can we expect a useful quantum computer?",
    "content": "Reading time: 24 minutes . Contents . | What parameters are relevant? | How many qubits are needed? | How long until we have million-qubit machines? | Putting it all together | Further reading | . At a glance The earliest commercial quantum applications will need several million qubits, according to the most rigorous studies. Assuming an exponential growth similar to Mooreâ€™s Law, we predict that the first applications could be within reach around 2035-2040. The billion-dollar question in our field is: . When will quantum computers outperform conventional computers on relevant problems? . In the previous chapter, we defined the requirements more precisely and coined the term â€˜utilityâ€™ for such an achievement. Unfortunately, nobody can confidently answer this question today, and past predictions often proved inaccurate. Moreover, a relevant quantum computer wonâ€™t just appear from one day to another: thereâ€™s a continuous evolution where these devices will become increasingly capable. In this chapter, we will show how we can make a rough prediction about future timelines and discuss what will happen on the path towards large-scale quantum computation. As an important disclaimer, this chapter is highly subjective. Itâ€™s not hard to arrive at different conclusions simply by choosing other sources and making different assumptions. I did my utmost best to rely on the most up-to-date information, combining the views of the most widely accepted papers and making assumptions that align with the view of most experts to present a balanced perspective. ",
    "url": "/essentials/timelines/#timelines-when-can-we-expect-a-useful-quantum-computer",
    
    "relUrl": "/essentials/timelines/#timelines-when-can-we-expect-a-useful-quantum-computer"
  },"27": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "What parameters are relevant?",
    "content": "Compared to currently available technology, weâ€™d require a fundamental improvement to these specifications: . | Number of qubits . | Accuracy of elementary operations (gates). This means that quantum computers have the ability to perform long computations without making mistakes. | . Quite a few other parameters matter, such as the connectivity, the available set of gates, the speed of operations, and so forth. In this chapter, we choose to simplify matters by assuming that all of these other parameters are not a bottleneck, allowing us to focus only on the number of qubits and gate accuracies. The relevance of accuracy is often overlooked, perhaps because this hardly plays a role for classical computers anymore. The problem is as follows. A computation consists of many small, discrete steps called quantum gates. Unfortunately, even the most precisely engineered quantum computers are imperfect, and every gate has a slight chance of introducing an error. You can picture this intuitively as a qubit accidentally flipping from â€˜0â€™ to â€˜1â€™ or vice versa1. The probability that a gate introduces such an error on todayâ€™s hardware is around 0.1% to 1%. Sometimes the term â€˜accuracyâ€™ or â€˜fidelityâ€™ is used for the probability of not making an error, translating into numbers like 99.9%. Now, a serious computation can easily use billions of gates. You can hopefully see the issue here: for long calculations on current hardware, the output is almost certainly garbled by errors. In fact, given a certain error gate fidelity, there is a ballpark maximum number of steps that can be reasonably performed. With a 1:1000 probability of error, we can do roughly a thousand steps, and if the error is one in a million, we can do approximately a million gates. To solve increasingly complex problems, we do not only need to increase the number of qubits, but we also need to reduce the likelihood of errors. We should take a moment to appreciate the enormous challenge ahead of us. It took decades of engineering to minimise errors to about one in a thousand. Now, we should bring this rate down to one in billions. Thatâ€™s a huge gap that likely cannot be covered by hardware improvements alone â€“ even a breakthrough that reduces errors by 100x wouldnâ€™t cut it. Balancing qubits and accuracies . Luckily, there exists a technique that shrinks the probability of mistakes by any desired amount: error correction. It works roughly as follows. For every qubit that an algorithm requires, we donâ€™t just build a single hardware qubit, but instead, we dedicate a large number of qubits, like a hundred or a thousand. Weâ€™ll use the term physical qubits for the actual qubits present in the hardware, whereas the virtual error-mitigated ones are named logical qubits. For example, suppose we have a device with a million physical qubits. In that case, we might group a hundred of these to form a more error-resilient logical qubit, leaving a programmer with just 10.000 logical qubits to use. The image below shows a similar situation with a ratio of 1:12 between logical and physical qubits. For error correction to work, we need to make several assumptions. For example, depending on the precise error correction protocol, gate fidelities need to be quite good to start with â€“ numbers like 99.99% are often mentioned. This means that, as of 2024, the worldâ€™s best devices would still need to improve gate fidelities by more or less a factor of \\(10\\). Moreover, qubits need to be routinely measured and reset, and large amounts of classical processing are needed to deduce precisely how to repair a given error. These are significant engineering challenges, but experts are optimistic that this can be achieved. We discuss many more details in a separate chapter on error correction. For now, letâ€™s take for granted that we can somehow reach any desired accuracy (or any desired computation length) by simply adding sufficiently many physical qubits. Then, we can greatly simplify our analysis! For each application, we will forget about errors altogether and only count the number of physical qubits needed. This leads to an interesting situation. To solve larger, more complex problems, weâ€™ll need more qubits for two reasons: to store more data and to reduce the probability of errors so that the computation can run longer. Isnâ€™t the focus on just qubits a bit short-sighted? Doesnâ€™t this create a perverse incentive for manufacturers to focus only on qubit numbers, forgetting about all the other parameters? Well, I would indeed be worried that some companies can make headlines with unusable computers that happen to have a record qubit number. Luckily, most manufacturers seem dedicated to making the most â€˜usefulâ€™ computers, and customers will surely judge their products by the capabilities of their logical qubits. Weâ€™re obviously making a coarse simplification here, but making predictions about the future is hard enough as it is. Back to the main question: When can we expect a large quantum computer? Now that weâ€™re only counting qubits, we can break our billion-dollar question into two parts: . | How many qubits are needed? . | In what year will that many qubits be available? . | . ",
    "url": "/essentials/timelines/#what-parameters-are-relevant",
    
    "relUrl": "/essentials/timelines/#what-parameters-are-relevant"
  },"28": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "How many qubits are needed?",
    "content": "In the previous chapter, we discussed the three main applications of quantum computers: quantum simulation, breaking cryptography, and optimisation. The most concrete numbers can be given forÂ Shorâ€™s algorithmÂ (breaking cryptography), where we have a very clear problem to tackle: obtain a private (secret) key from a widely-used cryptosystem, like the RSA-2048 protocol. This is the perfect benchmark because there can be no discussion about whether the problem is solved: one either obtains the correct key or one doesnâ€™t. Moreover, weâ€™re quite convinced that even the best classical computers canâ€™t solve the problemÂ (or else you shouldnâ€™t use internet banking or trust software updates).Â  . AÂ recent estimateÂ finds that a plausible quantum computer would require roughlyÂ 20 millionÂ â€˜reasonably goodâ€™ physical qubits to factor a 2048-bit number. The whole computation would take about 8 hours2. Such estimates require several assumptions on what a future quantum computer would look like. In this case, the authors assume qubits are built using superconducting circuits, which are laid out in a square grid. Error correction is assumed to be done using the so-called surface code, assuming the best-known methods for error correction in 2020. Note that future breakthroughs could reduce the required time and number of qubits even further. ForÂ chemistry and material simulation, itâ€™s a lot harder to make such estimates because there is not just a single problem to tackle here: one typically uses computers to gradually improve our understanding of a complex structure or chemical reaction. This should be combined with theoretical reasoning and practical experiments. Moreover, classical computers can often perform the same computations that the quantum computer would make at the cost of making certain assumptions or simplifications. Thereâ€™s a fuzzy region between â€˜classically tractableâ€™ and â€˜quantum advantageâ€™. The most concrete task in quantum simulation is to compute the energy of certain molecular configurations. The benchmark is to obtain energies more accurately than done in conventional experiments; one canonically takes the â€˜chemical accuracyâ€™ of roughly 1 kcal/mol as the precision to beat. Then, we should focus on molecules where classical computers cannot already achieve such accuracies. Â Â  . Note that the accuracy of a chemical energy should not be confused with the accuracy of a quantum gate, which is a whole different number. A highly promising and well-studied benchmark problem is the simulation of the so-called FeMo cofactor of the nitrogenase enzyme, in short, FeMoco. This active site is relevant when bacteria produce Ammonia (NH3), a compound that is of great relevance to a plantâ€™s root system. A better understanding of this process could help us reduce theÂ ridiculously large carbon emissionsÂ now associated with the production of artificial fertiliser.Â We give more details in a separate chapter. Simulating FeMoco is believedÂ to require aroundÂ 4 million qubits3Â (and around 4 days of computation time). The hardware and error correction assumptions are similar to those of Shorâ€™s algorithm: the estimate is based on a square grid of superconducting qubits, using surface code to correct errors. For a different enzyme, namely cytochrome P450, itÂ has been estimated that aroundÂ 5 millionÂ qubits are needed4Â (again taking roughly 4 days of computation). Altogether, we conclude that a few million qubits (of sufficiently high quality) can make quantum computers relevant for R&amp;D in chemistry.Â  . Some tasks that are mainly of interest for scientific purposes, such as simulating models of quantum magnets, can be achieved with fewer resources. Under similar assumptions, simulating a 2D transverse field Ising model is estimated to take just under 1 million qubits5. For manyÂ optimisation problems, itâ€™s practically impossible to give reasonable estimates. As we saw previously, a true killer algorithm for optimisation problems is not known yet. The algorithms that are presented as the most promising are oftenÂ heuristic,Â meaning that itâ€™s hard to predict how accurate their results will be compared to conventional methods. Weâ€™ll need to test them in rigorous benchmarks once larger quantum computers become available. Our perspective starkly contrasts some other sources claiming that quantum computers are already solving practical problems today. But donâ€™t be fooled: these articles state that quantum computersÂ canÂ indeed solve relatively simple problems but often fail to mention that there existÂ differentÂ approaches by which classical computers can solve the same problems much, much faster.Â  . Moreover, many of these algorithms involve optimisation problems that have a plethora of potential solutions, but the goal is to find theÂ optimalÂ solution (say, the one that incurs the least costs or gets you to your destination the fastest). The solution space is often so large that we donâ€™t even know if we hit this optimal solution, but weâ€™re okay with finding one thatâ€™sÂ pretty close.Â Several papers claim that a quantum computer already finds solutionsÂ faster, but in all cases, worrying sacrifices were made in the optimality of the solutions for more complex problems. What about D-Waveâ€™s quantum annealer? A particularly difficult case is the approach taken by D-Wave. This Canadian scale-up manufactures a quantum computer that is purpose-built to execute a specific optimisation algorithm called quantum annealing. With around 5000 qubits, it can handle reasonably large problems. The bare hardware alone doesnâ€™t seem to perform that well, but D-Wave cleverly combines it with classical high-performance computing in what they call a â€˜hybridâ€™ solver. Comparisons and benchmarks of the hybrid solution report results ranging from â€˜much worseâ€™ to â€˜very competitiveâ€™ relative to classical optimisation solutions. Because it is unclear to what extent the hybrid solver actually exploits quantum phenomena and little is known about D-Waveâ€™s future plans, I donâ€™t dare to make any future predictions about annealing. See also: . | D-Wave claims a scaling advantage when simulating quantum materials. | ETH Zurich researchers conclude that the 2015 version of D-Waveâ€™s annealer is comparable to a modern high-performance CPU. | Los Alamos researchers find annealing speedups in certain cases, but also note challenges to commercial adoption. | Researchers report that D-Waveâ€™s bare quantum hardware is quite slow, but the hybrid solution is very competitive with classical optimisation techniques. | . We can summarise our conclusions in the table below. | Application | How well can we estimate qubit requirements? | Use case example | Physical qubits needed | Gate error assumed | . | Breaking cryptography | Good | Cracking RSA-2048 | ~ 20 million | ~ 0.1% | . | Chemistry | Reasonable | Simulation of FeMoco | ~ 4 million | ~ 0.1% | . | Â  | Â  | Simulation of P450 | ~ 5 million | ~ 0.1 % | . | Optimisation / AI | Bad | ? | ? | Â  | . What about future improvements? . It seems almost inevitable that the above methodologies will improve. Unfortunately, itâ€™s impossible to estimate by how much. Will we reduce the number of qubits required by a few per cent? Or by a factor of ten? By a factor of one thousand? . Some sources actually try to extrapolate the reduction in required qubits over time (like Youtube science educator Veritasium6 and a report by McKinsey7), but this is such a wonky extrapolation over a handful of data points that we will not follow this strategy. On the other hand, it would also be naive to stick to the numbers above without assuming some margin for improvements. In error correction techniques alone, there appears to be steady progress to improve the ratio between logical and physical qubits. Based on discussions with scientists, lowering the qubit requirements by a factor of 3 to 10 seems plausible. Hence, for optimistic readers, we can set another target at around 400.000 qubits. Interestingly, this number is similar to the qubit requirements for the simulation of models that are especially of scientific interest. | Application | How well can we estimate qubit requirements? | Use case example | Qubits needed? | Gate error assumed? | . | Chemistry (Optimistic) | Reasonable | Simulation of FeMoco (with 10x improved methods) | ~ 400.000 | ~ 0.1 % | . | Science | Reasonable | 2D Transverse field Ising model | ~ 900.000 | ~ 0.1 % | . Can noisy algorithms be good enough? . Current quantum computers have a limited number of qubits and are not yet capable of large-scale error correction; they are Noisy Intermediate-Scale Quantum (NISQ) devices. An important question is: can we already achieve any utility with such noisy devices before the era of large-scale error correction? That is one of the most disputed topics in our field, and therefore it deserves some attention. A growing community of scientists, startups and enterprises are searching for such near-term applications. If successful, this would massively increase the overall usefulness of quantum computers. Some experts seem optimistic that this is possible, but a larger and more authoritative group remains highly sceptical about NISQâ€™s utility. In the past decades, when NISQ devices with just a handful of qubits were just on the horizon, several consultants made ridiculous claims about how such tiny machines would bring an exponential advantage over enormous supercomputers. Now that the field is coming of age, many are becoming more careful. To illustrate, when looking back at a 2021 report, consultancy firm BCG chivalrously admits8: . â€˜Our assumptions for near-term value creation in the NISQ era, however, have proved optimistic and must be revised.â€™ . The most serious recent claim about NISQ utility comes from the IBM team in a paper titled â€˜Evidence for the utility of quantum computing before fault-toleranceâ€™9, in which a quantum simulation of a specific physical system was performed using 127 noisy qubits. However, their arguments were quickly refuted by further studies that simulated IBMâ€™s impressive quantum experiment on a conventional laptop10. Maryland-based professor Sankar Das Darma expresses the view of many academics in his opinion article â€˜Quantum computing has a hype problemâ€™11. He stresses about NISQ that â€˜the commercialisation potential is far from clearâ€™, pointing out that claims of speedups in finance, machine learning and drug discovery have so far come with highly unsatisfying evidence. That certainly doesnâ€™t mean that NISQ utility is ruled out. Most experts seem to keep an eye on the developments of NISQ applications but will agree that no utility for NISQ machines has been found yet. To illustrate, an overview article about pharmaceutical applications12 has a careful yet suggestive message: . â€˜Most NISQ algorithms [â€¦] rely heavily on classical optimisation heuristics, and the actual run time is difficult to estimate. Furthermore, recent results suggest that in NISQ approaches, the number of measurements required to achieve a given error scales exponentially with the depth of the circuit. For these reasons, here we focus our discussion exclusively on fault-tolerant quantum computers.â€™ . Similarly, a recent overview13 of quantum chemistry seems to remain agnostic with regard to NISQ advantage while pointing out that fault-tolerance has a higher chance of succeeding. â€˜â€¦ it is difficult to predict when or if algorithms on near-term noisy intermediate-scale quantum devices will outperform classical computers for useful tasks. But it is likely that, at some point, the achievement of large-scale quantum error correction will enable the deployment of a host of so-called error-corrected quantum algorithms.â€™ . In this book, I choose to follow the view of most scientists and stick to the well-understood use cases for early fault-tolerant quantum computers that we discussed previously. Nobody can rule out new breakthroughs that allow NISQ utility, but it seems unwise to count on these. A potential scientific leap could completely stir up our fragile prediction â€“ but so would unexpected backlashes in hardware development or even unforeseen funding stops. ",
    "url": "/essentials/timelines/#how-many-qubits-are-needed",
    
    "relUrl": "/essentials/timelines/#how-many-qubits-are-needed"
  },"29": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "How long until we have million-qubit machines?",
    "content": "Now that weâ€™ve set our target to roughly a million qubits, weâ€™d like to estimate when such hardware will be available. We highlight the following sources: . | Road maps and claims of hardware manufacturers . | Surveys to experts . | Extrapolation of Mooreâ€™s law . | . What do manufacturers say? . Below, we see the qubit numbers that several manufacturers have already realised (solid disks) and what they will produce in the future according to their public road maps (opaque plusses). Note that the vertical axis is logarithmic, displaying a very broad range from around 10 to 10.000 qubits. A lower number of qubits does by no means indicate that these computers are worse. In fact, the machines with the lower numbers of qubits on this graph have an important edge in other parameters, such as gate accuracies and qubit connectivity. Figure: The number of qubits in the most mature quantum computers from a selection of various manufacturers, as of 2024. Besides their road maps, companies sometimes make more daring claims in media interviews or at presentations at large events. Based on the application targets above, it should be no surprise that manufacturers aim for around a million qubits as a â€˜moonshotâ€™ accomplishment. Back in 2020, IBM claimed to reach the 1 million qubit target by 203014. Around the same time, Google was interpreted by journalists to do this even faster (around 202915). The start-up PsiQuantum, which made waves thanks to record-high investments of over a billion dollars for their photonic quantum chips, went as far as claiming to have a million qubits by 202516 17. It seems that these claims were a bit too ambitious. In 2024, with only a year to go and no publicly presented product progression, PsiQuantum shifted its 1 million qubit road map to 202718. IBM took an even more conservative step, where itâ€™s now claiming to have just 100.000 qubits in 203319 (although this machine should meet the error correction assumptions that we dreamingly assumed in the previous sections). Although this delay sounds disappointing, hardware manufacturers are still making impressive progress, as the number of available qubits grows faster than one would predict according to Mooreâ€™s Law for classical chips! . Trapped-ion machines tend to have fewer qubits but higher gate accuracies. Perhaps this is why IonQÂ displays its road map in a different format: they aim to achieve 1024Â so-called algorithmic qubitsÂ by 202820. This means that IonQ will haveÂ at leastÂ this number of qubits, but also guarantees sufficient gate accuracy to run reasonably long circuits. Itâ€™s unclear whether error correction will be used for this. Competitor Quantinuum recently announced a more concrete road map21, predicting around 100 logical qubits in 2027. These should bring the effective gate errors down by roughly a factor of 10. Looking ahead to 2029, Quantinuum projects 1000â€™s of physical qubits that form 100â€™s of logical qubits. This might not be enough to run the algorithms discussed earlier, but itâ€™s not too far off either. What does Mooreâ€™s law say? . One could assume that quantum computers will â€˜growâ€™ at a similar rate as classical computers. Mooreâ€™s law states that the number of transistors in a dense integrated circuit grows exponentially: the number doubles roughly every two years. This has been a surprisingly accurate predictor for the development of classical IT. If we apply Mooreâ€™s law to quantum, then boosting qubit numbers from around a thousand to one million would take around 20 years â€“ predicting that the one million qubit mark wonâ€™t be passed until 2044. Clearly, most hardware manufacturers are more optimistic. If we assume the number of qubits doubles each year, one would predict that one million qubits will be available in ten years. While doubling a quantum computerâ€™s size each year is already a daunting challenge, companies like IBM, Pasqal and QuEra set the bar even higher for themselves, hoping to double every 7-9 months. What do experts say? . The Global Risk Institute conducts yearlyÂ surveys asking experts to state theÂ likelihoodÂ that quantum computers will pose a significant threat to public key cryptography 5 years from now. Similarly, respondents would also estimate the likeliness 10, 15, 20, and 30 years away. This essentially boils down to the question: when will a quantum computer run Shorâ€™s algorithm to crack RSA-2048?Â We previously saw that around 20 million qubits would be needed for this (although experts may take into account that this number can still be lowered). We consider this an important source because many important authorities in the field (like professors and corporate leaders) take part in this study. The results from December 202322, gathered from 37 participants, are displayed below. Figure: Results of the 2023 expert survey by Global Risk Institute (source: globalriskinstitute.org). Image rights belong to Global Risk Institute. How to read this graph? Letâ€™s look at the column labelled â€˜5 yearsâ€™. A total of 24 correspondents indicate that there is less than 1% probability that quantum computers pose a security threat in the next five years. A single person is quite pessimistic and assigns a &gt;70% chance that this will happen. On average, experts say that thereâ€™s a fairly small likelihood that quantum computers will pose a threat to cryptography in the next five years.Â  . Further to the right, the ratios shift. Looking at 20 years from now, the majority of experts believe that quantum computers pose a serious threat, with over half of them assigning a likelihood of 70% or more. It appears that the majority of experts believe that the tipping point is between 10-20 years from now. Somewhere between 15 and 20 years away, thereâ€™s a point where the median participant assigned roughly 50% chance to see a quantum computer capable of breaking cryptographic codes.Â However, we should take into account a significant uncertainty: even experts make wildly varying estimates, so thereâ€™s no obvious conclusion from this data. These experts are likely aware of hardware manufacturerâ€™s road maps, as we shall see below. ",
    "url": "/essentials/timelines/#how-long-until-we-have-million-qubit-machines",
    
    "relUrl": "/essentials/timelines/#how-long-until-we-have-million-qubit-machines"
  },"30": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "Putting it all together",
    "content": "The infographic below sums up our earlier findings. Assuming that qubit numbers will grow exponentially (and that all other parameters will keep up accordingly), we can consider several scenarios. A pessimistic scenario would be that the number of qubits â€˜merelyâ€™ follows the classical version of Mooreâ€™s Law, and qubit numbers double only once every two years (dotted line). Then, weâ€™d have to wait well past 2040 to reach 100.000 qubits. An even worse scenario would be if we cannot achieve exponential growth, which would stretch the timelines even further. An extremely optimistic outlook would follow the blue dashed line (which extrapolates the progress by IBM, doubling their qubits every ~9 months). If one also believes in practical applications with much less than a million qubits, then these could be available by 2030. An intermediate perspective is to assume that the number of qubits doubles annually. Interestingly, this seems to approximately align IBMâ€™s latest claims and the typical expert opinion. Depending on the application, it would mean that quantum chemistry simulation and codebreaking can be within reach between ~2033 and 2040. To conclude, our estimates strongly depend on the assumptions that youâ€™re willing to accept (who wouldâ€™ve thought!). Do you believe that improving algorithms and error correction techniques will allow for applications with much less than a million qubits? How quickly do you believe that the hardware will improve? If you force me to make a prediction, Iâ€™d say the first applications arise around 2035, with the understanding that thereâ€™s a considerable margin for error. As a final remark, a full utility-scale quantum computer requires much more than just some number of qubits. To reach the first useful applications, we likely require simultaneous progress in algorithmics, software, gate accuracies, error correction techniques, fridges, lasers, and many other important subfields of quantum computing. Hopefully, all these disciplines will find the required breakthroughs that will sustain the exponential growth of quantum computing hardware. ",
    "url": "/essentials/timelines/#putting-it-all-together",
    
    "relUrl": "/essentials/timelines/#putting-it-all-together"
  },"31": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "Further reading",
    "content": ". | Scientist Samuel Jaques (Waterloo) makes insightful graphs that combine the number of qubits and the error rates, and puts them in the perspective of applications requirements. | . | Technically, quantum gates are continuous operations, so numbers like fidelity are defined slightly differently. Still, the picture of discrete bit flips is not too much off and will lead to the same conclusions, so I prefer this more accessible explanation.Â &#8617; . | Gidney, Craig, and Martin EkerÃ¥. â€˜How to Factor 2048 Bit RSA Integers in 8 Hours Using 20 Million Noisy Qubits.â€™ Quantum 5 (April 15, 2021): 433. https://doi.org/10.22331/q-2021-04-15-433.Â &#8617; . | Lee, Joonho, Dominic W. Berry, Craig Gidney, William J. Huggins, Jarrod R. McClean, Nathan Wiebe, and Ryan Babbush. â€˜Even More Efficient Quantum Computations of Chemistry Through Tensor Hypercontractionâ€™. PRX Quantum 2, no. 3 (8 July 2021): 030305. https://doi.org/10.1103/PRXQuantum.2.030305.Â &#8617; . | Goings, Joshua J., Alec White, Joonho Lee, Christofer S. Tautermann, Matthias Degroote, Craig Gidney, Toru Shiozaki, Ryan Babbush, and Nicholas C. Rubin. â€˜Reliably Assessing the Electronic Structure of Cytochrome P450 on Todayâ€™s Classical Computers and Tomorrowâ€™s Quantum Computersâ€™. Proceedings of the National Academy of Sciences 119, no. 38 (20 September 2022): e2203533119. https://doi.org/10.1073/pnas.2203533119.Â &#8617; . | Beverland, M.E. et al. (2022) â€˜Assessing requirements to scale to practical quantum advantageâ€™. arXiv. https://doi.org/10.48550/arXiv.2211.07629.Â &#8617; . | See https://www.youtube.com/watch?v=-UrdExQW0cs&amp;t=1024s, starting at 17:04.Â &#8617; . | McKinsey Digital. â€˜Quantum Technology Monitor,â€™ April 2024. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/steady-progress-in-approaching-the-quantum-advantage.Â &#8617; . | BCG Global. â€˜The Long-Term Forecast for Quantum Computing Still Looks Bright,â€™ July 3, 2024. https://www.bcg.com/publications/2024/long-term-forecast-for-quantum-computing-still-looks-bright.Â &#8617; . | Kim, Youngseok, Andrew Eddins, Sajant Anand, Ken Xuan Wei, Ewout van den Berg, Sami Rosenblatt, Hasan Nayfeh, et al. â€˜Evidence for the Utility of Quantum Computing before Fault Tolerance.â€™ Nature 618, no. 7965 (June 2023): 500â€“505. https://doi.org/10.1038/s41586-023-06096-3.Â &#8617; . | BeguÅ¡iÄ‡, Tomislav, and Garnet Kin-Lic Chan. â€˜Fast Classical Simulation of Evidence for the Utility of Quantum Computing before Fault Tolerance.â€™ arXiv, June 28, 2023. https://doi.org/10.48550/arXiv.2306.16372.Â &#8617; . | Das Sarma, Sankar. â€˜Quantum Computing Has a Hype Problem.â€™ MIT Technology Review (blog). Accessed September 26, 2024. https://www.technologyreview.com/2022/03/28/1048355/quantum-computing-has-a-hype-problem/.Â &#8617; . | Santagati, Raffaele, Alan Aspuru-Guzik, Ryan Babbush, Matthias Degroote, Leticia GonzÃ¡lez, Elica Kyoseva, Nikolaj Moll, et al. â€˜Drug Design on Quantum Computers.â€™ Nature Physics 20, no. 4 (April 2024): 549â€“57. https://doi.org/10.1038/s41567-024-02411-5.Â &#8617; . | Cao, Yudong, Jonathan Romero, Jonathan P. Olson, Matthias Degroote, Peter D. Johnson, MÃ¡ria KieferovÃ¡, Ian D. Kivlichan, et al. â€˜Quantum Chemistry in the Age of Quantum Computing.â€™ Chemical Reviews 119, no. 19 (October 9, 2019): 10856â€“915. https://doi.org/10.1021/acs.chemrev.8b00803.Â &#8617; . | Hackett, Robert. â€˜IBM Plans a Huge Leap in Superfast Quantum Computing by 2023.â€™ Fortune. Accessed September 26, 2024. https://fortune.com/2020/09/15/ibm-quantum-computer-1-million-qubits-by-2030/.Â &#8617; . | Finke, Doug. â€˜Google Goal: Build an Error Corrected Computer with 1 Million Physical Qubits by the End of the Decade.â€™ Quantum Computing Report (blog), September 5, 2020. https://quantumcomputingreport.com/google-goal-error-corrected-computer-with-1-million-physical-qubits-by-the-end-of-the-decade/.Â &#8617; . | Wang, Brian. â€˜PsiQuantum Targets Million Silicon Photonic Qubits by 2025â€™, April 23, 2020. https://www.nextbigfuture.com/2020/04/psiquantum-targets-million-silicon-photonic-qubits-by-2025.html.Â &#8617; . | ICV TAnK-icv. â€˜What Will Million-Qubit Computers Look like in a Few Years?â€™, 21 March 2022. https://www.icvtank.com/newsinfo/629365.html.Â &#8617; . | Finke, Doug. â€˜PsiQuantum Receives \\(940 Million AUD (\\)620M USD) to Install a 1 Million Qubit Machine in Australia by 2027.â€™ Quantum Computing Report (blog), April 30, 2024. https://quantumcomputingreport.com/psiquantum-receives-940-million-aud-620m-usd-to-install-a-1-million-qubit-machine-in-australia-by-2027/.Â &#8617; . | Baker, Berenice. â€˜IBM Details Road to 100,000 Qubits by 2033.â€™ IoT World Today. Accessed September 26, 2024. https://www.iotworldtoday.com/industry/ibm-details-road-to-100-000-qubits-by-2033.Â &#8617; . | Chapman, Peter. â€˜Scaling IonQâ€™s Quantum Computers: The Roadmap.â€™ IonQ (blog), December 9, 2020. https://ionq.com/posts/december-09-2020-scaling-quantum-computer-roadmap.Â &#8617; . | Quantinuum. â€˜Quantinuum Accelerates the Path to Universal Fully Fault-Tolerant Quantum Computing,â€™ September 10, 2024. https://www.quantinuum.com/blog/quantinuum-accelerates-the-path-to-universal-fault-tolerant-quantum-computing-supports-microsofts-ai-and-quantum-powered-compute-platform-and-the-path-to-a-quantum-supercomputer.Â &#8617; . | Mosca, Michele, and Marco Piani. â€˜Quantum Threat Timeline Report 2023,â€™ 2023. https://globalriskinstitute.org/publication/2023-quantum-threat-timeline-report/.Â &#8617; . | . ",
    "url": "/essentials/timelines/#further-reading",
    
    "relUrl": "/essentials/timelines/#further-reading"
  },"32": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "The timelines: when can we expect useful quantum computers?",
    "content": " ",
    "url": "/essentials/timelines/",
    
    "relUrl": "/essentials/timelines/"
  },"33": {
    "doc": "Applications in chemistry and material science",
    "title": "Applications in chemistry and material science",
    "content": "Reading time: 16 minutes . Contents . | What problems in chemistry and material science will we solve? | Algorithms for quantum chemistry | A hype around quantum computing for climate change | A case study of a potential killer application: FeMoco | Further reading | . Perhaps the most credible application of quantum computers is to study quantum physics itself. This helps us deepen our understanding of microscopic systems like molecules, atoms, or even sub-atomic particles, ultimately leading to the discovery of new drugs, materials and chemical production methods. At first sight, there seems to be a significant advantage compared to conventional computers, which struggle to store the complex quantum state of systems with many particles. As far back as 1981, physicist Richard Feynman ended a conference talk with a famous quote, hinting at the opportunities of quantum computing1: . â€˜Iâ€™m not happy with all the analyses that go with just the classical theory, because nature isnâ€™t classical, dammit, and if you want to make a simulation of nature, youâ€™d better make it quantum mechanicalâ€™. Since then, scientists have become increasingly adept at accurately controlling quantum systems. Today, universities boast a wide spectrum of analogue quantum experiments that help us understand nature under exotic circumstances. Weâ€™re now lining up our tools to take these simulations to the next level: studying nature with digital quantum machines. In this chapter, we will assess how quantum computers can impact the fields of chemistry and material science. That makes this chapter more technical, and weâ€™ll assume some (very) basic background in chemistry and physics. We discuss the most relevant algorithms, evaluate claims about quantum computingâ€™s benefits in the fight against climate change, and analyse why the nitrogenase enzyme receives such widespread attention. ",
    "url": "/applications/chemistry/",
    
    "relUrl": "/applications/chemistry/"
  },"34": {
    "doc": "Applications in chemistry and material science",
    "title": "What problems in chemistry and material science will we solve?",
    "content": "The computational problems that chemists care about typically come in two flavours: static and dynamic problems. The most studied problem is the static variant, where the goal is to find the arrangement(s) of particles with the lowest possible energy. We call such an arrangement the ground state. These states are relevant because we usually find systems in (or close to) their lowest energy states in nature. In the context of molecules, the atomic nuclei are relatively heavy, while the lightweight electrons move much faster and are more prone to be entangled or in a quantum superposition. Therefore, chemists tend to make approximations that allow them to focus primarily on the positions and spins of the electrons: the electronic structure problem. The other main problem is about dynamics: given some initial configuration of particles, how do they reconfigure themselves after a certain amount of time? This is often referred to as a systemâ€™s (time) evolution. Both problems are informally referred to as quantum simulation. We often receive the question of why itâ€™s so hard to simulate quantum mechanics on a classical computer. Intuitively, this hardness arises when we deal with many particles that exhibit large amounts of superposition and entanglement, such that the location of one particle is heavily dependent on the (undecided) position of many other particles. We call such states strongly correlated. Classical computers struggle because they need to keep track of all the possible locations that particle A can be, but also all the locations of particle B, and the same for particle C, etcetera. As the number of particles grows, the number of possible configurations of these particles increases exponentially. This means that the number of relevant amplitudes (see the chapter on quantum physics) that a classical computer needs to process grows very quickly. Even with a mere one hundred particles, brute-force simulation is far beyond the capabilities of the worldâ€™s best supercomputers. It is a common misconception that quantum computers straightforwardly offer an exponential advantage compared to classical computers for all chemistry problems. An influential recent paper reports2: . â€˜[â€¦] we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.â€™ . Note that this comment is specifically about finding ground states, which is still arguably the most relevant problem in chemistry. There is still ample evidence that quantum computers offer an exponential speedup for time evolutions. There is more bad news for quantum computers. Over the years, computational chemists have found brilliant approximations, hacks, and optimisations to work around the classical computerâ€™s bottlenecks, raising a high bar before a quantum computer can meaningfully compete. For nearly every problem in chemistry, there appears to be a clever trick to solve it somewhat efficiently on a classical machine. For a killer application, we likely need to search in a fairly specific niche, right at the sweet spot where classical methods struggle while a quantum computer excels. It is not entirely clear how large this niche is, and it is an active research area to identify more systems where classical methods fall short. One promising area involves multi-metal systems, where multiple metal ions are close together. Such systems are present in biologically relevant enzymes such as P450 and FeMoco3. Another is in heterogeneous catalysis, where the catalyst and reagents/products are in a different phase of matter4. The first practical users of quantum simulation algorithms will most likely be scientists who study the fundamentals of quantum systems. Physicists are already employing devices that are very similar to early quantum computers to mimic certain classes of materials. We wouldnâ€™t call these devices computers yet, but rather analogue simulators. One of the first actual applications of a fully digital quantum computer could be to analyse theoretical models of quantum materials, such as the famous Hubbard model5. The first error-corrected quantum computers will hopefully find their place in industrial R&amp;D settings. One of the first application areas could be to better understand theÂ aforementioned multi-metal systems, which are relevant in theÂ calculations of ligand binding affinities in drugs and in understanding the mechanism behind the biological production of ammonia. We address the latter example at the end of this chapter. Another exciting area could be to explore the mechanism behind Type-II superconductivity and to search for materials that become superconducting at even higher temperatures6. It is hard to say what the impact of quantum computers will be beyond such niche areas, as this will depend strongly on the usefulness of small polynomial speedups and unpredictable breakthroughs in quantum algorithms. We see a broad palette of other impactful applications that have been proposed, such as photocatalytic reactions (for example, efficiently splitting water to produce hydrogen fuel)7, carbon capture mechanisms8, the study of efficient solar cells9 and the development of higher-capacity batteries10. ",
    "url": "/applications/chemistry/#what-problems-in-chemistry-and-material-science-will-we-solve",
    
    "relUrl": "/applications/chemistry/#what-problems-in-chemistry-and-material-science-will-we-solve"
  },"35": {
    "doc": "Applications in chemistry and material science",
    "title": "Algorithms for quantum chemistry",
    "content": "We describe three of the most important quantum simulation algorithms. The first is the Trotter-Suzuki method, sometimes called â€˜Trotterisationâ€™, which simulates time evolution. In this case, we assume that some correct initial state of the world is encoded in the qubits of some quantum computer. The Trotter-Suzuki method is guaranteed to return a good approximation of the state at a later time, again encoded in the qubit registers. The second algorithm is quantum phase estimation (QPE), which reports the energy of a certain quantum state and can be used to produce a systemâ€™s ground state. As a subroutine, it requires some time evolution method, like Trotter-Suzuki. Unfortunately, QPE can only provide information about a certain state if it receives an input that is already a reasonable approximation to this state. Especially in the context of describing low-energy configurations, this shifts the problem to producing good candidate ground states. The most popular algorithm for creating states with certain properties (like very low energies) is the variational quantum eigensolver (VQE). This is an example of a variational quantum circuit: a series of gates that can be gradually changed until the output matches certain requirements. Just like other variational approaches, it is a heuristic algorithm, lacking rigorous guarantees that it will produce the desired output in a reasonable time. However, it is a popular method today thanks to its ease of use and the ability to work with small, noisy computers. Creating a good approximation to a ground state is, in general, NP-hard. This means that it is extremely unlikely that a rigorous algorithm exists that can find the ground state of any quantum system. On the other hand, there is good hope that more heuristic methods (just like VQE) will be found that work well on certain subsets of systems. In fact, such heuristic methods already form the workhorse of classical computational chemistry, with tools such as Density functional theory (DFT), Configuration Interaction (CI) and Quantum Monte Carlo (QMC). These work for small systems but are often too slow to study large systems such as proteins or drugs11. A workaround is to apply these methods to just a small part of the target system, employing faster but less accurate methods to oversee the larger whole. An example of a basic workflow to find a ground state on a quantum computer could be as follows. The first step is to train a VQE to output states with low energy12. These might not be the exact ground states, but they will hopefully be very similar (in jargon, they have a large overlap with the ground state). As a second step, we append a QPE circuit, that will not only report the energy of the VQE states, but also has a fair probability of changing these states into perfect ground states (in jargon: it projects onto the ground state). Running the VQE + QPE combination a few times will almost certainly give the lowest energy states, assuming the VQE produces proper approximations of it. Further reading on simulation algorithms . Various more technical and sophisticated methods exist, for which we refer to other more technical sources. These require expert knowledge of quantum chemistry. | Introduction to Quantum Algorithms for Physics and Chemistry (2012)13, a pedagogical book chapter. Open version: https://arxiv.org/abs/1203.1331. | Quantum Algorithms for Quantum Chemistry and Quantum Materials Science (2020)14, a scientific overview article. Open version: https://arxiv.org/abs/2001.03685. | . ",
    "url": "/applications/chemistry/#algorithms-for-quantum-chemistry",
    
    "relUrl": "/applications/chemistry/#algorithms-for-quantum-chemistry"
  },"36": {
    "doc": "Applications in chemistry and material science",
    "title": "A hype around quantum computing for climate change",
    "content": "Some businesses make spectacular claims about how quantum computing could be a cornerstone in solving climate change, thanks to the boost to R&amp;D on batteries, carbon capture, and more efficient chemical factories. However, rarely do we see any evidence â€“ most seem to assume that quantum computers simply spit out blueprints for revolutionary sustainable technologies. McKinsey takes the biscuit with their report titled â€˜Quantum computing just might save the planetâ€˜15. The article rightfully selects some of the most impactful technologies to reduce CO2 emissions, like electrification of transport, improved solar panels, and even vaccines that reduce methane emissions by cattle (indeed, due to cow farts). The article concludes that the selected innovations could reduce global warming from 1.7-1.8 Â°C by 2050 down to just 1.5 Â°C. It is a mystery to me why they throw in quantum computing because there is no mention whatsoever about why specifically quantum algorithms would be the key enabling factor. This exemplifies what we see more frequently in popular articles: quantum computers are depicted simply as insanely fast computers that will magically solve the barriers to other new technologies on our wishlist. What are the true prospects for quantum computing in the context of climate change? Sceptics may point out that technological innovations alone will not be sufficient to avert a climate disaster â€“ we will remain agnostic in this debate. A much more concrete issue is the mismatch in timelines. Climate experts agree that, to limit global warming to no more than 1.5Â° C, we need to take action relatively soon. Imperial College London concludes on their website16, referencing the 2014 IPCC report: . â€˜Limiting warming to 1.5Â°C will only be possible if global emissions peak within the next few years, and then start to decline rapidly, halving by 2030.â€™ . Our chapter on timelines shows that it is exceedingly unlikely that significant quantum utility is possible anywhere before the 2030s. Additionally, it will take several years before a computational discovery is sufficiently mature for large-scale deployment. For this reason, we donâ€™t see quantum computers as a good investment against climate change, but rather as a long-term development that can help us tackle other problems that humanity will face in the future. Do we really have no concrete applications in climate science? Well, we do have some concrete leads. In the search for a killer application in chemistry, perhaps the most-studied topic is the enzyme Nitrogenase. Its active site is precisely a multi-metal system that classical methods struggle with, and as weâ€™ll soon see, it appears in reputable plans for decarbonisation. To understand the relevance of this molecule, we need to dive into the world of food production. ",
    "url": "/applications/chemistry/#a-hype-around-quantum-computing-for-climate-change",
    
    "relUrl": "/applications/chemistry/#a-hype-around-quantum-computing-for-climate-change"
  },"37": {
    "doc": "Applications in chemistry and material science",
    "title": "A case study of a potential killer application: FeMoco",
    "content": ". Figure: Chemical structure of the FeMo cofactor of the Nitrogenase enzyme, taken from Wikimedia. Todayâ€™s agriculture relies heavily on the use of artificial fertilisers. Without large-scale use of supplementary nutrients, we would not be able to sustain intensive farming practices and feeding our worldâ€™s huge population would be problematic. In fact, aboutÂ half of the nitrogen atomsÂ in our body have previously passed a fertiliser factory! . Unfortunately, the production of fertiliser involves enormous energy consumption and carbon emissions. The main culprit is the ingredient ammonia (NH3), of which we use as much asÂ 230 Mton per year. Although our air consists mainly of molecular nitrogen (N2), plants cannot directly absorb this. Instead, they rely on bacteria (or, in the case of artificial fertiliser, humans) to perform so-called nitrogen fixation, breaking the strong triple bond of molecular nitrogen and converting this into ammonia. Microorganisms can convert this into further nitrogen-containing compounds that the root system can absorb. Pretty much all of the worldâ€™s ammonia production facilities follow the so-called Haber-Bosch process, where hydrogen gas (H2) and nitrogen gas (N2) react together to form ammonia. This method has the benefit that it can be implemented in large, high-yield production lines but comes with the disadvantage of its staggering energy consumption. The inefficiency stems from two essential steps: first, producing sufficiently pure hydrogen and nitrogen gasses, and later, separating the H2 and N2 molecules into individual atoms. Breaking N2 is especially challenging due to its strong triple bond. As an effect, factories operate at extreme conditions, with high temperatures (~400 degrees Celsius) and high pressure (over 200 atmospheres), driven mainly by natural gas. As much asÂ 1.8% of the worldâ€™s CO2 emissionÂ is caused by factories performing such reactions, consuming around 3-5% of the worldâ€™s natural gas production! . Canâ€™t this be done more efficiently? We strongly suspect so. Certain bacteria are also capable of making ammonia, but in a seemingly more efficient way, without high temperatures or high pressure. It would be extremely valuable to copy this trick. To imitate bacteria, we need to better understand a particular substance, the FeMo cofactor (short: FeMoco), which acts as a catalytic active site during ammonia production. A perfect simulation of FeMoco is not possible on classical computers, as the structure of roughly 120 strongly reacting electrons rapidly becomes intractable. In 2016,Â researchers from ETH Zurich and MicrosoftÂ were the first to report that a moderately large quantum computer could come to the rescue. A few years later, Google researchers refined the prospects even further, describing how simulations could be accomplished with aboutÂ 4 million qubits and 4 days of computing time. With FeMoco, we seem to finally have an example that confidently ticks all the boxes for quantum utility: classical methods are limited, we have well-understood quantum methods, and computational outputs have a significant commercial and societal impact. Unfortunately, there is yet another catch - innovation never comes so easily. A recent article17 quotes that industrial production of a ton of Ammonia costs around 26 GJ of energy, compared to at least 24 GJ (estimated) in bacteria. This is indeed not the massive reduction we were hoping for. The article concludes that perhaps the true value lies in a better understanding of this process: . â€˜The chemical motivation to study nitrogenase is thus less to produce an energy-efficient replacement of the Haber-Bosch process but rather because it is an interesting system in its own right, and perhaps it may motivate how to understand and design other catalysts that can activate and break the nitrogen-nitrogen triple-triple bond under ambient conditions.â€™ . As a final note, we want to stress that quantum computers do not magically spit out recipes for fertilisers, nor for medicines, batteries, or catalysts. For real breakthroughs, we need collaborations between chemists, engineers, and many other experts who spend several years running experiments, having discussions, employing computer simulations, making mistakes, going back to the drawing board a few times, and slowly converging to practical solutions. We should not forget that quantum computers merely provide a new set of tools. The best we can hope for is that smart people will use them in the right way! . ",
    "url": "/applications/chemistry/#a-case-study-of-a-potential-killer-application-femoco",
    
    "relUrl": "/applications/chemistry/#a-case-study-of-a-potential-killer-application-femoco"
  },"38": {
    "doc": "Applications in chemistry and material science",
    "title": "Further reading",
    "content": ". | (Scientific overview article) Prospects of quantum computing for molecular sciences https://link.springer.com/article/10.1186/s41313-021-00039-z . | (Scientific overview article) Quantum Chemistry in the Age of Quantum Computing https://pubs.acs.org/doi/10.1021/acs.chemrev.8b00803 . | (Scientific article) Toward the first quantum simulation with quantum speedup https://www.pnas.org/doi/10.1073/pnas.1801723115 . | . | Feynman, R.P. (1982) â€˜Simulating physics with computersâ€™, International Journal of Theoretical Physics, 21(6), pp. 467â€“488. https://doi.org/10.1007/BF02650179.Â &#8617; . | Lee, S. et al. (2023) â€˜Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistryâ€™, Nature Communications, 14(1), p. 1952. https://doi.org/10.1038/s41467-023-37587-6.Â &#8617; . | Santagati, R. et al. (2024) â€˜Drug design on quantum computersâ€™, Nature Physics, 20(4), pp. 549â€“557. https://doi.org/10.1038/s41567-024-02411-5.Â &#8617; . | Hariharan, S., Kinge, S. and Visscher, L. (2024) â€˜Modelling Heterogeneous Catalysis using Quantum Computers: An academic and industry perspectiveâ€™. ChemRxiv. https://doi.org/10.26434/chemrxiv-2024-d2l1k-v2.Â &#8617; . | Daley, A.J. et al. (2022) â€˜Practical quantum advantage in quantum simulationâ€™, Nature, 607(7920), pp. 667â€“676. https://doi.org/10.1038/s41586-022-04940-6.Â &#8617; . | Chan, G.K.-L. (2024) â€˜Quantum chemistry, classical heuristics, and quantum advantageâ€™. arXiv. https://doi.org/10.48550/arXiv.2407.11235.Â &#8617; . | Leijnse, K. (2024) â€˜Photocatalysis for Water Splittingâ€™, Quantum Application Lab, 8 January. https://quantumapplicationlab.com/2024/01/08/photocatalysis-for-water-splitting/ (Accessed: 28 August 2024).Â &#8617; . | von Burg, V. et al. (2021) â€˜Quantum computing enhanced computational catalysisâ€™, Physical Review Research, 3(3), p. | https://doi.org/10.1103/PhysRevResearch.3.033055. | . &#8617; . | Hutchins, Mark. â€˜Quantum Physics, Supercomputers, and Solar Cell Efficiencyâ€™. pv magazine International, 4 August 2023. https://www.pv-magazine.com/2023/08/04/quantum-physics-supercomputers-and-solar-cell-efficiency/.Â &#8617; . | Choi, Charles Q. â€˜How Quantum Computers Can Make Batteries Betterâ€™. IEEE Spectrum. Accessed 28 August 2024. https://spectrum.ieee.org/lithium-air-battery-quantum-computing.ssÂ &#8617; . | Santagati, R. et al. (2024) â€˜Drug design on quantum computersâ€™, Nature Physics, 20(4), pp. 549â€“557. https://doi.org/10.1038/s41567-024-02411-5. Quote from this article: â€˜Current classical quantum-chemistry algorithms fail to describe quantum systems accurately and efficiently enough to be of practical use for drug design.â€™Â &#8617; . | An interesting subtlety is how we measure the energy that the VQE is supposed to optimise. Luckily, there exist very short circuits that we can append to measure the output states in different bases. By running the VQE a relatively small number of times, we can make good estimates of the energy of its output states. This avoids performing the more complex QPE during the optimisation phase. We only need the QPE to produce an accurate representation of the state weâ€™re searching for.Â &#8617; . | Yung, M.-H. et al. (2014) â€˜Introduction to Quantum Algorithms for Physics and Chemistryâ€™, in Quantum Information and Computation for Chemistry. John Wiley &amp; Sons, Ltd, pp. 67â€“106. https://doi.org/10.1002/9781118742631.ch03.Â &#8617; . | Bauer, B. et al. (2020) â€˜Quantum Algorithms for Quantum Chemistry and Quantum Materials Scienceâ€™, Chemical Reviews, 120(22), pp. 12685â€“12717. https://doi.org/10.1021/acs.chemrev.9b00829.Â &#8617; . | Cooper, Peter, Philipp Ernst, Dieter Kiewell, and Dickon Pinner. â€˜Quantum Computing Just Might Save the Planetâ€™. McKinsey, 19 May 2022. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/quantum-computing-just-might-save-the-planet.Â &#8617; . | How and when do we need to act on climate change? (no date) Imperial College London. https://www.imperial.ac.uk/grantham/publications/climate-change-faqs/how-and-when-do-we-need-to-act-on-climate-change-/ (Accessed: 23 September 2024).Â &#8617; . | Chan, G.K.-L. (2024) â€˜Quantum chemistry, classical heuristics, and quantum advantageâ€™. arXiv. https://doi.org/10.48550/arXiv.2407.11235.Â &#8617; . | . ",
    "url": "/applications/chemistry/#further-reading",
    
    "relUrl": "/applications/chemistry/#further-reading"
  },"39": {
    "doc": "The impact on cybersecurity",
    "title": "The impact on cybersecurity",
    "content": "Reading time: 13 minutes . Contents . | Cryptography is much more than just secrecyÂ  | The quantum threat is mainly to public key cryptographyÂ  | What solutions exist? | Conclusion | Further reading | . In the world of quantum computers, the most convincing exponential speedup lies in codebreaking. Anyone who wants to understand the impact of quantum computers must know the basics of cryptography. Letâ€™s start at the beginning.Â  . ",
    "url": "/applications/cybersecurity/",
    
    "relUrl": "/applications/cybersecurity/"
  },"40": {
    "doc": "The impact on cybersecurity",
    "title": "Cryptography is much more than just secrecyÂ ",
    "content": "Why do we actually use cryptography? Pretty much everyone will immediately think of: . | Privacy/confidentiality:Â ensuring others cannot read your data (especially when messages are sent over a network). | . However, there are many more threats that cryptography protects us from. Most people wouldnâ€™t normally worry about them, but when any of the following is missing, cybercriminals can cause a lot of harm:Â  . | Authentication/identification:Â You want to verify that a message really came from the entity that claims to send the message. For example, during online banking, you want to be 100% sure that you are communicating with your bank and nobody else. Another example is when installing a new piece of software. When executing the latest Windows update, your computer makes sure to check that there is aÂ â€˜digital signatureâ€™Â that belongs to Microsoft. Imagine how unsafe your laptop would be if anyone could send fake updates! . | Integrity:Â You want to verify that nobody changed the message during transit. Imagine the damage when anyone can alter emails or file transfers, or when the commands coming from an air traffic control tower are modified. Similarly, any software installer confirms that the software wasnâ€™t changed by anyone but the original publisher, by verifying a digital signature. | Exchanging secret keys:Â How do you negotiate a new secret key with a brand new web shop that you have never visited before? This is a seemingly impossible task if anyone can read bare internet traffic, but modern cryptography has a solution. | . There are many other vital functionalities, likeÂ non-repudiation andÂ availability, that we donâ€™t discuss here. Remember the bold-faced terms above, as we will often come across these.Â  . We hope that this introduction makes you aware of the enormous importance of proper cryptography and the sheer number of cryptographic checks required for the proper functioning of our IT. You would be surprised how often you use cryptography on a daily basis through your laptop, phone, car keys, or smart cards. ",
    "url": "/applications/cybersecurity/#cryptography-is-much-more-than-just-secrecy",
    
    "relUrl": "/applications/cybersecurity/#cryptography-is-much-more-than-just-secrecy"
  },"41": {
    "doc": "The impact on cybersecurity",
    "title": "The quantum threat is mainly to public key cryptographyÂ ",
    "content": "A common misconception, which we see a lot in popular literature, is that the quantum threat can be summarised as follows. (Both of the statements below areÂ incorrect!)Â  . | â€˜A quantum computer will break all of todayâ€™s cryptography.â€™ . | â€˜A quantum internet is needed to keep our cryptography safe again.â€™ . | . To better understand this, letâ€™s first look at what cryptography a quantum computer will break, and which it wonâ€™t. Later, we will look at the necessity of a quantum internet.Â  . In line with common cryptography jargon, we will typically have two parties, Alice and Bob, who want to communicate with each other. We distinguish two different types of cryptography: the symmetric and the asymmetric (public key) variants.Â  . InÂ symmetric (or private key) cryptography,Â we assume that both Alice and Bob already know some secret key. This could be a password that they both know or, more commonly, a very long number represented by (say) 128 bits in their computer memory. Alice can use the key to encrypt any message using a cipher likeÂ AES. Bob can then use the same key to decrypt this message. The details of how encryption and decryption work are unimportant for our purposes. The only relevant thing is that our computers can do this very efficiently and that itâ€™s considered sufficiently safe: without the key, nobody could reasonably break this encryption. In asymmetric cryptography, more often calledÂ public key cryptography (PKC), each participant has two keys: aÂ public keyÂ and aÂ private key. TheÂ public keyÂ can be shared with anyone, while theÂ private keyÂ must be kept secret. Thatâ€™s why we use the suggestive colours green (save to share) and red (keep private!). If Alice wants to send an encrypted message to Bob, she usesÂ Bobâ€™sÂ public keyÂ to encrypt the message. The message can only be decrypted using Bobâ€™s private key, ensuring that only Bob can read the message. The setting with two keys offers more functionality. For example, using public key cryptography, Alice could securely send a secret key to Bob that they can subsequently use for symmetric cryptography, which is faster in practice. When public key cryptography is built for this purpose, we call it a key encapsulation mechanism (KEM). Â  . Furthermore, the protocol works in â€˜reverseâ€™. Alice can use herÂ private keyÂ to encrypt a message, which then anyone in the world (including Bob) can decrypt using the correspondingÂ public key. Bob should then be confident that Alice is the only person who could have encrypted this message. Indeed, something encrypted with theÂ private keyÂ canÂ only be decrypted with theÂ public key, and vice versa. The encrypted message is much like a signature that only Alice can produce. This forms the basis of digital signatures and certificates.Â Â  . You can see public key cryptography in action whenever you visit a web page. Your browser (like Chrome or Firefox) will display that the connection is secure, which means that it verified that the digital signature is valid, amongst other things. This guarantees authenticity (the page came from a registered server) and integrity (the site arrived unchanged).Â  . It should come somewhat as a surprise that public key cryptography is even possible at all! Itâ€™s kind of a small miracle that encryption and decryption with two totally different keys can be made to work, thanks to some powerful mathematics. However, it turns out that the delicate relationship between the two keys is also a weak spotâ€¦ . How good are quantum computers at cracking cryptography?Â  . Symmetric-key cryptographyÂ is quite safe against quantum hackers. The biggest problems are brute-force attacks, where an attacker effectively tries every possible secret key. Using a key size of 128 bits, the total number of possible keys is 2128Â â€” thatâ€™s an incomprehensibly large number, much more than the number of atoms in a human body.Â  . We know that Groverâ€™s algorithm speeds up brute-force search by reducing the number of attempts fromÂ 2128 to its square root, which isÂ 264. This is something that cryptographers are not happy about, but considering the slowness and extra overhead that comes with quantum computers, this doesnâ€™t seem to be a problem in the foreseeable future. Still, to be on the safe side, it is recommended to double key lengths, hence, to use the same algorithm with 256-bit keys. Changing this in existing IT infrastructure is relatively straightforward, although one shouldnâ€™t underestimate the time and costs for such changes within large organisations. The situation is entirely different withÂ public key cryptography.Â The most-used algorithms today,Â RSAÂ andÂ ECC, can be straightforwardly broken by a large quantum computer. We discussed the details of Shorâ€™s algorithmÂ earlier and saw that around 20 million qubits and 8 hours are needed to retrieve a secret RSA key. Luckily, there exist PKC systems that are believed to be safe against quantum computers, and an obvious way forward is to start using these. We call such systemsÂ post-quantum cryptography, and despite the confusing name, theyâ€™re built to work on conventional computers. We discuss the rabbit hole of migrating to new cryptographyÂ in a different chapter. Unfortunately, even todayâ€™s communication could be at risk due to a practice calledÂ harvest now, decrypt later.Â Encrypted messages that are sent over a network can be intercepted and stored for many years, until a quantum computer can efficiently decrypt the messages. Even though we use public key encryption mainly to establish temporary keys for symmetric cryptography, a smart attacker could still retrace all the intermediate steps and retroactively spy on our communication. It is unclear at what scale storage of sufficiently detailed internet data is genuinely happening, but it seems plausible that security agencies of larger nations are already doing this. The following table summarises how our cryptosystems are threatened: . | Â  | Symmetric | Public-key | Quantum networks | . | Â  | Today (AES, â€¦ ) | Today (RSA, ECC) | PQCÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â  | QKD | . | Safe against classical computers | âœ” | âœ” | âœ” | âœ” | . | Safe against quantum computers | âœ”* . *with double key lengths . | Unsafe | âœ” | âœ” | . Why donâ€™t we switch to symmetric cryptography?Â  . Public key cryptography solves a very fundamental problem: how can Alice and Bob agree on a secret key before they have a means of encryption in the first place? They cannot just send a new key over the internet without any form of encryption, because anyone would be able to read this. This is the fundamental problem ofÂ key distribution. Let us look at the functionality offered by the two types of cryptography:Â  . | Â  | Symmetric | Public-keyÂ  Â  Â Â Â  Â  Â  Â  Â  Â  Â  | Quantum key distribution | . | Confidentiality (privacy) | Only with pre-shared keys | âœ” | âœ— | . | Authentication / IntegrityÂ Â  | Only with pre-shared keys | âœ” | âœ— | . | Establishing secret keys | âœ— | âœ” | âœ”* *Only when another mechanism takes care of authentication. | . If only we could somehow give Alice and Bob pre-shared keys in a secure way, we would resolve most of these problems. Without public key cryptography, there are other options: . | Trusted courier. Alice and Bob could meet every other week to exchange USB drives with secret codes. | Trusted third party. Alice and Bob could both trust a large â€˜key serverâ€™. If both share a secret key with the key server, they can securely ask the server to generate a new secret key that they can use together.Â  . | Quantum key distribution. We discuss this solution further below. | . Unfortunately, trusted couriers or trusted third parties are rarely an attractive alternative to public key cryptography, especially when scaling up to networks with thousands or millions of connected users. Couriers are simply too slow for todayâ€™s standards, and single trusted parties would pose a particularly interesting target for attackers. ",
    "url": "/applications/cybersecurity/#the-quantum-threat-is-mainly-to-public-key-cryptography",
    
    "relUrl": "/applications/cybersecurity/#the-quantum-threat-is-mainly-to-public-key-cryptography"
  },"42": {
    "doc": "The impact on cybersecurity",
    "title": "What solutions exist?",
    "content": "There is a clear need for post-quantum cryptography to replace commonly used cryptosystems like RSA and ECC. Luckily, back in 2016, the American National Institute of Standards and Technology (NIST) started a competition to select a new cryptosystem which should balance safety and practical usability (for example, it should not be too slow or memory-inefficient). They invited experts from around the globe to propose cryptographic algorithms, which peers assessed. Four rounds and several broken algorithms later, NIST selected a first set of winners that are suitable for large-scale use. As of August 2024, the first three PQC algorithms are now official NIST standards. Even though this effort was coordinated by an American institute, the process was backed and carried out by cryptographers from around the world. A broad majority of cybersecurity experts have confidence in NISTâ€™s competition and recommend the final standards. National security organisations from other countries like BSI (Germany) and ANSSI (France) may prefer different algorithms but have also explicitly stated that this does not mean that they consider NISTâ€™s standards unsafe. The results of the competition are as follows. Firstly, NIST selected one Key Encapsulation Mechanism that can be used to establish secret keys over an unencrypted connection - remember the problem of communicating with a web shop that you had never encountered before. | Functionality | NIST Name | Problem family | Documentation | Original name | . | Key Encapsulation Mechanism | ML-KEM | Module-Lattice based | FIPS 203 | CRYSTALS-Kyber | . Secondly, NIST selected three different Digital Signature Algorithms. These are used for authentication and integrity â€“ remember how we donâ€™t want our messages to be altered in transit or how we want to prevent malware injected in software updates. Â  . | Functionality | NIST Name | Algorithm family | Documentation | Original name | . | Digital Signatures Algorithm | ML-DSA | Module-Lattice based | FIPS 204 | CRYSTALS-DilithiumÂ  | . | Digital Signatures Algorithm | SLH-DSA | Stateless Hash-Based | FIPS 205 | SPHINCS+ | . | Digital Signatures Algorithm | FN-DSA | Fast-Fourier Transform over NTRU-Lattice based | FIPS 206 (coming soon) | FALCON | . You might wonder why three algorithms were selected. Unfortunately, all three standards come with downsides, for example, because the keys can take up more memory or because the performance (time to sign or verify) is worse. The real-world impact will differ per use case. ML-DSA is the main cryptosystem recommended for general use, whereas SLH-DSA and FN-DSA may be beneficial in specific circumstances. Are the new standards considered safe? . The short answer is yes: the new PQC standards are considered ready for use, and choosing algorithms such as ML-KEM or ML-DSA is widely regarded as a sound decision. There may be exceptions in specific high-security scenarios, but if you are operating in such a context, you are likely already aware of these nuances. However, there seems to be some uncertainty within the cryptographic community regarding whether the new PQC standards will be as reliable as our trusted RSA or ECC. The new standards have not yet stood the test of time, and it is possible that unexpected weaknessesâ€”whether minor implementation flaws or fundamental vulnerabilitiesâ€”may still be present. To illustrate, a PQC method called SIKE1 was in the race to become a new NIST standard and made it all the way to the 4th round until it was proven unsafe.Â  . To mitigate any unexpected vulnerabilities in the new standards, most authorities recommend a hybrid implementation that combines the strengths of both conventional and post-quantum PKC. Moreover, organisations are generally advised to invest in cryptographic agility, a broad term used to describe the ability to easily update cybersecurity defences. The above may sound somewhat negative, but we donâ€™t expect the slightly lower trust to stand in the way of adoption. Cryptographic algorithms themselves are rarely the weakest point, so it seems wise to focus on other potential vulnerabilities instead. What about Quantum Key Distribution (QKD)? . Quantum key distribution is also presented as a solution for key exchange, making it a potential alternative to RSA, ECC and ML-KEM. Still, many security authoritiesÂ warnÂ againstÂ adoptingÂ QKD today. Although the idea is promising, todayâ€™s hardware is still immature. Moreover, QKD doesnâ€™t provide any functionality for digital signatures, thus we will need the migration to PQC anyway. It is somewhat of a pity that QKD is not so mature yet, because it would be a viable weapon against Harvest Now, Decrypt Later. Nevertheless, since a quantum threat could be here as soon as the early 2030s, experts warn that companies and governments should fix their PQC first. At a later stage, QKD can be considered as an add-on for further security. What about Quantum Random Number Generators (QRNG)? . Good random number generators are exceptionally important in cryptography, and QRNGs could provide a good alternative to theÂ hardware random number generatorsÂ that are widely used today.Â  . However, all they do is generate random numbers â€“ that doesnâ€™t make any protocol in itself quantum-safe. As a general warning:Â products with â€˜quantumâ€™ in the name do not automatically protect against Shorâ€™s algorithm!Â  . What steps should a typical company or government take?Â  . We dedicate aÂ separate chapterÂ to that!Â  . ",
    "url": "/applications/cybersecurity/#what-solutions-exist",
    
    "relUrl": "/applications/cybersecurity/#what-solutions-exist"
  },"43": {
    "doc": "The impact on cybersecurity",
    "title": "Conclusion",
    "content": "Cryptography is strongly intertwined with quantum computing through Groverâ€™s algorithm, Shorâ€™s algorithm, and Quantum Key Distribution. Security experts recommend that there is an obvious way forward: . | Replace current public key cryptography with new, quantum-safe protocols (PQC). | Double key lengths in symmetric cryptography.Â  . | . Especially the first bullet is a major challenge. There are many legacy systems on the internet that can not be updated so easily. Billions of devices are all interconnected, so updating one device may cause incompatibilities somewhere else. Whatâ€™s more, PQC protocols will likely require more CPU power, memory, and bandwidth than todayâ€™s trusted methods. Companies may need to update the core code of hundreds or even thousands of applications. And lastly, the new protocols havenâ€™t been tested as extensively as our conventional methods, so it is not unlikely that new security issues will be found. Before they are even built, quantum computers are already causing headaches to cryptographers and cybersecurity managers. ",
    "url": "/applications/cybersecurity/#conclusion",
    
    "relUrl": "/applications/cybersecurity/#conclusion"
  },"44": {
    "doc": "The impact on cybersecurity",
    "title": "Further reading",
    "content": ". | Cloudflareâ€™s resource page â€˜The state of the post-quantum Internetâ€˜ explains many aspects of the migration to post-quantum cryptography. | The NSA publishes recommendations on which cryptographic algorithms should be used and sketches a concrete timeline about when governmental security systems should be updated. | â€˜The PQC Migration Handbookâ€˜ is a free guide for corporate managers on how to tackle the upcoming cryptography migration, written by Dutch research organisations TNO, CWI and the secret service AIVD. | In the context of Harvest Now, Decrypt Later, the urgency to migrate depends on how long your data should remain confidential, according to Moscaâ€™s Theorem. | . | Goodin, Dan. â€˜Post-Quantum Encryption Contender Is Taken out by Single-Core PC and 1 Hour.â€™ Ars Technica, August 2, 2022. https://arstechnica.com/information-technology/2022/08/sike-once-a-post-quantum-encryption-contender-is-koed-in-nist-smackdown/.Â &#8617; . | . ",
    "url": "/applications/cybersecurity/#further-reading",
    
    "relUrl": "/applications/cybersecurity/#further-reading"
  },"45": {
    "doc": "Applications of quantum networks",
    "title": "Applications of quantum networks",
    "content": "Reading time: 10 minutes . Contents . | The promises of the quantum internet | How useful is the quantum internet in practice?Â  | The case for QKD | Conclusion | Further reading | . If weâ€™re building computers that deal with qubits, superposition and entanglement, wouldnâ€™t these computers also need some way to send qubits to each other? This is the dream of the quantum internet: a network parallel to our well-known classical internet that allows the transmission of qubits. There is a bit of a paradox here. On the one hand, a full-blown quantum internet that stretches across the globe is very, very far away â€“ it will require quantum repeaters to bridge longer distances, purification mechanisms to repair imperfections, and many more technologies that weâ€™re only just figuring out. On the other hand, it is often said that quantum networks have a higher Technology Readiness Level than computing. That sounds like a contradiction, right? . The main explanation is that there are some applications for small-scale â€˜imperfectâ€™ quantum networks, particularly in the context of cryptography.Â  . In a sense, quantum networking applications have always been ahead of quantum computing. Already in 1984, long before quantum computers were seriously considered, quantum pioneers Charles Bennett and Gilles Brassard discovered a method to securely negotiate a secret key (think of a password) between two distant parties based on sending individual photons. Their result is now famously known as theÂ BBâ€™84 protocol. Similarly, the commercialisation of network technologies has long been ahead of computing. Early quantum startups like MagiQ Technologies and ID Quantique were founded around the start of this century, and their first commercial networking products were brought to the market in 2003 and . | This technology, where a quantum network is used to generate a secret key at two endpoints, is called Quantum Key Distribution (QKD) â€“ an application that we will address in much more detail below. | . ",
    "url": "/applications/networks/",
    
    "relUrl": "/applications/networks/"
  },"46": {
    "doc": "Applications of quantum networks",
    "title": "The promises of the quantum internet",
    "content": "There is a long list of arguments why we should be excited about the quantum internet. Here are some of the applications that we hear most frequently: . | Clustering quantum computers: By connecting multiple smaller computers, one might build a much larger computer with more combined memory, allowing it to tackle more complex problems. Â  . | Securing classical communication.Â The main contender here is Quantum Key Distribution (QKD), sometimes dubbed the â€˜unhackableâ€™ network.Â This allows two distant users to create a secret key (think of a password) that can be used in further cryptographic applications. | â€˜Blind computingâ€™: Encrypting data while still allowing someone else to process it.Â What if you hire an Amazon cloud computer to do calculations on your data, but you donâ€™t want Amazon to actually see the data itself? It turns out that you can make quantum computers do their computations even while the data remains encrypted, with some caveats. Similarly, one could use â€˜encryptedâ€™ software to solve someone elseâ€™s problem without them discovering this algorithm. Such applications often go by the name of blind computing or private computing.Â  . | A scientific (hard!) overview of blind computing applications.Â  | . | Position verification:Â Can you prove that you are currently at a given location in a way that cannot be spoofed?Â  . | A short introductory video [3:23] | . | Protocols with multiple parties, where not every participant can be trusted,Â such asÂ leader election orÂ Byzantine agreement. You can find many more in theÂ Quantum Protocol Zoo.Â  . | Make quantum sensors more effective. There exist proposals to combine different telescopes or gravitational wave detectors, and plans to synchronise quantum clocks.Â  . | A scientific (hard!) overview of distributed quantum sensing | . | . ",
    "url": "/applications/networks/#the-promises-of-the-quantum-internet",
    
    "relUrl": "/applications/networks/#the-promises-of-the-quantum-internet"
  },"47": {
    "doc": "Applications of quantum networks",
    "title": "How useful is the quantum internet in practice?Â ",
    "content": "The impact of many quantum network applications will depend on how much we will use quantum computers. If quantum computers become widespread in the future, then communication between them also seems to be extremely worthwhile. On the other hand, our current outlook of quantum computers focuses on special-purpose devices used to solve isolated problems. In the latter scenario, the value of exchanging quantum data is not immediately clear. There is an intriguing road map to build a reliable quantum internet in the future (involving fascinating tricks likeÂ entanglement distillation andÂ teleportation), but this would require multiple error-corrected quantum computers by itself! Therefore, in this book, weâ€™re not yet ready to look ahead at applications like clustering computers, multi-party computations, private computing, or making sensors more effective. Regarding clustered quantum computers,Â we frequently hear arguments that one can make a bigger quantum computer by connecting individual ones, giving us access to larger numbers of qubits in a single calculation. It seems to me that building these computers right next to each other (and calling it a single computer) is much more effective than transporting fragile quantum data over large distances â€“ clustering seems useful in extremely small networks. In the foreseeable future, the first interesting applications are those that work over a â€˜noisyâ€™ connection and transport just one qubit at a time (or perhaps a handful of them). For practical interest,Â Quantum Key Distribution (QKD) is by far the most interesting application. ",
    "url": "/applications/networks/#how-useful-is-the-quantum-internet-in-practice",
    
    "relUrl": "/applications/networks/#how-useful-is-the-quantum-internet-in-practice"
  },"48": {
    "doc": "Applications of quantum networks",
    "title": "The case for QKD",
    "content": "To fully understand QKD, we will need to have a bit more background about cryptography, especially the key distribution. For a full account, we recommendÂ first reading the chapter on cryptography. In short, weâ€™re wondering how Alice can agree on a secret key with her distant friend Bob in a world where everyone can read plain data sent over the internet. Surely, they canâ€™t just send their secrets or passwords over to each other without having any encryption in the first place! This problem is commonly solved usingÂ public key cryptography (which we know will be revamped in the following years). If you really donâ€™t trust public key cryptography, the main alternative is to physically transport a USB stick by a trusted courier.Â  . Compared to conventional cryptography, the unique selling point of QKD is that it is fundamentally impossible for cybercriminals to obtain the secret key as it is being distributed. As long as our understanding of quantum mechanics is correct (and weâ€™re quite convinced it is, as itâ€™s arguably the most well-tested theory in science), no amount of computational power or mathematical breakthroughs will let an attacker gain information about the key. Of course, this assumes that the protocol is executed precisely as prescribed and that there are no other vulnerabilities in the actual hardware or software.Â  . This fundamentally differs from todayâ€™s approach to public key cryptography, which must rely on certain mathematical assumptions. We know for sure that, with sufficient computational power, these codes can be broken, but we argue that this takes such a painfully long time that nobody will bother. Still, such statements about computation times are based on assumptions, and our trust derives from the empirical evidence that our smartest cryptographers have not found any weaknesses yet. In fact, well-regarded cryptosystems do get broken from time to time. A prototypical example is SIKE1, which was in the race to become a new NIST standard until it was proven unsafe.Â  . That said, although QKD is â€˜unhackableâ€™ in theory, the actual hardwareÂ andÂ software are likely to contain vulnerabilities. Contrary to well-trusted public key cryptography, no QKD system has received proper certification and accreditation, and a significant fraction of historical products have been hacked.Â  . QKD has the downside that it requires specialised hardware, although it is much less demanding than other quantum internet applications we mentioned. It can already be practical with a basic point-to-point network with just two connected parties, with one party limited to sending photons and the other limited to just measuring them. Moreover, the qubits need only be sent and measured one at a time, so no quantum memory or extensive quantum computations are needed. There have already been several demonstrations that use standard telecom fibre (the stuff thatâ€™s already in the ground) or satellite-based systems that communicate through air. QKD hardware is fancy and expensive but not completely out of reach.Â  . The fundamental downside of QKD is that it features no intrinsic way to confirm who the person on the other end of the line is. Some form of authentication is still needed â€“ which is done with secret keys that should already be present in the first place! This makes QKD just a partial solution to the key distribution problem: itâ€™s mostly a key extension protocol, creating arbitrary amounts of key material based on a small initial key. Further reading about QKD . | Companies likeÂ Toshiba andÂ ID Quantique offer commercial QKD systems for distances of around 100 km.Â  . | Chinese scientists achieveÂ QKD through satellites over 1000 km.Â  . | . What do experts say?Â  . Cybersecurity experts (indeed, the people who have been diligently keeping our classical computers safe for decades) are typically sceptical about QKD. In fact, all major security authorities that we are aware of currently advise against the use of QKD. They find the use of additional, uncertified hardware too large of a security risk and stress that there is a better solution that works on conventional computers:Â post-quantum cryptography (PQC). From their perspective, PQC offers all the required functionalities, and is currently more practical to test, certify and implement.Â  . Be careful not to confuse theÂ abbreviations PQC and QKD. QKD is about communication with a fancy quantum network. PQC runs on conventional hardware.Â You may call both of them â€˜quantum-safeâ€™ cryptography, as they should both resist attacks from a large-scale quantum computer. A fair argument in favour of QKD stems from the harvest now, decrypt later attacks that could be done today. These imply that even the privacy of todayâ€™s messages is at risk, which could be an argument for organisations to rapidly switch to QKD to protect their most sensitive data. Still, for those willing to go the extra mile for their privacy, looking at more mature and readily available solutions might be more worthwhile. For example, there exist certified solutions that rely on symmetric encryption with trusted couriers.Â Â  . Whatâ€™s left is a niche use case for the most forward-thinking organisations that deal with fierce security requirements. It is somewhat of a pity that QKD is not so mature today, as many organisations will start a migration to quantum-safe cryptography soon. Widespread adoption of QKD would likely lower the costs of quantum networks and make it easier to expand to a large-scale quantum internet in the future. Nevertheless, since a quantum threat could be here as soon as the earlyÂ 2030s, we stick with the recommendation to migrate to post-quantum cryptography first and to consider QKD as an add-on for additional security later, if needed.Â  . See also: . | Compumatica offers symmetric encryption with keys transported on SD cards. | TNO designed a â€˜quantum-safe proxyâ€™ as anÂ add-on to existing cryptography. | StackOverflow question:Â â€˜Why does the NSA find QKD impracticalâ€™? . | The French, Swedish, Dutch and German national security authorities sound their criticism in a collective publication, â€˜The uses and limits of quantum key distributionâ€˜. | . ",
    "url": "/applications/networks/#the-case-for-qkd",
    
    "relUrl": "/applications/networks/#the-case-for-qkd"
  },"49": {
    "doc": "Applications of quantum networks",
    "title": "Conclusion",
    "content": "In conclusion, most applications of a quantum internet will not be immediately relevant in the foreseeable future, with an exception for QKD. And even QKD might not be the killer applications that many investors are hoping for â€“ it most definitely shouldnâ€™t be called â€˜unhackableâ€™.Â  . Still, it seems unfair to us to dismiss a quantum internet because it would be â€˜too technologically challengingâ€™ or â€˜too expensiveâ€™. These arguments are correct today but could be naive on a scale of several decades. Would anyone from the 70â€™s have believed that today, almost everyone on the globe is streaming videos on a mobile phone for just a few dollars per month? Who knows what the quantum internet will look like 30 years from now?Â  . ",
    "url": "/applications/networks/#conclusion",
    
    "relUrl": "/applications/networks/#conclusion"
  },"50": {
    "doc": "Applications of quantum networks",
    "title": "Further reading",
    "content": ". | Much more about the various quantum network applications can be found in an online Quantum Internet magazine by TU Delft or on the website of the Quantum Internet Alliance. | A video explanation of QKD forÂ laymen orÂ experts. | A nature commentaryÂ onÂ why practical long-range QKD is still out of reach.Â  . | . | Goodin, Dan. â€˜Post-Quantum Encryption Contender Is Taken out by Single-Core PC and 1 Hour.â€™ Ars Technica, August 2, 2022. https://arstechnica.com/information-technology/2022/08/sike-once-a-post-quantum-encryption-contender-is-koed-in-nist-smackdown/.Â &#8617; . | . ",
    "url": "/applications/networks/#further-reading",
    
    "relUrl": "/applications/networks/#further-reading"
  },"51": {
    "doc": "Error correction",
    "title": "Error correction",
    "content": "Reading time: 16 minutes . Contents . | What is error correction? | Longer computations need more qubits | What is the current state-of-the-art? | Conclusion | See also | . At a glance To run long computations, we need to dramatically reduce the likelihood of error in each computational step â€“ not just a little bit, but by a factor of millions.Â  . Error correction is the most effective method to achieve extremely low error probabilities. It combines a small number of â€˜physicalâ€™ qubits (think of several hundred) into a single â€˜logicalâ€™ qubit that suppresses errorsÂ exponentially.Â  . Logical qubits are still not perfect: the â€˜number of stepsâ€™ that they can survive is an important specification that determines whether they can a particular application. Around 2024, weâ€™re seeing a major shift in the road maps of quantum computer manufacturers. Several companies no longer put their bare qubits in the spotlight, but instead focus on logical qubits. Error correction seems to be an essential component of large-scale quantum computing, adding yet another facet in which these devices differ from their classical counterparts. Although this is a relatively advanced topic, we find it so important that it deserves a dedicated chapter in this book. As with many aspects of quantum computing, error correction can be rather confusing. A statement that we often hear is the following (which is incorrect!) . â€˜Logical qubits (or error-corrected qubits) are resilient to errors that occur during a computation. Once we have logical qubits, we can increase the length of our computations indefinitely. â€˜ . Whatâ€™s the problem here? Well, not every logical qubit is created equally. In the near future, we expect to see logical qubits that are perhaps 2x more accurate than todayâ€™s bare hardware qubits, and later 10x, and in the future perhaps 1000x. Error correction is a trick toÂ reduceÂ the probability of errors, but it will not eliminate errors completely. In the following decade, we expect gradual improvements, hopefully down to error rates of 10-10Â and below. ",
    "url": "/advanced/error_correction/",
    
    "relUrl": "/advanced/error_correction/"
  },"52": {
    "doc": "Error correction",
    "title": "What is error correction?",
    "content": "In quantum error correction, we combine some number (think of hundreds or thousands) ofÂ â€˜physicalâ€™Â hardware qubits into a virtualÂ â€˜logicalâ€™Â qubit. The logical qubits are the information carriers used in an algorithm or application. Error correction methods can detect whenever tiny errors occur in the logical qubit, which can then be â€˜repairedâ€™ with straightforward operations. Under the assumption that the probability of hardware errors is sufficiently low (below a certain error threshold), the overall accuracy improves exponentially as we employ more physical qubits to make a logical qubit. Hence, we obtain a very favourable trade-off between the number of usable qubits and the accuracy of the qubits. Doesnâ€™t measuring a quantum state destroy the information in the qubits? . Indeed, if we naively measure all the physical qubits, we destroy potentially valuable information encoded in the qubits. However, quantum error correction uses an ingenious way to measure only whether or not an error occurred. It learns nothing about the actual information content of the qubit. It turns out that this way, the data stored in the logical qubit is not affected.Â  . Why are errors so much of a problem?Â How do errors screw up our computations?Â  . In short, even tiny errors are a problem because we want to perform an astonishing number of quantum operations successively â€” think of billions or trillions of them.Â  . Letâ€™s make this more concrete. A computer program is essentially a sequence ofÂ â€˜stepsâ€™, each of which a computer knows how to perform. We say that a program or algorithm has aÂ width,Â which is the number of qubits it requires. It also has aÂ depth,Â which is the number of consecutive steps that need to be performed. You may interpret one step in early hardware as a single quantum gate (although, in practice, gates may be performed in parallel, making theÂ impact of errors slightly more complicated).Â  . The concept of â€˜widthâ€™ is pretty straightforward: if the computer doesnâ€™t have enough memory, it cannot run the program. Dealing with â€˜depthâ€™ is harder. To run a program of 109Â steps, we need to limit errors to roughly the inverse, say, a probability of 10-9Â per step. If the error is larger, it becomes extremely unlikely that the quantum computer will produce the correct outcome. These are not hard numbers: a computer with 10-10Â error would be a significant improvement (resulting in much fewer mistakes), and a computer with 10-8Â error might be pushed to also find the correct answer after many tries. However, as the imbalance between depth and error grows, the probability of finding a correct outcome is reducedÂ exponentially. We illustrate this in more detail in the box below.Â  . To illustrate, why do we need such small error rates? Letâ€™s look at a very simple model of a computer, which is not unlike what happens inside a quantum computer or a modern (classical) CPU. As above, the computer is supposed to work through a list of instructions. We can consider various specifications of a computer: . | The available memory, measured in bits (or perhaps megabytes or gigabytes, if you like).Â  | . | The speed at which the computer operates, measured in steps per second.Â  . | The â€˜probability of errorâ€™, describing the likelihood that one gate introduces some mistake. This is given as a number between 0 and 1 (or a percentage between 0 and 100%). Many sources use the word â€˜fidelityâ€™ instead, which can be roughly interpreted as the opposite (fidelity â‰ˆ 1 â€“ probability of error). In this text, we sometimes just say â€˜errorâ€™ while we mean its probability. | . In this simple model, the time taken to complete the computation equals â€˜depthâ€™ x â€˜speedâ€™. You can make the calculation faster by increasing the speed of the computer or by writing a â€˜betterâ€™ program that takes fewer steps.Â  . The influence of errors is harder to track. For contemporary computers, we typically donâ€™t worry about hardware mistakes at all, as every step has essentially 100% certainty to output the correct result. However, letâ€™s see what happens when this is not the case.Â  . Assume that each step has a 1% (= 10-2) probability of error. What will the impact be on the final computation? Below, we compute the probability to finish the computation without any errors, for various numbers of computational steps. | Error probability: 1% | Â  | . | Number of steps | P(success)Â Â  | . | 1Â  | ( 0.99 )1Â = 99% | . | 100 | ( 0.99 )100Â = 37% | . | 1000 | ( 0.99 )1000Â = 0.004 % | . | 10,000 | ( 0.99 )10,000Â = Â  10-44Â  | . In this simple model, we assume thatÂ anyÂ error is catastrophic. This is quite accurate for most programs. You might argue that there is a miniscule probability that two errors cancel, or that the error has very little effect on the final result, but it turns out that such effects are statistically irrelevant in large computations.Â  . Now, if we improve the hardware to have an error rate of just 0.1% (=10-3), we find the following. | Error probability: 0.1% | Â  | . | Number of steps | P(success)Â Â  | . | 1Â  | ( 0.999 )1Â = 99.9% | . | 100 | ( 0.999 )10Â = 90% | . | 1000 | ( 0.999 )1000Â = 37% | . | 10,000 | ( 0.999 )10,000Â = 0.004Â  | . A 37% probability of succeeding may sound bad, but for truly high-end computations, we might actually be okay with that. If the program results in a recipe for a brand-new medicine or tells us the perfect design for an aeroplane wing, then surely we donâ€™t mind repeating the computation 10 or 100 times, after which weâ€™re very likely to learn this breakthrough result. On the other hand, if the probability of success is 10-44, then we willÂ neverÂ find the right result, even if the computer repeats the program billions of times.Â  . In the table above, we see a pattern: to reasonably perform 102Â steps, we require errors of roughly 10-2Â or better. To perform 103Â steps, we need roughly a 10-3Â probability of error. These are very rough order-of-magnitude estimates, but they lead to a very valuable conclusion when dealing with very large circuits (or very small errors): if you want to execute 10nÂ steps, youâ€™d better make sure that your error probability is not much bigger than 10-n.Â  . This simplified model assumes that an operation either works correctly or fails completely, with nothing in between. In reality, quantum operations act on continuous parameters, and therefore, they have an inherent scalar-value accuracy. For example, a quantum gate with 99% accuracy might change a parameter from A to A+0.49, where itâ€™s supposed to do A+0.5. Luckily, for our discussion, these details donâ€™t matter much. It suffices to see a â€˜99% accurateâ€™ quantum gate as simply having a 99% probability of succeeding. We also overlook various other technical details, like operations carried out in parallel, different types of errors, native gate sets, connectivity, and so forth â€” these make the story much more complicated but will not change our qualitative conclusions. Why donâ€™t we just make the hardware more stable? To some degree, we can further reduce errors by creating more accurate hardware. However, quantum objects are so incredibly fragile that even getting down to 10-2Â errors requires some of the worldâ€™s most astonishing engineering. We definitely hope to see two-qubit gate errors reduced to 10-3Â and perhaps even 10-4, but achieving targets of 10-9Â seems unlikely with incremental hardware engineering alone. On the other hand, quantum error correction is incredibly effective: the error drops dramatically at the cost of adding a modest number of qubits, which is assumed to be scalable anyway. Thatâ€™s why experts agree that error correction is the right way forward.Â Â  . Do we use error correction in classical computers too? This might be a good moment to appreciate the incredible perfection of classical computer chips. While doing billions of steps per second, running for months in a row, sometimes with hundreds of cores at a time, errors in CPUs practically never occur. I was hoping to find hard numbers on this, but companies like Intel and AMD seem to keep this under stringent non-disclosure agreements. However, someÂ researchÂ shows that errors well under 10-20Â are easily attained as long as we donâ€™t push processors to their limits (in terms of voltages and clock speeds), sufficiently low that error correction is rarely needed. Memory (RAM) for high-performance computers still frequently has built-in error correction, and some form ofÂ CPU error correctionÂ was sometimes used in older mainframes and (even today) inÂ space probes.Â  . ",
    "url": "/advanced/error_correction/#what-is-error-correction",
    
    "relUrl": "/advanced/error_correction/#what-is-error-correction"
  },"53": {
    "doc": "Error correction",
    "title": "Longer computations need more qubits",
    "content": "As problems become more complex, they typically require better computer hardware, both in terms of width (number of bits) and depth (number of steps). We could illustrate this below. We define a number â€˜Nâ€™ that indicates the difficulty or the size of the problem. For example, we might consider the task of â€˜factoring a number that can be written down using at most N bitsâ€™).Â  . Remember that weâ€™re talking about the requirements to solve a problem, so here, width indicatesÂ logicalÂ bits. If a computer does not have error correction, then one logical bit is simply the same as one physical bit â€“ or its quantum equivalent.Â  . For â€˜perfectâ€™ classical computers, the situation is straightforward: if a problem gets bigger, we need more memory, and we need to wait longer before we obtain the result. For (quantum) computers that make errors, the situation is more complex. With increasing depth, not only do we need to wait longer, but we also need to lower the error probabilities and, hence, need more extensive error correction.Â  . Letâ€™s consider two computers for which we show the width and depth that they can handle (where the available â€˜depthâ€™ is assumed to be 1 / â€˜probability of errorâ€™). On the left is a computer without error correction (hence, it has a small, fixed depth). The other is an error-corrected computer that can trade between depth and width (in certain discrete steps).Â  . The computer without error correction might have enough memory to solve a problem but often lacks the depth. Even an error-corrected computer might not have a suitable trade-off to solve the hardest problems. Looking at the above example, it seems that both computers can solve the N=10 problem. Here, only the error-corrected computer can solve the N=20 problem, as depicted below. For the N=40 problem, which would be represented by an even larger box, the error-corrected computer might have sufficient depth OR sufficient width, but it doesnâ€™t have both at the same time. Hence, neither computer could solve the N=40 problem.Â  . Towards cracking the N=40 problem, our best bet is to upgrade the error-corrected computer to haveÂ more physical qubits. Using error correction, these can be traded to achieve sufficient depth (whilst also reserving just enoughÂ logical qubitsÂ to run the algorithm).Â  . We have found a paradoxical conclusion here. Larger problems not only require more memory (to store the calculation) but also more depth, which requires more qubits again! To summarise:Â  . â€˜Harderâ€™ problems -&gt; More depth -&gt; Better error correction -&gt; More physical qubitsÂ  . Once we reach an era of error correction, scaling the number of physical qubits will still be at the top of our wishlist, as this will be the key enabler of longer computations. ",
    "url": "/advanced/error_correction/#longer-computations-need-more-qubits",
    
    "relUrl": "/advanced/error_correction/#longer-computations-need-more-qubits"
  },"54": {
    "doc": "Error correction",
    "title": "What is the current state-of-the-art?",
    "content": "This section is more technical and can be safely skipped. As of 2024, there have been several demonstrations of error correction (and the slightly less demanding cousin: errorÂ detection), but these have all been with limited numbers of qubits and with very limited benefit to depth (if any at all). However, we seem to be at a stage where hardware is sufficiently mature that we can start exploring early error correction.Â  . Below are the three most popular approaches to error correction. Each of them can be considered a â€˜familyâ€™ of different methods based on similar ideas: . | Surface codes . | Colour codes . | Low-Density Parity Check (LPDC) codes . | . The surface code (or toric code) has received a lot of scientific attention, as this seems to be on the roadmap of large tech companies like Google and IBM. Their superconducting qubits cannot interact with each other over long distances, and the surface code can deal with this limitation. Many estimates that we use in this book (such as the resources required to break RSA or to simulate FeMoco) are based on this code. It has already been tested experimentally on relatively small systems: . | A team from Hefei/Shanghai experiments with a 17-qubit surface code. | Google sees improvements when scaling the surface code from 17 to 49 qubits. | . Colour codes are somewhat similar to surface code but typically lack the property that only neighbouring qubits have to interact. This makes them less interesting for superconducting or spin qubits, but they appear to work extremely well for trapped ions and ultracold atoms.Â  . | (Scientific presentation) Startup QuEra demonstrates 48 logical qubits using a colour code . | Already in 2014, an early experiment on a single logical qubit (colour code) was performed in Innsbruck. | . LDPC codes are now rapidly gaining attention. They build on a large body of classical knowledge and could have (theoretically) more favourable scaling properties over the surface code.Â  . | French startup Alice &amp; Bob is aiming for a unique combination of â€˜cat qubitsâ€™ together with LDPC codes, which can theoretically match very elegantly.Â  | . Which code will eventually become the standard (if any) is still completely open.Â Â  . What are the main challenges? . Firstly, we would need justÂ slightlyÂ more accurate hardware. We mentioned a certain accuracyÂ thresholdÂ earlier: state-of-the-art hardware seems to be close to this threshold but not comfortably over it. Secondly, error correction also requires significant classical computing power, which needs to solve a fairly complex â€˜decodingâ€™ problem within extremely small time bounds (within just a few clock cycles of a modern CPU). Classical decoding needs to become more mature, both at the hardware and the software level. It is not unlikely that purpose-built hardware will need to be developed, which for some platforms might be placed inside a cryogenic environment (placing stringent bounds on heat dissipation). Theoretical breakthroughs can still reduce the requirements of classical processing.Â  . Lastly, it turns out that â€˜mid-circuit measurementsâ€™ are technically challenging. Without intermediate measurements, one might retroactively detect errors, but one cannot repair them. We should also warn that manyÂ related terms exist, such as â€˜error mitigationâ€™ and â€˜error suppressionâ€™. They might be useful for incremental fidelity improvements, but they donâ€™t bring an exponential increase in depth like proper error correction does.Â  . ",
    "url": "/advanced/error_correction/#what-is-the-current-state-of-the-art",
    
    "relUrl": "/advanced/error_correction/#what-is-the-current-state-of-the-art"
  },"55": {
    "doc": "Error correction",
    "title": "Conclusion",
    "content": "The bottom line is that one shouldnâ€™t naively take â€˜logical qubitsâ€™ as perfect building blocks that will run indefinitely. A logical qubit is no guarantee that a computer has any capabilities; it merely indicates that some kind of error correction is applied (and it doesnâ€™t say anything about how well the correction works). A much more interesting metric is the probability of error in a single step (in jargon: the fidelity of an operation), which gives a reasonable indication of the number of steps that a device can handle! . ",
    "url": "/advanced/error_correction/#conclusion",
    
    "relUrl": "/advanced/error_correction/#conclusion"
  },"56": {
    "doc": "Error correction",
    "title": "See also",
    "content": ". | The Quantum Threat Timeline ReportÂ asked several experts what they find the most likely approach to fault-tolerance (section 4.5).Â  . | British startupÂ Riverlane builds a hardware chipÂ that â€˜decodesâ€™ which error occurred on logical qubits. (Technical report).Â  . | Craig Gidney (Google) has aÂ more technical blog postÂ on why adding physical qubits will remain relevant in the following decades.Â  . | [Technical!] SomeÂ scientificÂ work speaks of â€˜early fault-tolerantâ€™ quantum computing, such as: . | â€˜Early Fault-Tolerant Quantum Computingâ€™, discussing how we can squeeze as much as possible out of limited devices. | â€˜Assessing the Benefits and Risks of Quantum Computersâ€™ takes a similar width x depth approach as we do here, but uses it to assess what applications will be within reach first. | . | . ",
    "url": "/advanced/error_correction/#see-also",
    
    "relUrl": "/advanced/error_correction/#see-also"
  },"57": {
    "doc": "Quantum hardware",
    "title": "Quantum hardware",
    "content": "Reading time: 7 minutes . Contents . | Different functionalities | Different building blocks | . Conventional computer hardware is extremely reliable. Professional servers are supposed to run non-stop for years without any hardware failures. If you take a new product out of a box, you can be reasonably sure that it will work precisely as advertised - and even if not, it should be straightforward to replace. Moreover, classical IT is extremely well-standardised. No matter what supplier you buy a computer from, you can be reasonably sure you can run your favourite applications on them. Thanks to such high reliability and clear compatibility, it is rather easy to compare different machines, for example, by looking at speed (e.g. floating-point operations per second, FLOPS) and memory size. We will see that this is radically different for quantum computers. Devices make mistakes, have limited functionalities, and memory is scarce compared to classical computing standards. Several manufacturers focus on niche applications, making trade-offs in certain features to enhance performance in others. In this chapter, we take a high-level perspective at quantum computing hardware. We address the two most important aspects: . | What functionality does a device have? . | What type of qubits are used? . | . ",
    "url": "/advanced/hardware/",
    
    "relUrl": "/advanced/hardware/"
  },"58": {
    "doc": "Quantum hardware",
    "title": "Different functionalities",
    "content": "The figure below shows three different functionalities that quantum computers can have (top, red), along with some examples of products on the market (yellow), built from different building blocks. This list is by no means complete! It should, at best, give an indication of the current state of the art. Let us start by taking a closer look at the functionalities. Our biggest dream is to have aÂ â€˜universal quantum computerâ€™. The word â€˜universalâ€™ indicates that it can execute any quantum algorithm (or, technically, it can approximate any algorithmâ€™s output to arbitrary precision). For comparison, your laptop, phone, and even a modern coffee machine are universal classical computers, making them capable of running any classical application you can think of: spreadsheets, 3D games, data encryption, and so on. Similarly, a proper universal quantum computer is suitable for any quantum application, regardless of whether it is already known today or invented in the future.Â  . The definition of â€˜universalâ€™ is blind to some details, such as memory limitations (it assumes you will never run out of RAM), and omits tedious details about software compatibility (a PlayStation game wonâ€™t run on an Xbox). In our high-level overview, such details are unimportant: the main point is that there also exist devices that canÂ notÂ run just any algorithm. Does a universal computer need to be â€˜gate-basedâ€™? No, there are various computational models that are universal. There are different ways to make a â€˜universal quantum computerâ€™. The most popular way is to use aÂ gate-basedÂ approach, where elementary operations (â€˜gatesâ€™) change the data stored one or two qubits at a time. This perspective is most intuitive for those used to conventional logical circuits (with AND, OR and NOT gates), and most quantum algorithms are presented in this language. Other alternatives includeÂ adiabaticÂ computation andÂ measurement-basedÂ computation, which can theoretically run any algorithm written for a gate-basedÂ computer without issues and vice versa.Â  . At this moment, gate-based computers are by far the most widespread and appear to be the most popular approach in the race towards a million-qubit quantum computer: nearly all large tech companies rely on this architecture. There is one important exception. SomeÂ photonics startupsÂ are working towards measurement-based computing, as this overcomes the challenges in performing â€˜entanglingâ€™ quantum gates with photons. In the following, we will focus mostly on gate-based computers. No matter what architecture or qubit type you pick, todayâ€™s technology will only allow you to run relatively short computations. This is due to the inherent imperfections in qubit construction and control methods. The imperfections cause errors to accumulate, so after some number of steps, the result is almost surely corrupted and unusable. For longer computations, fixing errors on the fly is essential, using so-calledÂ error correction. At the time of writing, we live in the so-called NISQ era, withÂ Noisy Intermediate-Scale Quantum devices. Many are theoretically fully universal, except that they are limited both in the number of qubits and, most of all, in the number of steps they can execute. Companies like IBM, IonQ, Quantinuum, and Pasqal all have NISQ computers available to test over the cloud.Â  . A universal computer is a jack-of-all-trades, but it excels at nothing. Engineers can makeÂ special-purpose devices that improve in certain areas (like the number of qubits or clock speed) by omitting certain functionalities. AÂ quantum simulator specialises in mimicking the behaviour of a particular class of materials or molecules. The precise capabilities can be described in the mathematical language of a â€˜Hamiltonianâ€™ that specifies which materials qualify. For example, Harvard-spinoff QuEra offers a quantum simulator over the cloud that mimics a quantum Ising model1. Todayâ€™s simulators (like QuEraâ€™s) are fairly similar to a universal NISQ computer, missing only a few essential ingredients, and similarly having restrictions due to noise. Although they look similar, they are not designed to run conventional (gate-based) algorithms. The jargon around simulators can be a bit confusing. Firstly, the term â€˜quantum simulationâ€™ is also used when a classical computer tries to calculate the output of a quantum algorithm. To differentiate, some prefer the term â€˜emulationâ€™ for such classical approaches.Â Secondly, we often hear a distinction between â€˜analogâ€™ and â€˜digitalâ€™ simulation. Ironically, both approaches tend to discretise information over discrete qubits (which Iâ€™d call digital). In practice, the terms are rather used to distinguish between continuous and discrete time steps. An analog simulation would use longer, continuous operations on the qubits, whereas a digital simulation uses quantum gates that act in short, discrete bursts on the qubits. Another special-purpose device is theÂ quantum annealer,Â popularised mainly by the Canadian scale-up D-Wave. These special-purpose devices can solve a specific class of optimisation problems that goes by the name ofÂ QUBO: quadratic unconstrained binary optimisation. There is a well-developed theory of mapping various industrial problems into the QUBO formalism, making annealers fairly versatile machines. However, quantum annealers will never be able to take advantage of the various other quantum algorithms out there: even with enough qubits, we wonâ€™t see them cracking codes using Shorâ€™s algorithm.Â  . Further reading . | D-Waveâ€™s introduction to its quantum annealing platform . | Scale-up Pasqal reports on a material science simulation with 196 qubitsÂ andÂ sells a 100-qubit simulator in a EuroHPC tender. In another article, they explain why an â€˜analogâ€™ quantum simulation has its advantages. | QuEra makes a 256 qubit simulatorÂ available over the Cloud.Â  . | . ",
    "url": "/advanced/hardware/#different-functionalities",
    
    "relUrl": "/advanced/hardware/#different-functionalities"
  },"59": {
    "doc": "Quantum hardware",
    "title": "Different building blocks",
    "content": "Another important question concerns the materials used to create qubits. Scientists have cooked up several competing approaches, such as superconducting materials, photons, individual atoms, or ions, each with their own strengths and weaknesses. When comparing different qubits, we use the terminology of qubit implementation, the qubit type, or (what we prefer) qubitÂ platform.Â  . The conventional computer electronics industry has settled on a single choice of material and manufacturing process: essentially, all computer chips are made using lithography on silicon wafers. On the contrary, there is an ongoing race between wildly different qubit platforms, and it is still unclear which will eventually be the winner â€” or whether we will converge to a single winner at all.Â  . There is fascinating physics behind the different hardware types, but we wonâ€™t delve into that in this non-technical book (would you otherwise care what material your classical CPU is made of?). However, as soon as you want to test a prototype quantum program on real-world NISQ hardware, you probably want to learn more details. Interested readers are invited to take a look at the references below. It is interesting to note that all these different functionalities (universal computers, annealers, and simulators) can, in principle, be built using any type of qubit. Returning to the figure at the top, you can see that specific qubit platforms have been used for multiple purposes, and itâ€™s not unlikely that the empty fields will also be populated in the future. Further reading . | Different types of qubits explained by Sifted.eu . | Different types of qubits at IQC Waterloo . | Different types of qubits on Wikipedia . | A MOOC about different hardware types by TU Delft . | . | Hamiltonian simulation on QuEraâ€™s 256-qubit Aquila machine. https://www.quera.com/events/hamiltonian-simulation-on-queras-256-qubit-aquila-machine (Accessed: 10 September 2024).Â &#8617; . | . ",
    "url": "/advanced/hardware/#different-building-blocks",
    
    "relUrl": "/advanced/hardware/#different-building-blocks"
  },"60": {
    "doc": "What steps should your organization take?",
    "title": "What steps should your organisation take?",
    "content": "Reading time: 12 minutes . Contents . | Common first steps | Prepare to use quantum applications | Migrating to post-quantum cryptography | . In the previous chapters, we discussed theÂ use cases, theÂ threats,Â and theÂ timelines of quantum technologies. We will now look at the strategic perspective of a typical non-quantum enterprise. We will assume a typical large-scale organisation that does not sell IT products per se, but relies heavily on computing infrastructure to optimise its operations, supervise processes, communicate with suppliers and clients, and potentially invest in computer-aided R&amp;D. While these organisations may be excited about the potential of quantum computing, they may also feel vulnerableâ€”whether due to competitors advancing ahead or due to hackers attacking legacy cryptography. The first steps, like growing expertise, finding adequate staff, and doing first proof-of-concept studies, will be largely sector-independent. Further steps can become more organisation-specific, and we will highlight several tools for tailored assessment. For more bespoke advice beyond this chapter, organisations would typically resort to specialised consultancies. The larger players have extensive writings, for example: . | Capgemini â€“ Quantum technologies: How to prepare your organisation for a quantum advantage now . | McKinsey â€“Â Quantum computing use cases are getting realâ€”what you need to know . | BCG â€“Â Quantum Computing Is Becoming Business ReadyÂ  . | . We find most of these sources somewhat hyped, emphasising the risks of missing out and urging organisations to start whichever quantum project as soon as possible. Nevertheless, practically all sources (whether academic, governmental or consultancies) agree on the first strategic steps that one should take. We break these down into three stages below. ",
    "url": "/advanced/strategic-actions/#what-steps-should-your-organisation-take",
    
    "relUrl": "/advanced/strategic-actions/#what-steps-should-your-organisation-take"
  },"61": {
    "doc": "What steps should your organization take?",
    "title": "Common first steps",
    "content": "Step 1: Start with no-regret moves . Most companies start with early steps aimed at better understanding the situation. These can be done with very little financial risk. Some must-do actions: . | Appoint a quantum lead or a quantum working group tasked with following the developments. | Read up and learn. If youâ€™ve come this far in this Guide, youâ€™re already doing a fantastic job.Â We have a separate chapter on further learning resources. | Create internal awareness. Many employees will enjoy inspirational talks, tours or demonstrations that academics or quantum manufacturers can provide. | . Optionally: . | Put quantum on the agenda with senior management.Â  . | Involve collaborators, suppliers and vendors, and make your interest in quantum known. It is to your benefit if suppliers are well-prepared.Â  . | Participate in a workshop, hackathon, or similar event. | . Towards more concrete actions, it makes sense to split your quantum journey into two different categories: . a. Preparing forÂ quantum applications,Â where the goal is to leverage quantum technologies to gain some competitive advantage (for example, by strengthening your R&amp;D, further optimising your logistics, improving a product, etc). b. Migrating toÂ quantum-safe cryptography, where the goal is to keep your IT secure against attackers with a quantum computer. These endeavours serve very different purposes and are likely spearheaded by different departments. Hence, it seems logical to break these down into separate projects. We discuss further steps in both directions separately. ",
    "url": "/advanced/strategic-actions/#common-first-steps",
    
    "relUrl": "/advanced/strategic-actions/#common-first-steps"
  },"62": {
    "doc": "What steps should your organization take?",
    "title": "Prepare to use quantum applications",
    "content": "Step 2a: Explore use cases . At this stage, most organisations will want to make low-regret moves that get them prepared to leverage quantum technologies fairly soon after practical utility becomes available. Some of the bottlenecks could be the lack of in-house knowledge, a limited available workforce, or a long timeline to integrate quantum applications in production environments. Must do: . | Identify the most impactful use cases in your sector. | Sketch a road map for the coming years.Â  . | . Optionally:Â  . | Start concrete proof-of-concept projects. Right now, these are unlikely to offer practical utility and will likely tackle just a toy problem. However, these help build experience in setting up quantum projects and can uncover â€˜unknown unknownsâ€™. For staff with a strong physics or mathematics background, it is relatively accessible (and fun!) to get acquainted with quantum programming packages andÂ implement a first test algorithm. | Find strategic partners. Organisations can save costs by collaborating on early, pre-competitive exploration.Â  . | Create PR! We notice that many companies are very actively promoting their early results on quantum applications, even if these do not offer significant advantages yet.Â  . | Hire staff with a strong background in quantum technologies who understand the market, have the right skills to lead proof-of-concept studies, and can offer advice for strategic decisions. | . Step 3a: Implementing actual applications, whenever ready . From here onwards, it gets increasingly difficult to give concrete advice, as priorities may depend on your business and on the way the field of quantum computing will progress. Several sources will simply tell you do â€˜develop a long-term strategyâ€™ or similar. Others highlight the need to â€˜remain agileâ€™ to quickly adapt to this rapidly evolving field. For inspiration or a dot on the horizon, you may think towards a competence centre for quantum computing, similar to how many companies have special departments for data science and/or AI. A concrete task could be to elaborate on the list of impactful use cases from the previous step, benchmarking the performance of various quantum and classical software tools. Another task could be to professionalise an earlier proof-of-concept project, bringing it closer to implementation in a production environment. Identifying fruitful use cases . From a top-down perspective, it is a good exercise to identify your current needs in high-performance computing.Â What do you currently spend your computing budget on? Are there any areas where new tools in computation or modelling could provide serious business value (for example, by being faster, tackling bigger problems, or delivering higher accuracy)? Which quantities would you ideally have calculated but are beyond the reach of current computers?Â This results in a longlist of use cases where new computational tools are worth further investigation. The next step would be to research to what extent a quantum computer (or whichever other new computational tool) offers any advantage. We recommend this top-down approach because it can lead to conclusions sooner, especially because it will identify use cases that are not worth your time (for example, because additional computational power provides little value). It is also possible to take a bottom-up approach. Looking at the available quantum algorithms, which would speed up processes in your existing IT? Would any of them provide value for your business? This more technical perspective requires some in-depth quantum expertise but can definitely be worth the effort, especially if you have people with the right skills available. The Quantum Application Lab is a collaboration between various Dutch research organisations. They invite end-users to explore the benefits of quantum computers in projects that last anywhere between 3 and 12 months, ranging between a first exploration of use cases to advanced development of quantum prototype software. Several example projects can be found on their website: www.quantumapplicationlab.com. Further reading . | Scientists propose a framework to discover which real-world problems are potentially accelerated by quantum computers.Â Â  . | Consultant Olivier Ezratti proposes a framework to assess the maturity of quantum computing case studies. | (Youtube) A recording of Quantum.Amsterdamâ€™s online seminar â€˜What do companies get out of quantum projects today?â€˜ . | . What does an R&amp;D collaboration with academia look like? Several end-users have started collaborations with universities to better understand the use cases of quantum computing. This is often a win-win situation, as companies can learn from renowned experts at relatively low costs, whereas academics benefit from additional funding and showcasing that their research has practical interests. Moreover, many countries provide subsidies for so-called â€˜public-private partnershipsâ€™. Below, I will sketch my personal experience with the process of starting a public-private partnership. You will most likely be dealing with a universityâ€™s tech transfer office (TTO), which specialises in making in-house knowledge available externally. As a first step, it is important to agree on the scope of the project: what are the research questions, what are the expected outcomes, how long will the project run, and so forth. Ideally, this would be a discussion between an expert from your organisation and a universityâ€™s (assistant) professor. The professor will most likely take a supervising role, as the actual work is often executed by a junior researcher employed as a PhD candidate or a postdoctoral (PD) researcher. PhD programs take relatively long, 3-5 years depending on your locale, and it may take some time before the first results come in. Postdoc projects often take 1-3 years and can lead to results sooner, but as of 2024, it can be much harder to hire a postdoc with the right competencies. When the topic and duration of the project are clear, it is important to discuss details around intellectual property (IP), often done by legal experts. For universities, it is important that researchers can keep building upon the projectâ€™s IP in an academic setting. Moreover, they will demand that the results can be published in scientific journals. At the same time, a paying company will want sufficient options to patent new discoveries and will require exclusive use of the IP within their sector. These demands do not necessarily conflict with each other, and in principle, it should be possible to find an arrangement that satisfies both parties. Â  . A straightforward way to ensure that the company learns from the academic developments is by organising meetings or workshops throughout the collaboration project, in which the ongoing R&amp;D is discussed with company staff. The occasional dialogue with company staff is arguably more important than a shiny final report or paper, which risks disappearing in someoneâ€™s drawer. ",
    "url": "/advanced/strategic-actions/#prepare-to-use-quantum-applications",
    
    "relUrl": "/advanced/strategic-actions/#prepare-to-use-quantum-applications"
  },"63": {
    "doc": "What steps should your organization take?",
    "title": "Migrating to post-quantum cryptography",
    "content": "This section relies on technical knowledge from the previous chapter on cybersecurity. Step 2b: Prepare your migrationÂ  . Cryptography is a completely different beast, with a more concrete goal, and more urgent timelines for most organisations. Contrary to the applications in the previous section, the cryptography migration is not optional. Luckily, most organisations face the same problem, and there is ample research on effective steps. The core challenge is to upgrade all existing public key cryptography to Post-Quantum Cryptography (PQC) in the next decade, which could be spread over hundreds or thousands of different applications. Many businesses, especially those dealing with critical infrastructure, may additionally deal with regulators who may or may not have guidelines ready. Moreover, IT transitions can be incredibly slow - it is not uncommon to see plans that cover 5 or even 10 years1. Authorities seem to agree that the following initial steps should be taken urgently by all large organisations. | Create awareness: make sure that the quantum threat is well-understood in your security departments and among IT managers and product owners throughout the organisation. | Create an inventory of cryptographic assets used within the organisation. This should include both software and hardware and should clearly specify the used algorithms, whether developed in-house or purchased from a vendor. Some parties refer to a â€˜cryptographic bill of materialsâ€™ (CBOM). | Determine the risk and urgency of PQC migration. Most organisations already perform regular risk assessments of their IT infrastructure. Additionally, organisations should assess whether they classify as an urgent adopter of PQC (see below). | Create a migration plan. This is a more complex step, which should at least prioritise which assets must be migrated first and indicate whether the migration of all urgent systems can be realistically achieved in time, before the arrival of cryptographically relevant quantum computers. | . For more details, we recommend following theÂ PQC Migration Handbook, a free guide written by the Dutch secret service AIVD and research organisations CWI and TNO.Â Security authorities in other countries have made similar guidance available. Are you an urgent adopter? . Planning ahead to transition to new cryptography can be more critical depending on the organisation. We can distinguish betweenÂ regular and urgent adopters. You are an urgent adopter when you: . | Handle sensitive or personal data with a long confidentiality span. | Handle critical infrastructure on which large groups of people rely. | Provide systems with a long lifespan; hence, your products will still be around when quantum computers are available. | . Based on these criteria, a significant fraction of organisations would classify as urgent adopters, such as banks, governments, car manufacturers, grid operators, hospitals, and so forth. Examples of non-urgent adopters could be schools, webshops, travel agencies, some construction agencies, and so forth. Urgent adopters are encouraged to start their migration as soon as possible if they havenâ€™t already. Step 3b: Migrate . This is a much more technical step for which you will need a well-prepared migration plan from the previous step. Organisations are strongly discouraged from implementing their own cryptographic functions. The best practice is to rely on standard libraries written by cryptographic experts, which should be safe against a broad spectrum of attacks and have seen careful reviews. We expect NISTâ€™s standards to soon be available in popular open-source packages like OpenSSL or BouncyCastle. This makes the migration less technical, although organisations still deal with the operational challenge of updating a huge number of applications within a limited time. Due to harvest now, decrypt later attacks, most organisations will focus on updating key exchange algorithms before updating digital signature methods. On the technical side, cryptographic experts recommend the use of hybrid algorithms that combine the strengths of PQC (to defend against quantum attacks) with a proven conventional public key algorithm (which guarantees at least the original security in case the new PQC algorithm turns out to be less safe than expected). For example, early versions of quantum-safe connections with the Chrome web browser use a combination of X25519 and Kyber-768 (ML-KEM). Moreover, the practice of cryptographic agility is strongly encouraged, meaning that security protocols can be easily updated and replaced. This is a vague term that isnâ€™t just a software feature - it requires alignment with business protocols and internal policies. Further reading . | To learn more about transitioning to quantum-safe cryptography, we strongly recommend theÂ PQC Migration Handbook written by the Dutch secret service AIVD and research organisations TNO and CWI. | An extension to the handbook is the PQChoiceAssistant, a tool that recommends what cryptographic algorithms are best used in specific situations. | In 2022, the NSA published requirements for national security systems. They indicate a timeline with concrete deadlines between 2025 and 2033. | . | To illustrate, the PQC Migration Handbook mentions that â€˜Judging from previous migrations this process might take well over five yearsâ€™. The NSAâ€™s requirements for national security systems, published in 2022, demand that quantum-safe algorithms be exclusively used from 2033 onwards.Â &#8617; . | . ",
    "url": "/advanced/strategic-actions/#migrating-to-post-quantum-cryptography",
    
    "relUrl": "/advanced/strategic-actions/#migrating-to-post-quantum-cryptography"
  },"64": {
    "doc": "What steps should your organization take?",
    "title": "What steps should your organization take?",
    "content": " ",
    "url": "/advanced/strategic-actions/",
    
    "relUrl": "/advanced/strategic-actions/"
  },"65": {
    "doc": "Further reading",
    "title": "Further reading",
    "content": "Below, we give a selection of recommended sources to learn more about this fascinating topic. ",
    "url": "/resources/further-reading/",
    
    "relUrl": "/resources/further-reading/"
  },"66": {
    "doc": "Further reading",
    "title": "I want to learn the technical details",
    "content": "For (late) high school students: . (or those who followed high-school level mathematics): . | Quantum QuestÂ [Book/website] is an intensive 5-week online course about the theoryÂ (mathematics) of quantum computing. Materials are freely available for self-study.Â  . | Quantum in Pictures (Cooke) [Book] teaches the theory (mathematics) of quantum computing using diagrams. | . Undergraduate (Bachelorâ€™s) university level: . | Quantum.CountryÂ [Website] â€“ the â€˜Duolingo of Quantum Computingâ€™, a very well-written introduction for those with a late high-school or early university-level math background.Â  . | Quantum Computation and Quantum InformationÂ (Nielsen, Chuang) [Book] â€“ the â€˜bible of quantum computingâ€™. Perhaps not the most up-to-date, but definitely the most well-known resource in our field. Sets the standards for jargon and notation.Â  . | . | Quantum Computer Science: An IntroductionÂ (Mermin)Â [Book] â€“ a well-written introduction, with quite some focus on manipulating quantum circuits. | Quantum Computing Since DemocritusÂ (Aaronson) [Book] â€“ Aaronson is an authority in the field. His book touched upon many topics, such as the foundations of computer science, black holes and consciousness, making it a good read for those looking for something much broader than just quantum computing. | . Graduate (Masterâ€™s) level: . These assume no prior knowledge about quantum physics but require a strong background in mathematics (i.e. linear algebra, calculus, advanced inequality bounds and approximations, etc.). In exchange, they go into much more detail.Â  . | Lecture Notes for UvA course â€˜Quantum Computingâ€™ by Ronald de Wolf,Â which is frequently updated and features some cutting-edge algorithms. Via theÂ course website, you can find the link and password to view all the recorded lectures.Â  | . | Lecture Notes for Caltech course â€˜Quantum Computingâ€™ by John PreskilÂ  | . Scientific overview papers . The papers below are aimed at scientists from fields other than quantum computing itself. All papers we mention are open-access and peer-reviewed, making them very suitable for citation.Â  . | Quantum algorithms: an overviewÂ (Ashley Montanaro) . | The Potential Impact of Quantum Computers on SocietyÂ (Ronald de Wolf) [video lecture] . | . Scientific opinions and discussions . | Scott Aaronsonâ€™s blog.Â Although written from a theoretical computer science perspective, this blog addresses a very broad range of quantum computing topics. Prof. Aaronson has a strong authority in the field, and his posts attract readership and comments from a broad range of prominent scientists.Â  | . ",
    "url": "/resources/further-reading/#i-want-to-learn-the-technical-details",
    
    "relUrl": "/resources/further-reading/#i-want-to-learn-the-technical-details"
  },"67": {
    "doc": "Further reading",
    "title": "I want to learn to program a quantum computer",
    "content": "Several programming packages for quantum computers exist, mostly maintained by major hardware providers. All of them offer great introductory tutorials. The ones we recommend below are all in Python.Â  . | Qiskit, the language by IBM, probably features the largest catalogue of learning materials. To start from scratch, we recommend following the â€˜Basics of Quantum Informationâ€˜, which teaches both the mathematics behind qubits and the usage of the package itself. | Cirq is a very similar package developed by Google. As of 2024, they have a more focused tutorial to explain the programming package itself without extensive theory of quantum mechanics. | QWorld BronzeÂ offers tutorials in the form of Jupyter notebooks and hosts various training days around the world, mostly focused on Qiskit and sometimes ProjectQ. | PennyLane is a package by startup Xanadu with a strong focus on machine learning applications. | Classiq is one of the largest players that focuses on a higher-level programming language. This makes it easier to re-use code and to synthesise circuits for different types of hardware, but it also requires more background knowledge to get started. | . ",
    "url": "/resources/further-reading/#i-want-to-learn-to-program-a-quantum-computer",
    
    "relUrl": "/resources/further-reading/#i-want-to-learn-to-program-a-quantum-computer"
  },"68": {
    "doc": "Further reading",
    "title": "I want to stay up-to-date with the latest developments",
    "content": "Major business conferences . | Q2B (organised by QCWare) . | IQT (Inside Quantum Technology) . | Quantum.Tech (organised by Alpha Events) . | Commercialising Quantum (organised by The Economist) . | . Major scientific conferences . These are very technical and only recommended for those acquainted with theÂ field. They take place at a different location each year. | Quantum Information Processing (QIP) . | Theory of Quantum Computation, Communication and Cryptography (TQC) . | Quantum Computing Theory in Practice (QCTIP) (mostly based in the UK) . | . Business News . | Quantum Computing Report - donâ€™t be fooled by the basic look on the website. The content is written with a very critical eye and with very relevant contextual information, making it our favourite source for quantum-related news. Â  | . | The Quantum Insider | . Scientific news . None of these focus exclusively on Quantum Technology, but all offer high-quality news (and surely none would miss any important quantum breakthroughs).Â  . | Quanta Magazine . | Phys.org . | . ",
    "url": "/resources/further-reading/#i-want-to-stay-up-to-date-with-the-latest-developments",
    
    "relUrl": "/resources/further-reading/#i-want-to-stay-up-to-date-with-the-latest-developments"
  },"69": {
    "doc": "Further reading",
    "title": "I want to learn more about business implications",
    "content": "Several sources cover similar topics as this book. Most of these come from consultants of hardware providers who have a financial interest in making others get started with quantum. In our opinion, the articles are sometimes too optimistic and predict that quantum applications will come much sooner than the typical expert would anticipate. On the other hand, they collect insightful details about financial matters.Â  . | McKinsey publishes yearly â€˜Quantum Technology Monitorâ€™ reports, focusing on the economic impact the quantum computers will have. | Cloudflareâ€™s support pages contain an incredibly complete bible of Post-Quantum Cryptography. | Are you looking for a much more extensive source that covers pretty much everything there is to know about quantum computers? French consultant Olivier Ezratti maintains a 1500+ page book, â€˜Understanding Quantum Technologiesâ€˜. | . Workshops and trainings . Short workshops will likely cover content similar to this book. A one-afternoon trainingÂ can be particularly useful to inspire your colleagues and friends. | The Workshop General Awareness Quantum Computing follows the same philosophy as this book: an introduction to business opportunities that should be understandable for everyone. | Qureca is a British startup that offers several trainings, such as â€˜Quantum for everyoneâ€™ and â€˜Quantum Training for Businessâ€™. | . ",
    "url": "/resources/further-reading/#i-want-to-learn-more-about-business-implications",
    
    "relUrl": "/resources/further-reading/#i-want-to-learn-more-about-business-implications"
  },"70": {
    "doc": "Hardware available today",
    "title": "Overview of quantum computers available today",
    "content": "This list shows a selection of the larger quantum computers as of August 2024, based on publicly available sources. The list is not exhaustive, there are many other systems that are not mentioned here. | Â  . Company . | #Qubits + chip name | Bold claims | Platform + notes | . | IBM | 1121Â  . â€œCondorâ€ . | ~25 systems permanently available over the cloud. Claims first â€˜utilityâ€™ with 127 qubits.Â  | Superconducting + Fast . + Precise gates . - Limited connectivity . | . | 127 . â€œEagleâ€ . | . | Rigetti | 79 . â€œAspen-M-3â€ . 83 . â€œAnkaa-2â€ . | Some systems available over the cloud.Â  | . | Google | 105 . 72 . â€œSycamoreâ€ . | 2019: First to claim quantum supremacy | . | University of Science and Technology of China, Hefei | 66 | 2021: Claimed quantum supremacy | . | IQM | 20 â€œGarnetâ€ | Â  | . | PsiQuantum | 0 | Targets 1M qubits in a first product. Obtained over 1B dollar of investments.Â  | Photonic . + Fast . - Imprecise . - Different formalism . | . | Quix | 20 modes | Â  | . | University of Science and Technology of China, Hefei | 100 modes, 50 photons, (equivalent to roughly 90 limited qubits).Â  | 2020: Second to claim â€˜quantum supremacyâ€™ | . | IonQ | 36 â€œForteâ€Â  | Achieves 35 â€˜algorithmic qubitsâ€™, effectively showing that relatively complex circuits can be executed.Â  | Trapped ions . + Connectivity . + Precise . -Â  Slow operations . | . | QuantinuumÂ  | 32 . â€œH1-2â€ . | Â  | . | 56 . â€œH2-1â€ . | . | Alpine Quantum Technologies | 24 | Â  | . | Pasqal | 100 for computer . 196 for simulator . | Â  | Cold atoms . + Connectivity . -Â  Slow operations . | . | QuEra | 280 | Published experiments with up to 48 logical qubits.Â  | . | D-Wave | 5000 | Â  | D-Waveâ€™s Quantum Annealers use superconducting qubits; specialized in a single algorithm: annealing. | . ",
    "url": "/resources/hardware-today/#overview-of-quantum-computers-available-today",
    
    "relUrl": "/resources/hardware-today/#overview-of-quantum-computers-available-today"
  },"71": {
    "doc": "Hardware available today",
    "title": "Hardware available today",
    "content": " ",
    "url": "/resources/hardware-today/",
    
    "relUrl": "/resources/hardware-today/"
  },"72": {
    "doc": "Quantum hype bingo",
    "title": "Quantum Hype Bingo",
    "content": "| â€˜Unprecedented capabilitiesâ€™ | â€˜Our algorithm solves â€¦â€™ (without comparison to classical computers) | â€˜Future-proof your businessâ€™ | Straightforwardly solving generic (NP-)hard optimisation problems | . | â€˜Harness the commercial potentialâ€™ | â€˜Game-changingâ€™ | Trying all solutions at once | â€˜Transformativeâ€™ | . | â€˜Unhackableâ€™ | Solving climate change | â€˜The next frontierâ€™ | â€˜X times fasterâ€™ (without fair benchmark) | . | Quantum parallelism | Quantum computers will replace classical computers | Get quantum-ready | Enable artificial general intelligence (AGI) | . ",
    "url": "/resources/hype-bingo/#quantum-hype-bingo",
    
    "relUrl": "/resources/hype-bingo/#quantum-hype-bingo"
  },"73": {
    "doc": "Quantum hype bingo",
    "title": "Quantum hype bingo",
    "content": " ",
    "url": "/resources/hype-bingo/",
    
    "relUrl": "/resources/hype-bingo/"
  }
}
