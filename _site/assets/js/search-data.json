{"0": {
    "doc": "Design showcase",
    "title": "LaTeX",
    "content": "LaTeX works when importing the MathJax javascript code? . \\[\\pi = \\frac{1}{2}\\] And inline it goes like \\( 3+3 = \\frac{1}{\\pi} \\) this. ",
    "url": "/design-tests.html#latex",
    
    "relUrl": "/design-tests.html#latex"
  },"1": {
    "doc": "Design showcase",
    "title": "Hyperlinks and (base)url",
    "content": "Correct way to set hyperlinks: . &lt;a href=\"/part1/chapter_2\"&gt;baseurl href to p1ch2&lt;/a&gt; . baseurl href to p1ch2 . Or use the link command (without baseurl) . &lt;a href=\"/part3/chapter_1/\"&gt;LINK href to p3ch1&lt;/a&gt; . LINK href to p3ch1 . ",
    "url": "/design-tests.html#hyperlinks-and-baseurl",
    
    "relUrl": "/design-tests.html#hyperlinks-and-baseurl"
  },"2": {
    "doc": "Design showcase",
    "title": "Tables",
    "content": "| Food | Description | Category | Sample type | . | Apples | A small, somewhat round … | Fruit | Fuji | . | Bananas | A long and curved, often-yellow … | Fruit | Snow | . | Kiwis | A small, hairy-skinned sweet … | Fruit | Golden | . | Oranges | A spherical, orange-colored sweet … | Fruit | Navel | . | Priority apples | Second priority | Third priority | . | ambrosia | gala | red delicious | . | pink lady | jazz | macintosh | . | honeycrisp | granny smith | fuji | . ",
    "url": "/design-tests.html#tables",
    
    "relUrl": "/design-tests.html#tables"
  },"3": {
    "doc": "Design showcase",
    "title": "Expanding boxes / accordions",
    "content": ". | This is Accordion 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. | . | This is accordion2; you can provide the contents inline! For details, see _includes . | . Detailsbox using capture mode The first line is automatically selected to be the “summary”. \\( 3+3 = \\frac{1}{\\pi} \\) is great. Alternatively, you can directly specify content... This is detailsbox.html. It takes the 2nd line as summary header (assuming the 1st line is just ‘\\n’.) Here is more text. And math: \\( \\pi + \\frac{1}{\\cos(3)} \\). Final line. A fancy Details box depending on in-doc CSS Give me attention or face the wrath of my claws give me attention or face the wrath of my claws and pretend not to be evil cats go for world domination allways wanting food. Eat owner's food playing with balls of wool and meow and walk away, and bleghbleghvomit my furball really tie the room together. Cuddle no cuddle cuddle love scratch scratch cat. ",
    "url": "/design-tests.html#expanding-boxes--accordions",
    
    "relUrl": "/design-tests.html#expanding-boxes--accordions"
  },"4": {
    "doc": "Design showcase",
    "title": "“At a glance” header",
    "content": "TL;DR . | Show the post in a flexible way. | Show the figures any place | Show the maths, the code blocks in a beautiful way. | and many things else… | . ",
    "url": "/design-tests.html#at-a-glance-header",
    
    "relUrl": "/design-tests.html#at-a-glance-header"
  },"5": {
    "doc": "Design showcase",
    "title": "Fancy buttons",
    "content": "Standard button for GH-pages Standard button for GH-pages . Bigger button (won’t work on GH) – START READING . ",
    "url": "/design-tests.html#fancy-buttons",
    
    "relUrl": "/design-tests.html#fancy-buttons"
  },"6": {
    "doc": "Design showcase",
    "title": "Passing arguments to custom templates",
    "content": "This is a fancy box. **It gives an example** of how to pass arugments to a _include template file. I was wondering if it works with multiple lines. And maybe even [hyperlinks](google.com)? ",
    "url": "/design-tests.html#passing-arguments-to-custom-templates",
    
    "relUrl": "/design-tests.html#passing-arguments-to-custom-templates"
  },"7": {
    "doc": "Design showcase",
    "title": "Colored boxes",
    "content": "$ sudo apt-get update . See other colored boxes below. Fancy label . red label . Special highlighted text . This is a custom note title . A paragraph with a custom title callout . This is a “new” type . red label . Another paragraph . The last paragraph . ",
    "url": "/design-tests.html#colored-boxes",
    
    "relUrl": "/design-tests.html#colored-boxes"
  },"8": {
    "doc": "Design showcase",
    "title": "Design showcase",
    "content": ". | LaTeX | Hyperlinks and (base)url | Tables | Expanding boxes / accordions | “At a glance” header | Fancy buttons | Passing arguments to custom templates | Colored boxes | . This is a gradient background part. This is a test! This is a beautiful main page. ",
    "url": "/design-tests.html",
    
    "relUrl": "/design-tests.html"
  },"9": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Audience",
    "content": "This book targets anyone who encounters quantum technologies in their professional lives, but who don’t not need a full physics background. This includes: . | Managers and strategic decision makers . | Consultants . | Policy makers . | CIO or CISO departments . | Investors . | . |   | This book is about… |   |   | This book does not contain… | . | ✓ | The impact that quantum technology has on business and society |   | ✗ | Essential math or physics | . | ✓ | Opportunities and threats |   | ✗ | Quantum programming | . | ✓ | Timelines |   | ✗ | Exhaustive information about every possible detail | . | ✓ | Links to other great resources |   |   |   | . ",
    "url": "/#audience",
    
    "relUrl": "/#audience"
  },"10": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Get the softcover",
    "content": "Prefer to read a printed edition? A physical edition will be released in Q4 2024. ",
    "url": "/#get-the-softcover",
    
    "relUrl": "/#get-the-softcover"
  },"11": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Introduction to Quantum Computing for Business",
    "content": ". This free book contains everything you should know about quantum computers, without going into tedious technical details. It answers questions such as: . | What are the applications of quantum computers and quantum networks? . | How long will it take before quantum computing becomes competetive? . | What are the consequences for cybersecurity? . | How can an organisation effectively prepare? . | What is the status of today’s hardware? . | . Start reading . ",
    "url": "/",
    
    "relUrl": "/"
  },"12": {
    "doc": "Further reading",
    "title": "Further reading",
    "content": "I want to learn the technical details . For (late) high school students: . (or those who followed high-school level mathematics): . | Quantum Quest [EN] (intensive 5-week online course about theory of quantum computing. Materials freely available for self-study).  . | Quantum Rules [NL] (1-day lab experiments dealing with quantum physics) . | Masterclass Quantum [NL] (3-day extracurricular course about the theory of quantum computing and particle physics) . | . Undergraduate (Bachelor's) university level: . | Quantum.Country [Website] – the “Duolingo of Quantum Computing”, a very well-written introduction for those with late high-school or early university level math background.  . | Quantum Computation and Quantum Information (Nielsen, Chuang) [Book] – the “bible of quantum computing”. Perhaps not the most up-to-date, but definitely the most well-known resource in our field. Sets the standards for jargon and notation.  . | . | Quantum Computer Science: An Introduction (Mermin) [Book] – a well-written introduction, with quite some focus on manipulating quantum circuits. | Quantum Computing Since Democritus (Aaronson) [Book] – Aaronson is an authority in the field. His book touched upon many topics such as the foundations of computer science, black holes and consciousness, making it a good read for those looking for something much more broad than just quantum computing. | . Graduate (Master's) level: . These assume no prior knowledge, but require a strong background in mathematics (i.e. Linear Algebra, Calculus, advanced inequality bounds and approximations, etc.). In exchange, they go into much more detail.  . | Lecture Notes for UvA course “Quantum Computing” by Ronald de Wolf, which is frequently updated and features some cutting-edge algorithms. Via the course website, you can find the link and password to view all the recorded lectures too.  | . | Lecture Notes for Caltech course “Quantum Computing” by John Preskil  | .   . For research projects, internships or theses: . | Internships offered at Centrum Wiskunde en Informatica . | Quantum Delta NL hosts a job board.  . | Students of University of Amsterdam can find quantum projects on Datanose.   . | . Scientific papers . Introductions mostly aimed at (non-quantum) scientists. All papers we mention are open-access and peer-reviewed, which guarantees a correct and balanced presentation (and makes them suitable for you to cite).  . | Quantum algorithms: an overview (Ashley Montanaro) . | The Potential Impact of Quantum Computers on Society (Ronald de Wolf) [video lecture] . | . Scientific opinions and discussions . | Scott Aaronson’s blog. Although written from a theoretical computer science perspective, this blog addresses a very broad range of quantum computing topics. Prof. Aaronson has a strong authority in the field, and his posts attract readership and comments from a broad range of prominent scientists.  | . Scientific news . None of these focus exclusively on Quantum Technology, but all offer high-quality news (and surely none would miss any important quantum breakthroughs).  . | Quanta Magazine . | Phys.org . | Quantum Universe [NL] . | . I want to learn to program a quantum computer . There are several programming packages for quantum computers, mostly maintained by major hardware providers. Nearly all of these offer great introductory tutorials that guide your through both the basics of Quantum Computing and the usage of the package.  . The ones we recommend below are all in Python:  . | Qiskit: Introduction to Quantum Computing (by IBM) . | Cirq tutorials (by Google)  . | QWorld Bronze offers various online training programs around the world, mostly focused on Qiskit and sometimes ProjectQ. | . I want to learn more about business opportunities . Business News Websites . | The Quantum Observer – newly formed news collection platform, allowing upvotes and comments to news articles.  | . | Quantum Computing Report – don’t be fooled by the basic look on the website. The content is written with a very critical eye and with very relevant contextual information.  | . | The Quantum Insider | . Overview articles about business opportunities (close to this blog) . Of course, we first recommend that you start reading our Professional’s Guide to Quantum Computing!  . Many others have written similar guides. Most of these come from consultants of hardware providers who have a financial interest in making others get started with quantum. In our opinion, the articles are sometimes too optimistic and predict that quantum applications will come much sooner than the typical expert would anticipate. On the other hand, they collect insightful details about financial matters.  . | Deloitte – A business leader’s guide to quantum technology | . | McKinsey – A game plan for quantum computing  | . | BCG – The Next Decade in Quantum Computing—and How to Play | . Workshops . | The Workshop General Awareness Quantum Computing is an introduction to business opportunties that requires no deep mathemtical background, but gives an essential overview to how quantum computers will impact businesses and governments.  | . Major conferences . | Q2B (organized by QCWare) . | IQT (Inside Quantum Technology) . | Quantum.Tech (organized by Alpha Events)s . | . ",
    "url": "/section_13.html",
    
    "relUrl": "/section_13.html"
  },"13": {
    "doc": "1 Preface, Why this book?",
    "title": "Preface: Why this book?",
    "content": "“Quantum computing will change everything,” the man in front of me said. Standing tall and confident, he took another sip of his drink before continuing, “It will be the biggest revolution since the invention of the transistor. Imagine a world where we can cure any disease with personalised medicine. A world where new energy sources will free us from our dependence on fossil fuels. Not to mention that…” . “Well—” I tried to interrupt, but the man passionately rattled on. “It will finally enable us to build general Artificial Intelligence that can take over our tedious everyday jobs, so 95% of our population no longer has to work!” . “You know that quantum computers are still quite some years away, right?”, I countered. He leaned in, eyes still gleaming with excitement. “That’s what most people think. But the reality is, we’re closer than ever. Quantum supremacy has already been achieved. Google did it in 2019; since then, progress has been exponential. Did you see the presentation by that guy from Goldman Sachs? Their investments already see higher returns than ever since their new Monte Carlo algorithm.” . The above conversation captures a feeling that many seasoned experts in quantum computing will have. And this is by no means exaggerated. Plentiful reputable sources report that ‘quantum’ is key in tackling climate change, revolutionising AI, and building unhackable networks. Experts who are actually building these quantum computers are much, much more reluctant. At an academic conference, you hear a completely different story. Scientists ridicule the absurd claims that some consultants and startups make. They will point out that the applications of quantum computers are still very much uncertain, and that we’re still searching for convincing uses cases. The quantum scene seems to have two completely separate worlds. A business world, that reaches out to anybody who will hear them about the unprecedented capabilities of quantum computers. And the academic world, the community of experts that quietly bring this quantum computer to reality, sharing their results in technical papers that require a PhD to understand. I was grasped by this paradoxical situation. What can we really expect from quantum technology? How powerful are quantum computers really, and how does this compare to other promising technologies? In what year will we have a large-scale quantum computer, and what will it look like? These are billion-dollar questions, but the answers will wildly vary, depending on who you ask. After searching for these answers a decade, I think I’ve finally found quite a unique position to answer most of these questions. I have the luxury of being a native in both the academic and the commercial world. I’ve seen all the arguments from both sides and can cut through dishonest and deceptive claims. I have the privilege of being surrounded by the world’s most renowned experts from both worlds, and I have heard their unsalted opinions and predictions that they may not readily throw out publicly. And most of all, after training many new colleagues and setting up professional learning programs, I developed a good intuition about what newcomers want to know about quantum technology, and how to explain it in an accessible way. I started writing these texts for two reasons. First, I aim to offer an alternative to the hyped and unbalanced articles that would otherwise populate the top entries in Google search results (or even the New York Times best-selling books1). Second, I see the need for a reliable source of information that others can reference when disagreeing about facts or debunking myths. I am already very grateful for the many colleagues who frequently refer to an early version of this book. That doesn’t mean that this book contains only confirmed facts – not at all! Writing about a computer of the future comes with mountains of uncertainty. In 2005, nobody could have predicted that a mere 5 years ahead, everyone would be playing games and consuming the internet on their smartphones. In 2015, nobody could have predicted the impact of Large Language Models like ChatGPT. And indeed, today, our best predictions of a future quantum revolution won’t be quite so accurate either. Even worse, experts wildly disagree in several cases. For example, the usefulness of quantum AI and optimization is continuously disputed, and the rate at which hardware will progress depends on many yet-to-discover breakthroughs. The best I could do is describe various perspectives on these matters and highlight the best arguments from either side. Without plentiful discussions and disagreements, I wouldn’t have been able to gather the facts and opinions in this book. And it shouldn’t stop there. I keep welcoming criticism, opinions, and feedback about these complex topics, aiming to refine these texts even more in future updates. Even though much is still uncertain, I think that a reliable indication of the prospects of quantum computing is more important than ever. Quantum startups are acquiring huge investments, allowing them to hire managers, software developers, salesmen, and marketers. Governments need informed policymakers, and journalists should cover quantum breakthroughs. Pretty much every organisation that deals with IT will want to keep a close eye on the impact that ‘quantum’ will have on them. This book is for precisely these people who don’t need to understand all the technical details but still need to talk, read, and write about quantum technologies. This is why I will focus on an accessible language everyone can understand. We don’t care so much about the underlying math or physics, but rather about the functionality of a quantum computer: the opportunities, the threats, and the concrete actions organisations can take. How should you read this book? I chose to split the content into three parts. The first part contains the essentials that we recommend everyone read. This is an incredibly efficient way to learn all the background that you need – you should be ready to understand other sources and have some depth in professional discussions or meetings. To get into more detail, parts two and three contain more information about the (software) applications and about the (hardware) devices, respectively. | I’m refering to Michio Kaku’s book “Quantum Supremacy”, but before you even consider reading it, you might like to see the book review by a professor in quantum computer science (https://scottaaronson.blog/?p=7321). &#8617; . | . ",
    "url": "/part1/chapter_1/#preface-why-this-book",
    
    "relUrl": "/part1/chapter_1/#preface-why-this-book"
  },"14": {
    "doc": "1 Preface, Why this book?",
    "title": "1 Preface, Why this book?",
    "content": " ",
    "url": "/part1/chapter_1/",
    
    "relUrl": "/part1/chapter_1/"
  },"15": {
    "doc": "2 An introduction to the quantum world",
    "title": "An introduction to the quantum world",
    "content": "At a glance You don’t need to understand quantum mechanics to understand the functionality of quantum computers. But if you insist, quantum mechanics describes the behaviour of the smallest particles. It leads to many counter-intuitive phenomena: computer memory can store multiple pieces of data at the same time, but upon measurement, nature selects just a single piece (and throws away all the others). If you want to drive a car, do you need to understand how its engine works? Of course you don’t! In a similar vein, you don’t need to know the details of quantum physics to read the rest of this book. So feel free to skip this chapter. Nevertheless, I know that most people want to have some conceptual intuition about what quantum mechanics really is. It is not natural to leave one of the most used word in this book as an abstract concept, and it might be hard for the human brain to proceed without at least seeing some examples. Here is my best attempt to explain quantum mechanics in accessible terms. Proceed with caution, as things will surely get confusing from here. What is quantum?   . Quantum physics or quantum mechanics is the theory that describes the tiniest particles, like electrons, atoms, and sometimes even small molecules. They are the laws of nature that dictate cause and effect at the scale of nanometers. You may contrast it to Newton’s classical physics that we teach at high schools, which works great for objects the size of a building or football but becomes inaccurate at much smaller scales. Quantum is, in a sense, a refinement of classical physics: the theories are effectively identical when applied to a coffee mug, but the more difficult quantum theory is needed to describe very small things.  . Some examples of systems where quantum could play a role are: . | Atoms, and the electrons that orbit around them. | Flows of electricity in microscopic (nano-scale) wires and chips . | Photons, the particles out of which ‘light’ is made. | . To proceed, we need some physics jargon. We like to use the word ‘state’, which is a complete description of all the physical properties of the world at one instance: the locations of all the different particle, their velocities, how much they rotate, etcetera. Usually, the entire universe is too big to study, so we often simplify our world to just a single isolated particle, or to a limited piece of computer memory. For example, for a bare particle, we might only imagine we’re only interested in its location, which we’ll call \\(x\\) (and we forget about any other structure in the world). For example, the world might look something the image below, which can be described by a very simple state: \\(x = 5\\) (the ruler is just virtual). In the spirit of computing, we might look at a ‘bit’ that stores information. You may think of it as a tiny magnet that can either point ‘up’ (0) or ‘down’ (1). The state of a piece of memory is easy to describe, simply by stating the bit values on by one. For example: 11010. Importantly, the state of the world can change over time. We will often care about the state of the world at a certain moment, for example at the beginning of computation, or at the end of it. Four surprising phenomena . The most iconic quantum phenomenon is superposition. Think about any property that we can (classically) measure, such as the position of a particle, or the value of a bit on a hard drive (0 or 1). In quantum mechanics, state of the world can be such that many different measurement outcomes are somewhat ‘true’ at the same time: a particle can be at multiple positions at once, or a bit could be 0 and 1 simultaneously. When we say ‘at the same time’ we mean that, to predict any cause and effect, we need to keep track of all these possibilities. To illustrate, I sometimes picture a quantum particle to split into many opaque copies of itself, spread out over space, where the degree of transparency determines how likely the particle is to be found there: the darker, the more likely. How can you possibly describe a state like that? For a single particle, the state is a long list, where for each possible position, we store a number called the amplitude, which is related to how likely the particle is to be found at that location. In other, the state describes precisely to what extent a particle is at position \\(x = 0\\), to what extent at position \\(x = 1\\), and so forth, for every possible location that the particle can be at. And indeed, this list could be infinitely long! Luckily, focusing on computers, we work with simpler objects. A quantum-bit needs just two amplitudes, which denote the extent to which the bit is ‘0’ or ‘1’ respectively. Because we will talk about quantum-mechanical bits a lot, we will give them a shorter name: qubits. If we have a bunch of qubits together, we’ll call it a quantum memory. To throw in some more examples of weird quantum states, an electron can move at a velocity of 10 m/s and 100 m/s at the same time (which obviously also leads to a superposition in its location). More relevant for us: a quantum memory might use binary notation to store numbers 5 and 11 simultaneously, or even 46 different Microsoft Excel spreadsheets at once. These amplitudes feel somewhat analogous to probabilities, which can similarly describe the likelihood that a particle can be found at some location. However, there is a fundamental difference. Probabilities in the classical world help us deal with information we don’t have: surely the particle is already at some location, but perhaps we just don’t know which location yet. Quantum mechanics is different. We might know every tiny detail about to location of a particle, and still we need to describe it as a superposition. Fundamentally, the location is not determined yet. Hence, there is literally no better way to describe the particle than by tracking this convoluted superposition. Amplitudes are also slightly more finicky to deal with than probabilities, because these numbers can become negative (and for math experts, they can even be complex numbers). The second weird phenomenon is how quantum measurements work. Why do we never observe an electron at two places at the same time? Why do I never find a car both moving and standing still? In quantum mechanics, as soon as we measure the location of a particle, it instantly jumps to just a single location at random – making its location fully determined. Similarly, when we measure a qubit, it jumps to either ‘0’ or ‘1’ with some probability. When we measure the data in a quantum memory, we may find any one of the 46 spreadsheets that were stored. This means that the world is intrinsically random (and hence, not deterministic!). But this doesn’t imply that we cannot understand it. We can calculate the probabilities of measurement outcomes with incredible precision as long as we know the state before the measurement. It is important to note that we cannot learn anything about the world without measuring – it is our only way to obtain data from the world, and any process that extracts data must involve a measurement. However, measurements are destructive in the sense that they change the state of the world. In fact, they delete all the rich data encoded in a superposition! If a particle was initially at position \\(x = 0\\),\\(\\ x = 3\\ \\)and \\(x = 10,\\) all simultaneously, then upon measurement, it jumps to one of these three options. In jargon, we call this instantaneous change a ‘collapse’. From then onwards, it is 100% at a fixed location: if at first we measure the particle to be at \\(x = 3\\), then any subsequence measurement will give the same result, until some other force moves it again. This means that, during a quantum computation, we should carefully choose when we to perform any measurements – we cannot just peek at the data at any moment we like, or we risk disturbing a superposition. Importantly, this also means that a single piece of quantum memory cannot store an immense number of spreadsheets at the same time – at least, you wouldn’t be able to retrieve each of them. To store 15 MB worth of classical data, we need 15 MB worth of ‘qubits’. Hence, quantum computers are not particularly useful for storing classical data. The fact that a measurement changes the state of the world poses a serious problem for the engineers who are building quantum computers. No matter what material we construct our qubits from, they will surely interact with other nearby particles, and some of these interactions could be destructive measurements. We call this effect decoherence, and we will later see that this forms one of the core challenges to large-scale quantum computation. At this point, quantum data doesn’t seem particularly useful. Why would we want to deal with superpositions if they lead to all this uncertainty? Intuitively, think about the advantage of a quantum computer in this way: what if the quantum computer can perform certain actions that a classical computer could never do, such as creating certain superposition states? Can these actions somehow help us reach the final outcome of the computation more efficiently? . In jargon, we’re talking about quantum operations. These are physical forces that change the state of a particle or a quantum memory. They can turn a classical-looking state into a quantum superposition or vice versa. They can act like logical operations, such as AND and OR, but also like new quantum ‘logic’ that has no classical counterpart. In this book, we mainly deal with operations that work on qubits. We will call such operations quantum gates or simply ‘gates’. A quantum gate takes one or more qubits as input, changes their internal state, and then outputs the same number of qubits (with their altered states). In other words, the number of physical objects remains unchanged, but the overall state changes. As an example, you may think of our prototypical magnet that was initially pointing ‘up’, but a quantum gate might flip this to ‘down’. There are many such gates possible, each having a different effect on their input, we like to give them names in capital letters, such as X, Z, H, and CNOT. Importantly, a quantum gate is deterministic, meaning that it’s input-output behaviour is always the same, as opposed to the quantum measurements we saw earlier. The canonical way to describe a quantum computer program is by defining a sequence of quantum gates, one after the other, each of which could act on a different set of qubits. At the end of the computation, we measure all qubits. Below, an example of such a sequence is given, using the standard Quantum Assembly (QASM) language. Together, these steps can be graphically displayed in a quantum circuit, as shown here on the right. Quantum circuits represent each qubit with a horizontal line and indicate time flowing from left to right. Whenever a box with a letter is displayed over a qubit line, then the corresponding gate should be applied. This isn’t quite unlike the way we read sheet music! . Note that when we run a circuit on an actual quantum computer, the final measurements lead to probabilistic outcomes. We get to see a bunch of ones and zeroes: one classical bit for each qubit. If the circuit was a ‘good’ quantum algorithm, then with high probability, these classical bits will tell us the answer we were looking for. But even then, we might need to redo the computation a few times and take (for example) the most common result as our final answer. If you are completely confused at this point, you are not alone. The whole business of quantum superposition and quantum operations is incredibly complex and is not something you could possibly master after reading a few book pages. Scientists who study the subject for many years are still frequently baffled by deceptive paradoxes and counter-intuitive phenomena. On the other hand, I hope that the functionality of quantum circuits makes a lot of sense: we define a list of instructions, and feed them into a machine that can execute them. We don’t have to know precisely what’s going on under the hood! . There is one remaining quantum phenomenon to cover – one that comes with a mysterious flair around itself. We’re talking about quantum entanglement. Imagine that we have two qubits, which we can transport independently from each other without disturbing the data they store. Together, the bits can represent the 00, 01, 10 or 11, or a superposition of these. According to quantum mechanics, we can create a very specific superposition, where the pair of qubits is simultaneously 00 and 11. Now, imagine that computer scientist Alice grabs one of the qubits, takes it on her rocket ship, and flies it all the way to dwarf planet Pluto. The other qubit remains on Earth, in the hands of physicist Bob. Upon arriving on Pluto, Alice measures her qubit and finds outcome ‘1’. A deep question is: what do we now know about Bob’s qubit? . Since the only possible measurement outcomes were 00 and 11, the other qubit can only be measured as ‘1’ from now onwards. It essentially collapses to be 100% in the state ‘1’. But how could the earth-based qubit possibly know that a measurement occurred on Pluto? According to Einstein’s theory of relativity, information cannot travel faster than the speed of light, which translates into a few hours between Earth and Pluto. Nevertheless, measuring the qubits two faraway locations will always give a consistent result, even when done within the hour. This paradox shows once again how confusing quantum mechanics can be. However, the story above is perfectly consistent with both quantum mechanics and relativity. The core principle is that no information can be sent faster than light between Alice and Bob. For example, can you see why Bob has no way of detecting when Alice performs her measurement, just by looking at his entangled qubit? In the most common interpretation of quantum mechanics, the Earth qubit does indeed change its state instantaneously when Alice measures, although there is no way to exploit this effect. There’s a fascinating further discussion about the philosophy behind entanglement, but we’ll leave that to other sources. What matters to us is that distant qubits can still share specific properties that would be impossible to mimic classically, leading to new functionalities we can exploit. We will discover what these functionalities are in the chapter on quantum networks. So there you have it: four surprising phenomena you may hear frequently in quantum technology conversations. To summarise: . | Superposition: the phenomenon where a qubit is both 0 and 1 at the same time. | Quantum measurement: measuring a quantum memory destroys superposition. The result we obtain is probabilistic. | Quantum operations/gates: deterministic changes to the state of qubits, which generalise classical logic gates like OR, AND, NOT. A list of several quantum gates forms a quantum circuit. | Entanglement: Qubits separated over a long distance can still share unique properties. | . What does a quantum computer look like? . Most large-scale computing today happens in data centres, where we don’t care much about the specifics of the devices that do our calculations. We also expect that future quantum computers will mostly be tucked away in the ‘cloud’, making their appearance and inner workings largely irrelevant to most users. However, for this optional chapter, we can provide an idea of what a future quantum computer could look like. There are many different ways to build a quantum computer, each working in vastly different ways. Here, we choose the example of so-called superconducting qubits. We will see that only a tiny part of the computer is actually ‘quantum’, whereas most of the machine consists of bulky classical machinery that’s required to keep the computer working. The real quantum magic happens on a chip, not unlike the computer chips used in your laptop or phone. The qubits are formed by tiny electronic circuits where the flow of electrical current is restricted to just one out of two states: the ‘bit’ states 0 and 1. Since this is a quantum system, the current can also be in a superposition – picture all the electrons in the wire participating both in flow ‘0’ and flow ‘1’ simultaneously! This only works when the chip is cooled down to unimaginably low temperatures, down to around 10 milliKelvin – a hundredth of a degree above absolute zero. At these temperatures, the electronic circuits become superconducting, such that an initial current can flow indefinitely. This is important because any damping would cause unwanted disturbance to the qubit state. That’s why the quantum chip is placed in a massive dilution refrigerator, a cylinder of about half a meter in diameter and over a meter tall, which specializes in keeping the quantum chip cool. In the future, larger quantum computers may need even bigger fridges or have several of these close together. Deeper parts of the fridge have different temperatures, allowing us to cool in stages. An example could be to cool a first environment to 35 Kelvin (-283 °Celsius or -396.7 °Fahrenheit), followed by subsequent stages to ~3K, 900mK, 100mK, until the final stage of ~10mK is reached. One often hangs the fridge on the ceiling so that the higher temperatures are on top and the ultracold quantum chip is placed at the very bottom. The internals are shaped accordingly: several layers of gold disks are suspended below one another, one disk for each temperature zone. A large number of wires run between the disks, transporting signals between the ceiling and the lowermost areas. The whole structure forms the iconic metal chandelier that you often see in images, although it would all be covered by a boring metal case when the fridge is in operation. To make the qubits do something useful, like executing a quantum gate or performing a measurement, we need to send signals into the chip. Just like in classical computers, these signals are technically voltage differences between various wires. Some voltage remain constant over time, others oscillate at microwave frequencies. The wiring itself becomes increasingly challenging for larger quantum computers, for two reasons. Firstly, we currently need around 2-4 wires to control a single qubit, which is problematic when we scale to millions of qubits – it’s impossible to connect that many wires to a tiny chip. We’ll need to find ‘multiplexing’ solutions, where a single wire can serve multiple qubits at once. Secondly, many wires connect the ultracold chip to other hardware that sits at room temperature, forming a channel for heat and noise to enter. The dilution fridge circumvents this by incrementally cooling and damping the signals as they travel through the different layers of the fridge, but it can only handle so many cables. Besides the large chandelier, an array of specialized control electronics is needed to produce the necessary electronic pulses and to carefully read out the tiny signals that qubits produce when we measure them. These devices sit in one or multiple electronics racks, each half a meter wide and nearly two meters tall, similar to the ones you’ll find in a typical data centre. Ironically, the actual quantum software can be written on a simple laptop, from where the instructions are passed to the control electronics to run a quantum circuit. The whole situation is reminiscent of computers in the 1940s and 1950s, which similarly occupied a large room and required several engineers for all kinds of labourious manual maintenance tasks. On top of this, the dilution fridges are particularly noise – to the extent that those who operate them ideally do this from a different room – and are fairly power-hungry. The whole quantum computer consumes around 25 kW, comparable to driving an electric car. To summarize, a quantum computer based on superconducting qubits consists mostly of bulky classical stuff: a large cylindrical fridge and a rack full of classical electronic devices, all of which work together to keep the microscopic qubits in the coldest part of the fridge working. Further reading If you’d like to know more about the physics and math behind qubits, we recommend the following sources: . | Quantum Country – a great online textbook about Quantum Computing by Andy Matuschak and Michael Nielsen. | QuTech Academy’s School of Quantum – A broad range of topics explained using short videos.  . | A closer look at IBM’s superconducting quantum computer on Youtube. | . ",
    "url": "/part1/chapter_2/#an-introduction-to-the-quantum-world",
    
    "relUrl": "/part1/chapter_2/#an-introduction-to-the-quantum-world"
  },"16": {
    "doc": "2 An introduction to the quantum world",
    "title": "2 An introduction to the quantum world",
    "content": " ",
    "url": "/part1/chapter_2/",
    
    "relUrl": "/part1/chapter_2/"
  },"17": {
    "doc": "3 The background, why are we so enthusiastic about Quantum Technology?",
    "title": "The background: why are we so enthusiastic about Quantum Technology?",
    "content": "At a glance Quantum technology is an umbrella term for devices that exploit quantum phenomena like superposition and entanglement. The most notable innovations are expected in computers, networks, sensors, and simulators. Quantum computers will be much slower than conventional computers. Their advantage comes from the ability to run quantum algorithms, which solve specific problems in much fewer steps. What is quantum technology? . Quantum physics has already proven itself as an invaluable basis for technologies such as LED lighting, MRI scanners and solar cells. And it’s still relevant to push the limits of innovation, with nano-cars that consist of just a few atoms, or ever-smaller transistors on computer chips on the horizon. Just ahead of us is a new paradigm, which we’ll call quantum technology. The distinguishing factor is that it goes further than merely building stuff from small particles. Quantum technology is about devices that perform certain processes in a fundamentally different way. That is, the data (or operations) we work with can have special properties unique to quantum physics, such as superposition and entanglement. In our jargon, we will refer to ‘classical’ technology for devices that don’t carefully exploit the possibilities of quantum physics – they are based on ‘classical’ physics that we’re used to from high school. Your laptop and phone are examples of classical computers, and they’re connected to the classical internet. The internal transistors and electrical circuits might be so tiny that quantum physics is relevant there, but the fundamental point is that the information that they process is purely classical. Whereas classical computers work with ‘bits’, quantum technology will store information in something called ‘quantum bits’, or shortly ‘qubits’. Generally, under the nomer of quantum technology, we distinguish four categories: . Quantum Computers . Quantum computers are devices that use quantum physics to perform automatic calculations to solve a problem. Computing is considered the most impactful application for most organisations, and therefore, it’s the main focus of this book.  . Quantum Networks . Quantum networks are connections between quantum devices over which qubits (or similar forms of quantum data) can be transmitted. The most relevant use case is to strengthen cryptography used by classical computers, but there are many more applications. Quantum Sensors . Quantum sensors are devices that exploit the effects of quantum physics to accurately measure certain quantities, such as a magnetic field or the strength of the earth’s gravity. Quantum clocks also fall into this category.  . Quantum Simulators . Quantum simulators are devices similar to quantum computers, except that they specialise in solving a limited set of problems. Typically, they are built to reproduce the behaviour of atoms and electrons in a molecule or a piece of material, allowing us to measure properties like energies and reaction rates. In this book, we mainly focus on computers and networks: simply because these seem to have the biggest impact on typical (business) users. ### . The importance of high-performance computing . The abundance of cheap computational power has given humanity incredible wealth. We automated the most tedious tasks to free up time for leisure and to solve other urgent problems. It allowed us to scale factories, supply chains and logistics to unprecedented sizes, allowing us to transport resources around the globe at minimal costs. Computer chips, aeroplane wings, heart monitors, and LCD screens have improved with every generation. Today, our computers are already incredibly fast. In fact, for many applications, there is little economic gain in making these computers even faster. Decades-old machines can successfully oversee factory operations, and writing a Word document or scheduling a meeting with 8 busy colleagues is not limited by faster computers in any way. However, this book specifically is about the applications where we are still hungry for more computational power. If only we could feed more data into weather models, our forecasts would become more accurate. If staff rostering would take less time, we could take better take last-minute changes into account. Accurate predictions of drug reactivity in a human body could save on costly medical trials and reduce the time to market. Machine learning models like ChatGPT are still demanding more training hours to produce more sophisticated results. It should be clear that we’re not talking about computations that happen on your laptop. We’re thinking of problems where somehow there’s value in investing in the fastest possible computers on earth: high-performance computing (HPC), colloquially called supercomputers. Merely looking at the market, there seems to be incredible value computing stuff: companies and academics spend tens of billions of dollars on them1, and Nvidia recently became the most valuable company on earth by leading the market of special purpose Graphical Processing Units (GPU’s). ### . Why can quantum computers have an advantage?  . Quantum computers are devices that perform automatic computations at our command, very much like their classical counterparts. The ‘quantum’ part stems from the fact that it can actually exploit the laws of quantum mechanics. This should be contrasted to our conventional ‘classical’ computers, which technically can also be completely described by quantum mechanics, but they just happen to also work fine according to classical physics. By no means are they supposed to ever deal with quantum phenomena like superposition or entanglement. Contrarily, a proper quantum computer does exploit these purely quantum mechanical effects.  . A naive view of quantum computers is that they’re simply faster than their conventional cousins. Or perhaps one may naively point at Moore’s Law: with transistors reaching atomic scales, we run into quantum effects, so quantum physics may help us make better chips. However, none of these are our core motivations for looking at quantum computers.  . Firstly, quantum computers are much, much slower than conventional computers, if one focuses on their ‘time to perform a basic step’. A modern CPU can handle 64-bit numbers (meaning that arbitrary numbers up to 18,446,744,073,709,551,615 (!) can be processed). It can perform basic arithmetic like addition, multiplication, and so forth on these numbers, essentially in one single ‘step’. The speed of a modern CPU is incredible: a modern Intel or AMD processor works at a rate of several GHz, that is, several billions of steps per second. There’s no way a human can possibly keep up with such speeds when it comes to plain calculations!  . Now, quantum computers are supposed to be even faster, right? Well, it’s not hard to find backing for that claim:  . However, you may be disappointed to hear that quantum computers, at this moment, cannot even add or multiply numbers of more than 3 or 4 bits. And even if they could, their rate of operation would by no means reach several GHz, but more likely several MHz (a few million operations per second), at best. In other words, they’re more than a thousand times slower. To make things worse, the information in quantum computers is extremely fragile and needs to be constantly checked and corrected, using so-called error correction. This is a form of overhead that could make quantum computers another several orders of magnitude slower. Even in the far future, when quantum computers are more mature and more reliable, we still expect them to be much slower than the classical chips at that time.  . How does this rhyme with the news about ever-faster quantum computers? And why are we still interested in these slow machines? As we claimed before, we hope to do certain computations in a fundamentally different way. Let’s look at a beautiful analogy that Andy Matuschak and Michael Nielsen bring up in their online course Quantum.country.  . Imagine that you’d like to travel from Morocco to Spain. If your technology does not allow you to cross the Strait of Gibraltar (say, you have access to a car but not a boat), then you’d need to take an incredible detour: all the way through North Africa, past the Arabian Peninsula, and through Europe, before you can reach your destination. This represents the steps taken by a classical computer. In the same analogy, a quantum computer grants you the ability to traverse both land and sea (much like a hovercraft). The fundamentally new possibilities that quantum offers allow us to do computations in fewer steps: even with a slower vehicle (computer), one may arrive at the destination sooner. In fact, this advantage often grows as problems become larger and more complicated. It is still an active area of research to completely map the landscape over which quantum computers can travel and, hence, to determine which problems can be sped up and which cannot.  . I like this analogy because it indicates that some problems can profit greatly from a quantum computer, whereas many won’t: you would not want to travel by hovercraft from Amsterdam to Berlin. For this reason, we don’t expect that classical computers will be ‘replaced’ any time soon. Instead, different types of processors (CPUs, GPUs, quantum computers) are expected to co-exist. GPU and Quantum Processing Units (QPUs) are special-purpose devices, meant to only do tricks that they happen to be good at. In the analogy with the Strait of Gibraltar, the precise route that you travel denotes the chosen algorithm. An algorithm is a step-by-step list of instructions that describes how a computational problem should be solved. The ‘steps’ here should be sufficiently simple so that it is completely unambiguously how to do them. They could be operations such as adding, multiplying, or comparing two numbers. Needless the say, the fewer steps the algorithm requires, the better. By exploiting quantum mechanics, a quantum computer introduces new basic steps (like crossing the sea) that are impossible to perform on a classical computer. For example, the previous chapter introduced quantum logic gates that generalise operations like AND and OR. Using these building blocks, we can formulate quantum algorithms that might take fewer steps than the best classical algorithm ever could! . A recurring theme in this book is the search for quantum algorithms with a significant advantage over their classical counterparts. This turns out to be more challenging than it seems at first sight: the algorithm must have a significant advantage to compensate for the ‘slowness’ of the quantum computer, and the number of algorithms that accomplish this is still quite small. We might go as far as to say that, even if we had a large-scale quantum computer today, its value would be limited. For this reason, the ongoing development of novel algorithms is incredibly important. From algorithm to software . In the end, simply finding the ‘recipe’ itself is not enough: the algorithm has to be turned into software, a piece of language that tells a computer explicitly how to execute the step-by-step instructions. The difference between ‘algorithms’ and ‘software’ is subtle. An algorithm is a purely mathematical description that describes precisely how numbers should be manipulated. It could tell which two numbers must be multiplied, what function must be evaluated, or how an image must be transformed. However, different computers can use different types of processors and memory, and an algorithm does not describe how these operations are done on a specific computer. This is where software comes into play: it describes precisely what computer instructions must be called, where each number is stored in memory, and how an image is represented in binary. As an analogy, you may think of the algorithm as a recipe to bake the perfect chocolate cookie. The algorithm should unambiguously describe what should happen to the ingredients: in what order they should be mixed, how long they should be heated, etcetera. However, to build a factory that produces these cookies, you need to be even more specific: Where is the sugar stored? Out of what pipe does the dough flow? How are cookies laid next to each other in the oven? . Fundamentally, core scientific breakthroughs come from finding new algorithms. Once a new algorithm is found, it can be re-used many different times on any capable machine (assuming a good software developer will turn it into appropriate code!). In this book, we care less about quantum software and more about quantum algorithms. Firstly, the algorithms tell us precisely the functionality that quantum computers can offer. Moreover, we don’t yet know how a mature quantum computer will be programmed or how quantum hardware and software will change in the following years. On the other hand, newfound algorithms can be cherished forever. Now that we have come to appreciate algorithms, it is natural to ask which quantum algorithms we know of. What problems do quantum computers solve well? And how do these algorithms compare to their classical equivalents? This will be the topic of the next chapter. Further reading . | The Map of Quantum Computing (Youtube)  – A 30-minute video that forms a great supplement to this book.  . | Chris Ferrie’s book ‘What You Shouldn’t Know About Quantum Computers’ debunks several myths about quantum computers, presented in a very accessible way. | Are you looking for a much more extensive source that covers pretty much everything there is to know about quantum computers? French consultant Olivier Ezratti maintains a 1300+ page book “Understanding Quantum Technologies”. | . | See e.g. https://www.marketsandmarkets.com/Market-Reports/Quantum-High-Performance-Computing-Market-631.html and https://www.mordorintelligence.com/industry-reports/cloud-high-performance-computing-hpc-market &#8617; . | . ",
    "url": "/part1/chapter_3/#the-background-why-are-we-so-enthusiastic-about-quantum-technology",
    
    "relUrl": "/part1/chapter_3/#the-background-why-are-we-so-enthusiastic-about-quantum-technology"
  },"18": {
    "doc": "3 The background, why are we so enthusiastic about Quantum Technology?",
    "title": "3 The background, why are we so enthusiastic about Quantum Technology?",
    "content": " ",
    "url": "/part1/chapter_3/",
    
    "relUrl": "/part1/chapter_3/"
  },"19": {
    "doc": "4 The applications, What problems will we solve with quantum computers?",
    "title": "The applications: What problems will we solve with quantum computers?",
    "content": "At a glance . | Four key applications in IT are: simulation of chemistry and materials, cracking cryptography, using quantum networks to distribute cryptographic keys, and solving large-scale optimization and AI problems.  . | Not every quantum speedup is useful: a much faster classical computer is often a better choice. In optimisation and AI, we have not found a truly valuable ‘killer application’ yet. | . In the previous chapter, we saw that quantum computers are extremely slow computers, but they happen to solve some problems more efficiently, that is, in fewer steps. The most important question in this field is: what advantage do quantum computers have on which problems? . The Quantum Algorithm Zoo1 lists pretty much all known quantum algorithms. It has grown into an impressive list that cites over 400 papers. Unfortunately, upon closer inspection, it’s hard to extract precisely the useful business applications from it, for various reasons. Several algorithms solve highly artificial problems for which no real business use-cases are known. Others may make unrealistic assumptions or may only offer a speedup in the formal number of computational steps (but not in actual wall clock time). Nevertheless, it’s definitely recommended to scroll through. For this book, we take a different approach. We focus specifically on algorithms with plausible business applications. To assess their advantage, we break our main question down in two parts: . | What are projected applications with a quantum speedup? . | How large is the advantage of known speedups? . | . What are projected applications with a quantum speedup? . We foresee four major use cases where quantum computing can make a real impact on society. We briefly discuss each of them here and link to a later chapter that discusses each application in more depth.  . 1. Simulation of other quantum systems: molecules, materials, and chemical processes . Most materials can be accurately simulated on classical computers. However, in some specific situations, the locations of atoms and electrons become notoriously hard to describe, sometimes requiring quantum mechanics to make useful predictions. Such problems are the prototypical examples of where a quantum computer can offer a great advantage. Realistic applications could be in designing new chemical processes (leading to cheaper and energy-efficient factories), estimating the effects of new medicine, or working towards materials with desirable properties (like superconductors or semiconductors). Of course, scientists will also be excited to simulate the physics that occur in exotic circumstances, like at the Large Hadron Collider or in black holes. Simulation is, however, not a silver bullet, and quantum computers will not be spitting out recipes for new pharmaceuticals by themselves. Breakthroughs in chemistry and material science will still require a mix of theory, lab testing, computation, and most of all, the hard work of smart scientists and engineers. From this perspective, quantum computers have the potential to become a valued new tool for R&amp;D departments. Read more: How can quantum computers help us produce agricultural fertiliser more efficiently? . See also: . | Startup PhaseCraft studies the famous Fermi-Hubbard model using a quantum computer . | Startup Zapata reduces the runtime and error rate of famous chemistry algorithm . | IBM and Daimler research next-gen batteries . | Roche started a project to find medicines for Alzheimer’s . | An overview of various simulation software packages for quantum computers . | . 2. Cracking a certain type of cryptography . The security of today’s internet communication relies heavily on a cryptographic protocol invented by Rivest, Shamit and Adleman (RSA) in the late 70s. The protocol helps distribute secret encryption keys (so that nobody else can read messages in transit) and guarantees the origin of files and webpages (so that you know that the latest Windows update actually came from Microsoft, and not from some cybercriminal). RSA works thanks to an ingenious mathematical trick: honest users can set up their encryption using relatively few computational steps, whereas ‘spying’ on others would require one to solve an extremely hard problem. For the RSA cryptosystem, that problem is prime factorisation, where the goal is to decompose a very large number (for illustration purposes, let’s think of 15) into its prime factors (here: 3 and 5). As far as we know, for sufficiently large numbers, this task can take a classical computer such a long time that nobody would ever succeed in breaking a relevant code – think of thousands of years. RSA was deemed adequately secure, at least, until computer scientist Peter Shor discovered that quantum computers are quite good at factoring. The quantum algorithm by Shor can crack RSA (and also its cousin called elliptic curve cryptography) in a relatively efficient way using a quantum computer. To be more concrete, according to a recent paper, a plausible quantum computer could factor the required 2048-bit number in roughly 8 hours (and using approximately 20 million imperfect qubits). The authors had to make several assumptions about what a future quantum computer would look like, and did so in a very prudent way: picking realistic properties of prospective hardware, and using the most up-to-date knowledge of error correction and compiling. Note that future breakthroughs will likely further reduce the stated time and qubit requirements. Luckily, not all cryptography is broken as easily by a quantum computer. RSA falls in the category of public key cryptography, which delivers a certain range of functionalities. A different class of protocols is symmetric key cryptography, which is reasonably safe against quantum computers but doesn’t provide the same rich functionality as public key crypto. The most sensible approach is replacing RSA with so-called post-quantum cryptography (PQC): public-key cryptosystems resilient to attackers with a large-scale quantum computer. Interestingly, PQC does not require honest users (that’s you) to have a quantum computer: it will work perfectly fine on today’s PCs, laptops and servers. Read more: How will quantum computers impact cybersecurity?  . See also: . | MinutePhysics has a fantastic (but technical!) explainer of Shor’s algorithm.  . | NIST’s competition to standardize new public-key algorithms    . | Nature feature article: The race to save the Internet from quantum hackers . | .   . During the following decade, every large organisation will have to worry about updating to post-quantum cryptography – a complex migration that comes in addition to the many existing cybersecurity threats. The American National Institute of Standards and Technology (NIST) runs a competition to select a new standard that is adequate for most applications. Nevertheless, many organisations run a vast amount of legacy software that is hard to update, so completing this update in the upcoming ~8 years poses a complex operational challenge. For this reason, organisations are encouraged to start this process as early as possible.  . A new type of cryptography poses additional risks: it has not yet been tested as thoroughly as the nearly 50-year-old RSA standard. Ideally, new implementations will be hybrid, meaning that they combine the security of a conventional and a post-quantum algorithm. On top of that, organisations are encouraged to adopt cryptographic agility, meaning that cryptosystems can be easily changed or updated if the need arises.  . Read more: What steps should your organisation take? . Other great sources are: . | The PQC Migration Handbook, written by the Dutch secret service AIVD and research organizations CWI and TNO. | Cloudflare tracks the adoption of post-quantum cryptography and explains many technical details extremely well.  . | US National Institute for Standards and Technology (NIST): Getting Ready for Post-Quantum Cryptography: Explore Challenges Associated with Adoption and Use of Post-Quantum Cryptographic Algorithms . | UK National Cyber Security Center: Preparing for Quantum-Safe Cryptography . | BSI (German secret service): Quantum-safe cryptography – fundamentals, current developments and recommendations . | NCSC (Dutch National Cyber Security Center): Factsheet Postquantumcryptografie [NL] . | . 3. Quantum Key Distribution to strengthen cryptography . Out of all the applications for quantum networks, Quantum Key Distribution (QKD) is the one to watch. It allows two parties to generate secure cryptographic keys together, which can then be used for everyday needs like encryption and authentication. It requires a quantum network connection that transports photons in fragile quantum states. Such connections can currently reach a few hundred kilometres, and there is a clear roadmap to expand to a much wider internet. The most likely usage will be as an “add-on” for high-security purposes (such as military communication or data exchange between data centres), in addition to standard post-quantum cryptography.  . Unfortunately, we often see media articles suggesting that QKD is a solution to the threat of Shor’s algorithm and that it would form an ‘unbreakable internet’. Both claims are highly inaccurate. Firstly, QKD does not offer the wide range of functionality that public-key cryptography offers, so it is not a complete replacement for the cryptosystems broken by Shor. Secondly, there will almost certainly be ways to hack a QKD system (just like with any other security system). Then why bother with QKD? The advantage of QKD is based on one main selling point: contrary to most other forms of cryptography, it does not rely on mathematical assumptions. This can be an essential factor when someone is highly paranoid about their cryptography, or when data has to remain confidential for an extremely long period of time.  . At this time, pretty much every national security agency discourages the use of QKD simply because the available products are far from mature (and because PQC should be prioritised). It is unclear how successful QKD could be in the future—we will discuss this in depth in a next chapter. Read more: What are the use cases of quantum networks? . See also:  . | A short video explainer about how QKD works. | The NCSC states that it “does not endorse the use of QKD for any government or military applications” . | The French ANSSI, German BSI, Dutch NLNCSA and Swedish SNCSA published a critical position paper on QKD in 2024.  . | . Quote: . Major security agencies do not support the use of QKD to secure communications and agree that post-quantum cryptography should be regarded as the best way to mitigate the quantum threat. THE PQC MIGRATION HANDBOOK . End quote . We firmly warn that other security products with the word “quantum” in the name are no guarantee for protection against Shor’s algorithm. In particular, “quantum random number generators” (QRNGs) are sometimes promoted as a saviour against the quantum threat, which is nonsense. These devices serve a completely different purpose: they compete with existing hardware to generate unpredictable secret keys, which find a use (for example) in hardware security modules in data centres.  . See also:  . | Samsung builds QRNGs into certain phones on the South Korean market.  | . 4. Optimisation and machine-learning . This is the part where most enterprises get excited: Can we combine the success of AI and machine learning with the radically new capabilities of quantum computers? Classical optimisation and AI techniques have had an incredible impact in many areas, from optimising train schedules to detecting fraud, from training chatbots to accurately predicting the weather.  . Under the hood, all such applications are based on concrete mathematical problems such as (discrete) optimisation, differential equations, classification, and optimal planning. For conciseness, we collectively refer to these problems (including machine learning tasks) as ‘optimisation’. The classical field of optimisation is of great importance and takes up a significant fraction of the world’s computational resources! . Contrary to the many classical successes, the impact of quantum optimisation or machine learning is not yet clear. To better understand the available algorithms, we will consider three different categories.  . Rigorous but slow algorithms . Many quantum optimisation algorithms have a well-proven quantum speedup: there is no dispute that these require fewer computational steps than any classical algorithm. For instance, a famous quantum algorithm invented by Lov Grover (with extensions by Durr and Hoyer) finds the maximum of a function in fewer steps than conventional brute-force search. Similarly, quantum speedups were found for popular computational methods such as backtracking, gradient descent, semidefinite programming, lasso, and interior point methods for solving differential equations.  . The main question is whether this also means that the quantum computer requires less time! All of the above optimisation algorithms offer a so-called polynomial speedup (in the case of Grover, this is sometimes further specified to be a quadratic speedup). As we will soon see, it is not entirely clear if these speedups are enough to compensate for the slowness of a realistic quantum computer – at least in the foreseeable future.  . | ***Heuristic algorithms ** On the other hand, some algorithms claim much larger speedups, but there is no undisputed evidence to back this up. Often, these algorithms are tested on small datasets using the limited quantum computers available today – which are still so tiny that not much can be concluded about larger-scale problems. Nonetheless, these ‘high risk, high reward’ approaches typically make the bold claims that receive media attention. The most noteworthy variants are: . | Variational quantum circuits (VQC) are relatively short quantum programs that a classical computer can incrementally change. In jargon, these are quantum circuits that rely on a set of free parameters. The classical computer will run these programs many times, trying different parameters until the quantum program behaves as desired (for example, it might output very good train schedules or accurately describe a complex molecule). The philosophy is that we squeeze as much as possible out of small quantum computers with short-lived qubits: the (fast) classical computer takes care of most of the computation, whereas the quantum computer runs just long enough to sprinkle some quantum magic into the solution. Although its usefulness is disputed, this algorithm is highly flexible, leading to quantum variants of classifiers, neural networks, and support vector machines. Variants of this algorithm may be found under different names, such as Quantum Approximate optimisation Algorithm (QAOA), Variational Quantum Eigensolver (VQE), and quantum neural networks.  . | Quantum annealing solves a particular class of optimisation problems. The problem is encoded into a physical system (in jargon: a Hamiltonian) so that at very low temperatures, the physical system somehow describes a solution to the problem. For example, when finding the optimal locations to place mobile phone masts, a qubit in the state “1” might indicate a good site. A quantum annealing algorithm is a smart way to ‘make’ such a low-temperature system (often by starting in a setting where it’s trivial to ‘cool’ and then slowly introducing the complex forces corresponding to the target system). Annealing itself is a mature classical algorithm. The advantage of a ‘quantum’ approach is not immediately apparent, although there are claims that hard-to-find solutions are more easily reached thanks to ‘quantum fluctuations’ or ‘tunnelling’. Quantum annealing was popularised by the Canadian company D-Wave, which builds dedicated hardware with up to 5000 qubits and offers a cloud service that handles relatively large optimisation problems.  . | .   . Fast solutions in search of a suitable problem . Lastly, there exist algorithms with large speedups, for which we are still looking for use-cases with any scientific or economic relevance. The most notable example is the quantum algorithm that solves systems of linear equations2 with an exponential advantage. This problem is ubiquitous in engineering and optimization, but unfortunately there are so many caveats that no convincing practical use cases have been found3. Recently, much attention has gone to the algorithm for topological data analysis (a method to assess certain global features of a dataset), which promises an exponential advantage under certain assumptions. Again, scientists are still searching for a convincing application. Similarly, a quantum version of a classical machine learning algorithm called Support Vector Machines was found to have an exponential advantage over classical methods4. Unfortunately, this only works with a very specific dataset based on the factoring problem that Shor’s algorithm is well known for, hence we see no practical uses. A fourth class: quantum-inspired algorithms . Some impressive speedups that were recently found have been ‘dequantized’: these algorithms were found to work on classical computers too! There’s a beautiful story behind this process, where Ewin Tang, a Master’s student at the time, made one of the largest algorithmic breakthroughs of the decade. A great report can be found on Medium5.  . Unfortunately, there does not yet exist an optimisation algorithm with obvious economic value: all of them come with serious caveats. This perspective is perhaps a bit disappointing, especially in a context where quantum computing is often presented as a disruptive innovation. Our main takeaway is that quantum optimisation (especially quantum machine learning!) is rather over-hyped.  . Of course, there are still good hopes that we will find new algorithms and applications. To truly understand this field, we should examine the prospects of finding a new ‘killer algorithm’. The next section becomes slightly more technical and helps us quantify the amount of ‘quantum advantage’ that different algorithms have.  .   . Further reading:  . | Volkswagen and ExxonMobil used annealing to optimise routes for buses and transport ships. | Professor Scott Aaronson warns us to ‘Read the fine print’ of optimisation algorithms. [Appeared in Nature Physics, with paywall]  . | Professor Sanker Das Sarma warns of hype within the field of quantum optimisation and machine learning. | . How large is the advantage of known speedups?  . When looking at the applications of quantum computers, one should always keep in mind: Are these actual improvements over our current state-of-the-art? Anyone can claim that their algorithm can solve a problem, but what we really care about is whether it solves it faster. Classical computers are already extremely fast, so quantum algorithms should offer a substantial speedup before they become competitive.  . What does an “algorithmic speedup” mean? . We can assess algorithms by their so-called “asymptotic complexity”: As a problem becomes ‘bigger’, how much longer does a computation take? The main figure of merit is how this scales towards very large sizes.  . For every instance of a problem, we can define a ‘size’ that influences the difficulty. For example, computing 54 x 12 is much easier than 231.423 x 971.321, even though they’re technically the same problem, and we’d use the very same multiplication algorithm. Similarly, creating a work schedule for a team of 5 is simpler than dealing with 10.000 employees. We typically use the letter ‘n’ to denote the problem size. You can see n as the number of digits in a multiplication (like 2 or 6 above) or the number of employees involved in a schedule.  . For some very hard problems, the time to solution takes the form of an exponential: T ~ en, where T is the time taken. Exponential scaling is typically a bad thing, as the function en becomes incredibly large even for moderate values of n. The problem of factoring (on a classical computer) scales somewhat similar to T ~ en.  . There are also problems for which the scaling looks like a polynomial, like T ~ n3 or T ~ n2. Polynomials grow much slower than exponentials, making it easier to solve large problems in a reasonable amount of time. The problem of factoring on a quantum computer takes scales roughly as T ~ n3 (thanks to Shor’s algorithm*). Because a quantum computer brought the exponential down to a polynomial, we call this an ‘exponential speedup’. Such speedups are a computer scientist’s dream because they have an incredible impact on practical runtimes.  . Often, we deal with ‘merely’ a polynomial speedup, which happens when we obtain a smaller polynomial (for example, going from T ~ n2 towards T ~ n), or perhaps even a ‘smaller’ exponential function (like T ~ en towards T ~ en/2. Reducing the exponent by a factor of two (like n2 -&gt; n) is also sometimes called a quadratic speedup (which is precisely what Grover’s algorithm gives us). Further reading: . | At a more coarse level, we can define different “complexity classes”.  | .    * You may find even sources stating smaller polynomials, like n2 log(n). These are theoretically possible but rely on asymptotic optimizations that are unlikely to be used in practice. Here is a rough overview of quantum speedups as we understand them today, categorised by their type of speedup: .   . 🟢   The “exponential” box is the most interesting one, featuring applications where quantum computers seem to have a groundbreaking advantage over classical computers. It contains Shor’s algorithm for factoring, explaining the incredible advantage that quantum computers have in codebreaking. We also believe it contains some applications in chemistry and material science, especially those relating to dynamics (studying how molecules and materials change over time).  . 🟡   The “polynomial” box is still interesting, but its applicability is unclear. Recall that a quantum computer would need much more time per step – and on top of that, it will have considerable overhead due to error correction. Does a polynomial reduction in the number of steps overcome this slowness? According to a recent paper, small polynomial speedups (as achieved by Grover’s algorithm) will not cut it, at least not in the foreseeable future.  . 🔴   For some computations, a quantum computer offers no speedup. Examples include sorting a list or loading large amounts of data.  . If this were the complete story, then most people would agree that quantum computing is a bit disappointing. It would be a niche product for hackers and a tiny community of physicists and chemists who study quantum mechanics itself.  . ⚪   Luckily, there is yet another category: many of the most exciting claims come from the heuristic algorithms. This term is used when an algorithm might give a suboptimal solution (which could still be useful), or when we cannot rigorously quantify the runtime. Such algorithms are quite common on classical computers: neural networks fall in this category, and these caused a significant revolution in AI. Unfortunately, it is unclear what the impact of currently known heuristic quantum algorithms will be.  . In summary, we see that the utility of the mentioned quantum applications is unclear – but some are more unclear than others. The following graph summarises this well: the most ‘valuable’ applications (like quantum machine learning) also come with the largest risks, whereas the most convincing speedups (like codebreaking) offer less value. The applications of chemistry and material science sit somewhere in between. Unfortunately, we don’t dare to assign concrete numbers to this graph. That’s something that we will need to empirically find out in the near future. See also: . | A quantitative analysis of Grover’s runtime compared to today’s supercomputers.  . | (Technical!) Amazon researchers lay out a comprehensive list of end-to-end complexities of nearly every known quantum algorithm. | . Where is the killer application? . For a quantum algorithm to be truly impactful, we require two properties:  . | [Useful] The algorithm solves a problem with real-world significance (for example, because organisations can work more efficiently or because it helps answer scientific questions). | [Better/faster] Out of all the possible approaches, this algorithm is the most attractive solution. This is achieved when a realistic quantum computer solves the problem faster than any classical machine could, but other aspects like energy consumption or total cost of hardware and software development can also play a role. Ideally, the quantum computer enjoys an exponential speedup, or at least a large polynomial speedup.  . | . Several algorithms, most notably Grover’s algorithm, have a very wide range of industrial applicability. However, it seems that in practice, other (classical) approaches solve such problems faster and more cheaply.  . Then there exist exponential speedups, like the algorithm for topological data analysis, for which no practical uses have been found (despite many scientific and industrial efforts).  . To our best knowledge, only codebreaking (Shor’s algorithm) is both exponentially faster and extremely impactful. Unfortunately, this is primarily a negative application that helps criminals – we are not aware of any positive uses of Shor, hence this is not quite the killer application that we’re looking for.  . Even in chemistry, it is hard to pinpoint a convincing application. Classical computers are already incredibly fast, and very good classical algorithmic techniques have been developed. Scientist Garnet Chan gives talks which are suggestively titled “Is There Evidence of Exponential Quantum Advantage in Quantum Chemistry?”.  . Could the nature of quantum mechanics be such that it helps us in codebreaking, but in literally nothing else? We think that such a scenario is unlikely. It seems that a killer algorithm has not yet been found, but there are good reasons to hope that we will find one in the future. Perhaps we need novel mathematical tools, or perhaps we simply need to play around on increasingly mature quantum hardware. We hope that we’ll have to incrementally update this page over the coming years, as we slowly uncover the complete set of capabilities that quantum computers have! . | https://quantumalgorithmzoo.org/ &#8617; . | Harrow, Aram W; Hassidim, Avinatan; Lloyd, Seth (2008). “Quantum algorithm for linear systems of equations”. Physical Review Letters. 103 (15) 150502. https://doi.org/10.1103/PhysRevLett.103.150502 &#8617; . | Aaronson, S. Read the fine print. Nature Phys 11, 291–293 (2015). https://doi.org/10.1038/nphys3272 &#8617; . | Liu, Y., Arunachalam, S. &amp; Temme, K. A rigorous and robust quantum speed-up in supervised machine learning. Nat. Phys. 17, 1013–1017 (2021). https://doi.org/10.1038/s41567-021-01287-z &#8617; . | https://medium.com/qiskit/how-ewin-tangs-dequantized-algorithms-are-helping-quantum-algorithm-researchers-3821d3e29c65 &#8617; . | . ",
    "url": "/part1/chapter_4/#the-applications-what-problems-will-we-solve-with-quantum-computers",
    
    "relUrl": "/part1/chapter_4/#the-applications-what-problems-will-we-solve-with-quantum-computers"
  },"20": {
    "doc": "4 The applications, What problems will we solve with quantum computers?",
    "title": "4 The applications, What problems will we solve with quantum computers?",
    "content": " ",
    "url": "/part1/chapter_4/",
    
    "relUrl": "/part1/chapter_4/"
  },"21": {
    "doc": "5 Timelines, When can we expect a useful Quantum Computer?",
    "title": "Timelines: When can we expect a useful Quantum Computer?",
    "content": "At a glance The earliest quantum applications will need several million qubits, according to the most rigorous studies. Assuming an exponential growth similar to Moore’s Law, we predict that these applications could be within reach around 2035-2040. ** . The billion-dollar question in our field is: . When will quantum computers outperform conventional computers on relevant problems? . Unfortunately, nobody really knows, and past predictions often proved inaccurate. Moreover, a relevant quantum computer won’t just appear from one day to another: there’s a continuous evolution where these devices will become increasingly capable. In this chapter, we will show how we can make a rough prediction about future timelines, and what will happen on the path towards large-scale quantum computation. As a small disclaimer, this chapter is very subjective: it’s not hard to arrive at different conclusions simply by choosing different sources and making different assumptions. I did my utmost best to rely on the most up-to-date information, combining the fiews of the most widely accepted papers and making assumptions that are in line with the view of most experts. Since I published the first estimates in 2022, I’ve seen this approach becoming increasingly popular, with the same images presented at conferences and Google selecting snippets from this text on certain search queries. Hence, I have good faith that the views presented here are among the best that we can do! . We will often refer to the goal of having a quantum computer that outperforms a classical computer on a useful task (by being faster, more accurate, or incurring lower costs). In this chapter, we’ll use the word ‘utility’ to refer to this concept. ### . What parameters are relevant? . Compared to currently available technology, we’d require a fundamental improvement to these specifications: . | Number of qubits . | Accuracy of elementary operations (gates). This means that quantum computers have the ability to perform long computations without making mistakes. | . There are quite a few other parameters that matter (such as the connectivity, the available set of gates, the speed of operations, and so forth). In this chapter, we choose to simplify matters by assuming that all of these other parameters are not a bottleneck, allowing us to focus only on the number of qubits and gate accuracies. Balancing qubits and accuracies . In the previous chapter, we learned that error correction allows us to sacrifice some fraction of qubits in exchange for better accuracies. In fact, we expect that this will be a necessity when performing billions of computational steps! Current estimates suggest that, for early applications, we’ll use around a hundred to a few thousand physical qubits for each logical qubit. For error correction to work, we need to make some assumptions. Several studies assume that the bare physical qubits in a quantum computer will have gate fidelities of roughly 99.99%, which intuitively means that an error occurs roughly every 10.000 steps. As of 2024, the largest quantum computers hit roughly 99.90% fidelity. We still need to reduce errors by roughly a factor of 10, but I think it’s reasonable to assume that this can be accomplished. If we’re willing to assume 99.99% gate fidelities and functioning error correction, then we can simplify our analysis even further. Depending on the computational problem to be solved, we can estimate the maximum logical error rates that we can tolerate, and bump up the number of qubits until that error rate has been reached. In other words, the only remaining parameter will be the number of qubits, making the resource estimates a lot easier. This leads to an interesting situation. For more complex problems, we’ll need more qubits both to store a larger amount of data and to reduce the probability of errors so that the computation can run longer. Isn’t the focus on just qubits a bit short-sighted? Doesn’t this create a perverse incentive for manufacturers to focus only on qubit numbers, forgetting about all the other parameters? Well, I would indeed be worried that some companies can make headlines with unusable computers that happen to have a record qubit number. But luckily, most manufacturers seem dedicated to making the most ‘useful’ computer, and customers will surely judge their products by the capabilities of their logical qubits. We’re obviously making a coarse simplification here, but making predictions about the future is hard enough as it is. Back to the main question: When can we expect a large quantum computer? Now that we’re only counting qubits, we can break our billion-dollar question into two parts: . | How many qubits do we need for relevant applications? . | How long will it take before we have that many qubits? . | . Wait! What exactly is this 'accuracy'? The relevance of accuracy is often overlooked, perhaps because this hardly plays a role for classical computers (which operate perfectly for basically all intents and purposes). The problem is as follows: every gate we perform on the quantum computer has a small chance of introducing an error. The more steps a computation takes, the larger the chance that the computation fails. In other words: harder problems put more stringent requirements on the quantum computer’s hardware, and require more accurate operations. Of course, our main focus is on massive-scale computational problems that take hours or even weeks on classical machines.  . To illustrate, imagine a quantum computer where each elementary step (“gate”) has a 1:1000 chance to introduce an error. This computer may suffice for computations of merely 100 gates, but as soon as of the order of 1000 or more gates are needed, the probability that the computer’s output is incorrect becomes quite significant. What if we need to run a circuit of 1 million gates? If the computer has sufficiently many qubits, we will use error correction to lower the probability of error to much less than one-in-a-million. We often state the ‘error’ of a gate, rather than the ‘accuracy’. For example, a gate with 1% chance of error can be said to have 99% ‘accuracy’. Scientists like to define these numbers more precise by talking about gate fidelities or coherence times. Luckily, there is a well-established field of quantum error correction. It allows us to combine many ‘physical qubits’ available on the device (think of the order of 1000) into a single, more accurate ‘logical qubit’, on which gates have a smaller chance of error (say, by a factor 1 billion). This allows much longer computations. Error correction allows a trade-off between the number of available qubits and gate accuracy. For more details, see the chapter on error correction.  . How many qubits are needed? . In a previous chapter, we discussed the three main applications of quantum computers: quantum simulation, breaking cryptography, and optimisation. The most concrete numbers can be given for Shor’s algorithm (breaking cryptography), where we have a very clear problem to tackle: obtain a private (secret) key from a widely-used cryptosystem, like the RSA-2048 protocol. This is the perfect benchmark because there can be no discussion about whether the problem is solved: one either obtains the correct key, or one doesn’t. Moreover, we’re quite convinced that even the best classical computers can’t solve the problem (or else you shouldn’t use internet banking or trust software updates).  . A recent estimate finds that a plausible quantum computer would require roughly 20 million ‘reasonably good’ physical qubits to factor a 2048-bit number. The whole computation would take about 8 hours1. Such estimates require several assumptions on what a quantum computer would look like in the future. In this case, the authors assume qubits are built using superconducting circuits, which are laid out in a square grid. Error correction is assumed to be done using the so-called surface code, assuming the best-known methods for error correction in 2020. Note that future breakthroughs could reduce the required time and number of qubits even further. For chemistry and material simulation, it’s a lot harder to give estimates, because there is not just a single problem to tackle here: one typically uses computers to gradually improve our understanding of a complex structure or chemical reaction. This should be combined with theoretical reasoning and practical experiments. Moreover, classical computers are often able to perform the same computations that the quantum computer would make, at the cost of making certain assumptions or simplifications. There’s a fuzzy region between ‘classically tractable’ and ‘quantum advantage’. The most concrete task is to compute the energy of certain molecular configurations. The benchmark is to provide energies more accurately than done in conventional experiments: one canonically takes the ‘chemical accuracy’ of roughly 1 kcal/mol as the precision to beat. Then, of course, we should focus on molecules where classical computers cannot already achieve such accuracies.    . Note that the accuracy of a chemical energy should not be confused with the accuracy of a quantum gate, which is a whole different number. A highly promising and well-studied benchmark problem is the simulation of the so-called FeMo cofactor, in short: FeMoco. This molecule is relevant when bacteria produce Ammonia (NH3), a compound that is of great relevance to a plant’s root system. A better understanding of this process could help us reduce the ridiculously large carbon emissions now associated with the production of artificial fertilizer. We give more details in a separate chapter.  . Simulating FeMoco is believed to require around 4 million qubits (and around 4 days of computation for a single simulation run). The assumptions of the hardware and error correction are similar to those of Shor’s algorithm: the estimate is based on a square grid of superconducting qubits, using surface code to correct errors. For a different enzyme, namely cytochrome P450, it has been estimated that around 5 million qubits are needed (again taking roughly 4 days of computation). Altogether, we conclude that a couple million qubits (of sufficiently high quality) can make quantum computers relevant for R&amp;D in chemistry.  . Some tasks that are mainly of interest for scientific purposes, such as simulating models of quantum magnets, can be achieved with fewer resources. Under similar assumptions, simulating a 2D transverse field Ising model is estimated to take just under 1 million qubits. For many optimization problems, it’s practically impossible to give reasonable estimates. For one, a true killer algorithm for optimization problems is not known yet. The algorithms that are presented as the most promising are often heuristic, meaning that scientists do not know how long an algorithm will take to find solutions. This can happen, for example, when an algorithm repeats a certain loop until a stopping criterion has been met. This may come as a shock, considering the many news items that report how quantum computers may already solve practical problems. But don’t be fooled: these articles state that quantum computers can indeed solve relatively simple problems, but often fail to mention that there exist different approaches by which classical computers can solve the same problems much, much faster.  . Many optimization problems have a plethora of potential solutions, but the goal is to find the optimal solution (say, the one that incurs the least costs or gets you to your destination the fastest). The solution space is often so large that we don’t even know if we hit the optimal solution, but we’re okay with finding one that’s pretty close. Several papers claim that a quantum computer already finds solutions faster, but in all cases, worrying sacrifices were made for the optimality of the solutions. One of the leading sources of wild claims is D-Wave’s quantum annealer. This is a ‘limited’ quantum computer that cannot run every quantum algorithm, but it’s extremely good at one specific algorithm called “quantum annealing”. With around 5000 qubits, it can handle reasonably large problems, and several companies have tried their hands at this machine. The results obtained through today’s quantum annealing approaches are likely also achieved by putting a similar amount of effort into a good classical method. We can summarize our conclusions in the table below. | Application | How well can we estimate qubit requirements? | Use case example | Qubits needed? | Gate error assumed? | . | Breaking cryptography | Good | Cracking RSA-2048 | ~ 20 million | ~ 0.1% | . | Chemistry | Reasonable | Simulation of FeMoco | ~ 4 million | ~ 0.1% | . |   |   | Simulation of P450 | ~ 5 million | ~ 0.1 % | . | Optimization / AI | Bad | ? | ? |   | . Figure 1: The electrons around an atom are generally in a superposition over an infinite number of locations. Shown are the areas with a large probability of finding the electron. In some regions, the amplitudes are positive numbers (blue), in others, they are negative (yellow). Image source: https://en.wikipedia.org/wiki/Atom#/media/File:Atomic-orbital-clouds_spdf_m0.png . What about future improvements? . It seems almost inevitable that methodologies to break cryptography and simulate molecules will improve. On the other hand, it’s impossible to estimate by how much. Will we reduce qubit requirements by a few per cent? Or by a factor of ten? By a factor of one thousand? . Some sources actually try to extrapolate the reduction in required qubits over time (like Youtube science educator Veritasium2 and a report by McKinsey3), but this is such a wonky linky line over incrompehensive scales that we will follow this strategy. We observe that, in error correction techniques alone, there appears to be steady progress to improve methodologies (see the chapter on challenges in hardware). These can lower the number of physical qubits needed, and set a lower bar for other parameters (like gate accuracies or connectivity). Based on discussions with scientists, lowering the qubit requirements by a factor of 3 to 10 seems plausible. Hence, for optimistic readers, we can set another target at around 400.000 qubits (which seems at a similar scale as early simulations for theoretical physics models). | Application | How well can we estimate qubit requirements? | Use case example | Qubits needed? | Gate error assumed? | . | Chemistry (Optimistic) | Reasonable | Simulation of FeMoco (with 10x improved methods) | ~ 400.000 | ~ 0.1 % | . | Science | Reasonable | 2D Transverse field Ising model | ~ 900.000 | ~ 0.1 % | . Figure 2: News header by Techradar. Source: https://www.techradar.com/news/google-creates-quantum-chip-millions-of-times-faster-than-the-fastest-supercomputer . ### . ### . Can noisy algorithms be good enough? . Current quantum computers are somewhat small and not yet capable of large-scale error correction: they are Noisy Intermediate-Scale Quantum (NISQ) devices. An important question is: can we already achieve any utility with such noisy devices before we achieve large-scale error correction? That is one of the most disputed topics in our field, and therefore it deserves some attention. There seems to be a large effort from startups and enterprises to cook up applications with a convincing advantage, even on noisy devices. This noble effort would massively increase the overall usefulness of quantum computers. So far, their findings have convinced some experts, but most remain extremely sceptical about NISQ utility. Maryland-based professor Sankar Das Darma underlines this picture in his opinion article “Quantum computing has a hype problem”.4 He states about NISQ that “the commercialization potential is far from clear”, pointing at the unsatisfying evidence behind several claims in finance, machine learning and drug discovery. Several consultants made ridiculous claims about how tiny machines would see an exponential advantage over enormous supercomputers. Now that the field is coming of age, many are becoming more careful. To illustrate, when looking back at a 2021 report, BCG chivalrously admits5: . “Our assumptions for near-term value creation in the NISQ era, however, have proved optimistic and must be revised.” . The most serious claim about NISQ utility comes from a Nature publication by the IBM team, in a paper called “Evidence for the utility of quantum computing before fault tolerance”.6 However, their arguments were quickly refuted by further studies that simulated IBM’s impressive quantum experiment on a conventional laptop.7 . Most experts seem to keep an eye on NISQ applications but will agree that no utility for NISQ machines has been found yet. An overview article about pharmaceutical applications8 has a careful yet suggestive message: . “Most NISQ algorithms […] rely heavily on classical optimization heuristics, and the actual run time is difficult to estimate. Furthermore, recent results suggest that in NISQ approaches, the number of measurements required to achieve a given error scales exponentially with the depth of the circuit. For these reasons, here we focus our discussion exclusively on fault-tolerant quantum computers.” . Similarly, a recent overview9 of quantum chemistry seems to remain agnostic with regards to NISQ advantage while pointing out that fault-tolerance has a higher chance of succeeding. “… it is difficult to predict when or if algorithms on near-term noisy intermediate-scale quantum devices will outperform classical computers for useful tasks. But it is likely that, at some point, the achievement of large-scale quantum error correction will enable the deployment of a host of so-called error-corrected quantum algorithms” . In this book, we follow the view of most authorities in the field by playing it safe and sticking to the well-understood use cases for early fault-tolerant quantum computers that we discussed previously. We can not rule out new breakthroughs that allow NISQ utility, but we’re unwilling to gamble that they will happen. Any such breakthrough could completely stir up our fragile prediction – but so would unexpected backlashes in hardware development or even unforeseen funding stops. ### . How long until we have million-qubit machines? . Now that we’ve set our target to roughly a million qubits, we’d like to estimate when such hardware will be available. We highlight the following sources: . | Road maps and claims of hardware manufacturers . | Surveys to experts . | Extrapolation of Moore’s law . | . What do manufacturers say? . Below, we see the qubit numbers that several manufacturers have already realized (solid disks) and what they will produce in the future according to their public road maps (opaque plusses). Note that the vertical axis is logarithmic, so it displays a very broad range from ~10 to ~10.000 qubits. A lower number of qubits does by no means indicate that these computers are worse. In fact, the machines with the lower numbers of qubits on this graph happen to have an important edge in other parameters, such as gate accuracies and qubit connectivity. Next to their road maps, companies sometimes make more daring claims in media interviews or at presentations at large events. Based on the application targets above, it should be no surprise that manufacturers shoot for around a million qubits as a ‘moonshot’ accomplishment. Back in 2020, IBM claimed to reach the 1 million qubit target by 203010. Around the same time, Google was interpreted by journalists to do this even faster (around 202911). The start-up PsiQuantum, which made waves thanks to record-high investments of over a billion dollars for their photonic quantum chips, went as far as claiming to have a million qubits by 202512. It seems that these claims were a bit too ambitious. In 2024, with only a year to go and no publicly presented product progression whatsoever, PsiQuantum shifted its 1 million qubit road map to 202713. IBM took an even more conservative step, where it’s now claiming to have just 100.000 qubits in 203314 (although this machine should meet the error correction features that we dreamingly assumed in the previous sections). Although this delay sounds disappointing, hardware manufacturers are still making impressive progress, as the number of qubits grows faster than one would predict according to Moore’s Law for classical chips! . Companies working on trapped ion machines tend to have fewer qubits, but higher gate accuracies. Perhaps this is why IonQ displays its road map in a different format: they aim to have 1024 so-called algorithmic qubits by 202815. This means that IonQ will have at least this number of qubits, but also guarantees sufficient gate accuracy to be able to run reasonably long circuits. It’s unclear whether a simple form of error correction may be needed to achieve this. If so, the actual number of physical qubits may be some orders of magnitude higher. Quantinuum (previously working under the name Honeywell) makes less concrete predictions but expects fault-tolerant computing by 203016 (meaning that significant error correction is in place). What does Moore’s law say? . One could assume that quantum computers will ‘grow’ at a similar rate as classical computers. Moore’s law states that the number of transistors in a dense integrated circuit grows exponentially: the number doubles roughly every two years. This has been a surprisingly accurate predictor for the development of classical IT. If we apply Moore’s law to quantum, to get from one thousand to one million would take around 20 years – predicting that the million-qubit mark won’t be passed until 2044. Clearly, most hardware manufacturers are more optimistic. If we assume the number of qubits doubles each year, one would predict that 1-million-qubit machines will be available in ten years. While doubling a quantum computer’s size each year is already a daunting challenge, companies like IBM and Pasqal and QuEra set the bar even higher for themselves, hoping to double every 7-9 months. What do experts say? . The Global Risk Institute conducts yearly surveys asking experts to state the likelihood that quantum computers pose a significant threat to public-key cryptography 5 years from now. Similarly, respondents would also estimate the likeliness 10, 15, 20, and 30 years away. This essentially boils down to the question: when will a quantum computer run Shor’s algorithm to crack RSA-2048? We previously saw that around 20 million qubits would be needed for this (although experts may take into account that this number can still be reduced). We consider this an important source because many important authorities in the field (like professors and corporate leaders) take part in this study. The results from December 202317, gathered from 37 participants, are displayed below. **How to read this graph?** Let’s look at the column labeled ‘5 years’. A total of 24 correspondents indicate that there is less than 1% chance that quantum computers pose a security threat in the next five years. A single person is quite pessimistic and assigns &gt;70% chance that this will happen. On average, experts say that there’s a fairly small chance that quantum computers will pose a threat to cryptography in the next 5 years.  . Further to the right, the ratios shift: looking at 20 years from now, the majority of experts believe that quantum computers pose a serious threat: over half of them assign a likelihood of 70% or more. It appears that the majority of experts believe that the tipping point is between 10-20 years from now. Somewhere between 15 and 20 years away, there’s a point where the median participant assigned roughly 50% chance to see a quantum computer capable of breaking cryptographic codes. However, we should take into account a significant uncertainty: several experts make wildly varying estimates, so there’s no obvious conclusion from this data. These experts are likely aware of hardware manufacturer’s road maps, as we shall see below. Putting it all together . In the infographic below, we present all of our findings in a single graph. Assuming that qubit numbers will grow exponentially (and that all other parameters will keep up accordingly), we can consider several scenarios. A pessimistic scenario would be that the number of qubits ‘merely’ follows the classical version of Moore’s Law, and qubit numbers double only once every two years (dotted line). Then, we’d have to wait well past 2040 to even reach 100.000 qubits. An extremely optimistic outlook would follow the blue dashed line (which extrapolates the progress by IBM, doubling their qubits every ~9 months). If one also believes in useful applications with much less than a million qubits, then we may be able to run these around 2030. An intermediate perspective is to assume that the number of qubits doubles annually. Interestingly, this seems to be roughly in line with IBM’s latest claims and the typical expert opinion. Depending on the application, it would mean that quantum chemistry simulation and codebreaking canbe within reach between ~2033 and 2040. To conclude, our estimates strongly depend on the assumptions that you’re willing to accept (who would’ve thought!). Do you believe that improvements in algorithms and error corrections will allow for applications with much less than a million qubits? How quickly do you believe that the hardware will improve? If you force me to make a prediction, I’d say the first applications arise between 2035 and 2040, with the understanding that there’s a huge margin for error. As a final remark, a full utility-scale quantum computer requires much more than a growth in the number of qubits. To reach the first useful applications, we likely require simultaneous progress in algorithmics, software, gate accuracies, error correction techniques, fridges, lasers, and many other important subfields of quantum computing. Hopefully, all these disciplines will find the required breakthroughs that will enable the exponential growth of quantum computing hardware. Further reading: . | Scientist Samuel Jaques (Waterloo) makes insightful graphs that combine the number of qubits and the error rates, and puts them in the perspective of what is required for various applications. https://sam-jaques.appspot.com/quantum_landscape_2023 | . | How to factor 2048 bit RSA integers in 8 hours using 20 million noisy qubits; Craig Gidney, and Martin Ekerå, Quantum 5, 433 (2021), https://quantum-journal.org/papers/q-2021-04-15-433/ &#8617; . | See https://www.youtube.com/watch?v=-UrdExQW0cs&amp;t=1024s, starting at 17:04. &#8617; . | McKinsey Quantum Technology Monitor 2024, https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/steady-progress-in-approaching-the-quantum-advantage &#8617; . | https://www.technologyreview.com/2022/03/28/1048355/quantum-computing-has-a-hype-problem/ &#8617; . | https://www.bcg.com/publications/2024/long-term-forecast-for-quantum-computing-still-looks-bright &#8617; . | https://www.nature.com/articles/s41586-023-06096-3 &#8617; . | https://arxiv.org/abs/2306.16372 &#8617; . | Santagati, R., Aspuru-Guzik, A., Babbush, R. et al. Drug design on quantum computers. Nat. Phys. 20, 549–557 (2024). https://doi.org/10.1038/s41567-024-02411-5 &#8617; . | Quantum Chemistry in the Age of Quantum Computing, Chem. Rev. 2019, 119, 19, 10856–10915, https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.8b00803 &#8617; . | https://fortune.com/2020/09/15/ibm-quantum-computer-1-million-qubits-by-2030/ &#8617; . | https://quantumcomputingreport.com/google-goal-error-corrected-computer-with-1-million-physical-qubits-by-the-end-of-the-decade/ &#8617; . | https://www.nextbigfuture.com/2020/04/psiquantum-targets-million-silicon-photonic-qubits-by-2025.html; https://www.icvtank.com/newsinfo/629365.html &#8617; . | https://quantumcomputingreport.com/psiquantum-receives-940-million-aud-620m-usd-to-install-a-1-million-qubit-machine-in-australia-by-2027/ &#8617; . | https://www.iotworldtoday.com/industry/ibm-details-road-to-100-000-qubits-by-2033 &#8617; . | https://ionq.com/posts/december-09-2020-scaling-quantum-computer-roadmap &#8617; . | https://www.honeywell.com/us/en/news/2020/10/get-to-know-honeywell-s-latest-quantum-computer-system-model-h1 &#8617; . | https://globalriskinstitute.org/publication/2023-quantum-threat-timeline-report/ &#8617; . | . ",
    "url": "/part1/chapter_5/#timelines-when-can-we-expect-a-useful-quantum-computer",
    
    "relUrl": "/part1/chapter_5/#timelines-when-can-we-expect-a-useful-quantum-computer"
  },"22": {
    "doc": "5 Timelines, When can we expect a useful Quantum Computer?",
    "title": "5 Timelines, When can we expect a useful Quantum Computer?",
    "content": " ",
    "url": "/part1/chapter_5/",
    
    "relUrl": "/part1/chapter_5/"
  },"23": {
    "doc": "6 Four myths about quantum computing",
    "title": "Four myths about quantum computing",
    "content": "Myth 1: Quantum computers can try a huge number of solutions at once . This myth derives from the concept of superposition: if a qubit can represent the numbers 0 and 1 at the same time, then a mere 1000 qubits can represent \\(2^{1000}\\) unique numbers, all at the same time. That’s an incomprehensibly large quantity of numbers that are represented at the same time, much more than there are atoms in the visible universe – even the fastest computers in the world couldn’t loop through all these numbers in a lifetime. Remember that all of these numbers, as stored in a quantum memory, can also be interpreted differently, perhaps as different Excel files, webpages, CAD drawings, or whatever kind of data we choose to work with. A smart computer scientist can devise a way to make 1000 bits represent ‘solutions’ to some problem. For example, imagine that we want to find an optimal aeroplane wing that generates incredible lift while requiring as few materials as possible. Using quantum superposition, we might represent \\(2^{1000}\\) such wings all at once. We picked the example of aeroplane wings because it is clear that simulating its aerodynamic properties requires a pretty hefty computation. Let’s assume that we have written such a computer program that does an accurate simulation of the wing, and call that program \\(f\\). It will output 1 if the wing works well (according to whatever metric), and 0 otherwise. Surely, the program takes a very large number of computation steps, which we’ll call N. The program will need some input, denoted by \\(x\\), which is a 1000-bit description of all the relevant properties of a hypothetical aeroplane wing. In other words, the compute program computes \\(f(x) = 1\\) if \\(x\\) is a fantastic wing, and \\(f(x) = 0\\) if it’s rubbish. Now, a quantum computer should be able to execute any classical function, right? We should be able to run \\(f\\) on a quantum computer, but now we have the unique feature that the 1000-qubit input can actually represent a humongous number of potential aeroplane wings at the same time! By doing mere N computational steps, we can check the properties of \\(2^{1000}\\) solutions! . If this actually worked, quantum computers would have an astonishing power. They could straightforwardly find mathematical proofs that humans haven’t been able to solve in centuries. They would rapdily produce the perfect train and bus schedules, discover new drugs and straightforwardly hack encryption systems. They would solve problems in the complexity class NP, which is widely believed to be impossible with machines in our universe, owing to the famous P ≠ NP conjecture. So where’s the catch? For those who read the introduction to quantum physics, we shouldn’t forget about the postulate of quantum measurement. The output of the computation would be a superposition over \\(2^{1000}\\) outcomes. If we want to learn anything about this output, we’d perform a quantum measurement collapses this superposition. Instead of looking at \\(2^{1000}\\) different solutions simultaneously, we get to see only 1 outcome – corresponding to the performance of just a random aeroplane wing. In this case, there is no advantage compared to a classical computer, because we could’ve just as well picked a random wing at first, and then spent N steps on a (much faster) classical machine. Although this ‘quantum parallelism’ is too good to be true, quantum computers can use the above idea to some lesser extent. Using Grover’s algorithm, we can find desirable solutions (the \\(x\\) for which \\(f(x) = 1\\)) in roughly the square root of the number of values that \\(x\\) can take. In the above example, the number of required steps is reduced to \\(\\sqrt{2^{1000}}\\ N = 2^{500}\\ N\\). This is an incredible reduction, but we’re still looking at a number of steps larger than the number of atoms in the universe – which is far from ‘efficient’. Myth 2: Qubits can store much more data than the same number of classical bits. This myth is very similar to the previous one: can’t N qubits represent \\(2^{N}\\)different numbers at the same time? Or aren’t they perhaps even more powerful, because for each of the \\(2^{N}\\) different numbers, there’s a complex number, which can have as many decimal digits as we like? . Again, by the rules of quantum measurement, this is too good to be true. It’s impossible to store much information in a qubit because it collapses to a classical 0 or 1 when we measure it. The problem is really in retrieving the information, where we have very limited capabilities. For the same reason, when sending a classical message over a long distance, there’s little value in using qubits as information carriers. As a side note, there is a cool related protocol called ‘superdense coding’ that you may like to look up out of theoretical interest. Also, when your data is actually quantum (for example, the state of electrons in a molecule), then storing this data in qubits does have a potentially huge advantage. Myth 3: Entanglement allows you to send information faster than light, or to influence objects at a distance . Entanglement is an incredibly confusing phenomenon, especially because our most common interpretation of quantum mechanics states that whenever we measure one qubit, the state of a remote qubit can drastically change. Whilst this picture is useful for physicists when performing computations, it tricks our intuition. Imagine that, in the faraway future, we want to protect our solar system against an alien invasion. We installed sentinels on faraway outposts, who should signal earth of any approaching dangers. Alice is one of these sentinels, stationed at a remote asteroid in the icy Kuiper Belt. She brought with her a qubit labeled A, which is entangled with qubit B that’s safely kept on earth by her colleague Bob. Whilst it takes light signals around 5 hours to travel between them, isn’t there a way for Alice to alarm Bob any faster, possibly by doing some special operations on her qubit? Perhaps she could even give some clues about the type of looming threat? . Unfortunately, Alice cannot remotely change any measurable quantity of Bob’s qubit. The measurements that Bob performs will always have the same outcome probabilities, no matter what Alice does to her qubit. Having more qubits, or using different quantum objects won’t help either. Fundamentally, there is no way to signal any information faster than the speed of light. There is a subtle difference between “changing measurable quantities” and “knowing something” about the state of a particle. To illustrate, assume that we start with a special entangled state: measuring qubits A and B will result either in both qubits being “0”, or both qubits being “1”, let’s say with 50% probability each. Measuring something like A=“0” and B=“1” is impossible. When Alice measures her qubit and reads the outcome “0”, then she immediately knows the outcome of a future measurement made by Bob: she knows this will be “0” with 100% chance. However, this knowledge is not accessible to Bob. He doesn’t know even whether Alice measured or not! Even if they agreed in advance that Alice would measure at a set time, Bob doesn’t know her outcome. From his perspective, the probabilities to obtain “0” or “1” are still equally likely. Something interesting happens when Alice sends a message to Bob to inform him about her outcome. With this update knowledge, Bob suddenly knows precisely what the state of his qubit is: it must have collapsed to “0”, and he can perfectly predict the outcome of a subsequent measuremnt. In a way, this did indeed change the state of the qubit from Bob’s perspective, but it was only possible after some information carrier was transported from Alice to Bob, a process that must have taken at least 5 years! . What is quantum entanglement good for, then? Some useful applications include: . | Creating certifiably secure encryption keys at remote locations. | Creating certifiable randomness. | Forming connections between seperate quantum computers, allowing them to send quantum data to each other using teleportation. For this to work, the computers also need to communicate some classical data, so the data transfer is never faster than the speed of light. This could be a way to scale up quantum computers when a limited number of qubits can fit on a single chip, or within a single fridge. | . ### . Myth 4: Quantum computers are always 10 years away. This statement is a playful reference to the situation around nuclear fusion, where predictions of its realisation being just 30 years in the future have repeatedly been postponed. Scientists have been working on fusion for decades, but it’s still far from a mature source of energy. Similarly, I’ve heard several overly optimistic claims about quantum computers being made in the past 10 years, often claiming that quantum computers are somewhere between 3 to 10 years away. An article by TechCrunch1 boldly paraphrases Dario Gil (IBM) and Chad Rigetti (founder of Rigetti Computing) saying that “the moment that a quantum computer will be able to perform operations better than a classical computer is only three years away”, whilst this article was published back in 2018. For reference, the 127-qubit Eagle chip was announced by IBM at the end of 2021, but even several years later, it’s still primarily used for testing and education. In 2019, consulting firm Gartner published “The CIO’s Guide to Quantum Computing” where is indicates that 100—200 qubits are suficient for “key potential applications” in chemistry. They also predicted that “by 2023, 20% of organizations will be budgeting for quantum computing projects”. I’d like to take this opportunity for calling them out for over-hyping. Similarly, Microsoft made claims in 2018 that their cloud platform Azure would feature quantum computing in 5 years2, which admittedly is technically true. However, they have repeatedly hinted to do this with fault-tolerant topological qubits, which still remain elusive. Startup PsiQuantum famously claimed to have a million photonic qubits by 20253 (which remains to be seen), and consultants at BCG advised that quantum computers “generate business value” in the same year4. However, if you’re reading this book, you must have noticed that not all experts share the same vision. Most scientists have warned for a long time that quantum computing is a long-term effort. Nevertheless, the thesis that ‘quantum computing is always X years away’ is hard to defend, thanks to convincing evidence that we are steadily progressing towards a clear goal. Every year, quantum hardware sees major improvements, both in terms of the number of qubits, their stability, and the level of control that is demonstrated. Most experts even expect an exponential scaling of the number of qubits, similar to Moore’s Law, and manufacturers have clear roadmaps that underline these predictions. Moreover, theorists have set clear targets for when the hardware is good enough—and we’d sooner see the requirements drop with new breakthroughs rather than become more stringent. Building a quantum computer is a long marathon, and it’s impossible to predict the precise year in which we’ll have convincing commercial quantum computers, the rapid rate of progress is undeniable. ### . Further reading . | Veritasium explains Entanglement . | (Technical!) Minute Physics explains Teleportation (with some math notation). | Chris Ferrie debunks more myths in his free book “What you shouldn’t know about Quantum Computers” . | . | https://techcrunch.com/2018/09/07/the-reality-of-quantum-computing-could-be-just-three-years-away/ &#8617; . | https://www.computerweekly.com/news/252440763/Microsoft-predicts-five-year-wait-for-quantum-computing-in-Azure &#8617; . | https://www.ft.com/content/a5af3039-abbf-4b25-92e2-c40e5957c8cd &#8617; . | https://www.bcg.com/publications/2023/enterprise-grade-quantum-computing-almost-ready &#8617; . | . ",
    "url": "/part1/chapter_6/#four-myths-about-quantum-computing",
    
    "relUrl": "/part1/chapter_6/#four-myths-about-quantum-computing"
  },"24": {
    "doc": "6 Four myths about quantum computing",
    "title": "6 Four myths about quantum computing",
    "content": " ",
    "url": "/part1/chapter_6/",
    
    "relUrl": "/part1/chapter_6/"
  },"25": {
    "doc": "1 Applications in chemistry and material science",
    "title": "Applications in chemistry and material science",
    "content": "Perhaps the most credible application of quantum computers is to study of quantum physics itself, and how it drives the behaviour of microscopic systems like molecules, materials, or even sub-atomic particles. As far back as 1981, physicist Richard Feynman ended a conference talk with a famous quote, hinting at the opportunities of quantum computing1: . “I’m not happy with all the analyses that go with just the classical theory, because nature isn’t classical, dammit, and if you want to make a simulation of nature, you’d better make it quantum mechanical”. Since then, scientists found increasingly sophisticated algorithms to mimic nature on quantum devices. In this chapter, we will address the impact quantum computing may have on chemistry and material science, discuss the most relevant algorithms, and analyze why the enzyme FeMoco receives such widespread attention. What problems in chemistry and material science will we solve? . The computational problems that chemists care about typically come in two flavours. The most studied problem is finding the arrangement of particles that has the lowest possible energy, which we call the ground state. In nature, we often find a system in (or close to) its ground state. In the context of molecules, the atomic nuclei are relatively heavy, while the lightweight electrons move much faster and are more prone to be entangled or in a quantum superposition. Therefore, chemists tend to make approximations that allow them to focus primarily on the positions and spins of the electrons: the electronic structure problem. The other problem is about dynamics: given some initial configuration of particles, how do they reconfigure themselves after a certain amount of time? This is often referred to as the (time) evolution of a system. Both problems are often informally referred to as ‘quantum simulation’. We often receive the question: why is it so hard to simulate quantum mechanics on a classical computer? Intuitively, the problem arises when we deal with many particles that exhibit large amounts of superposition and entanglement, such that the location of one particle is heavily dependent on the (undecided) position of many other particles. We call such states strongly correlated. The computational problems arise because we may need to keep track of all the possible locations that particle A can be, but also all the locations of particle B, and the same for particle C, etcetera, which quickly winds up to an exponential number of possible sets of conditional locations. In other words, the number of relevant amplitudes (see the section What is quantum?) that we need to keep track of, grows very quickly. Even with a mere one hundred particles, brute-force simulation is far beyond the capabilities of the world’s best supercomputers. It is a common misconception that quantum computers straightforwardly offer an exponential advantage compared to classical computers for all chemistry problems. An influential recent paper reports2: . “[…] we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.” . Note that this comment is specifically about finding ground states, which is still arguably the most relevant problem in chemistry. There is still ample evidence that quantum computers offer an exponential speedup for time evolutions. There is more bad news for quantum computers. Over the years, computational chemists have found brilliant hacks and optimisations to work around the classical computer’s bottlenecks, which has raised a high bar before a quantum computer can meaningfully compete. For nearly every problem in chemistry, there appears to be a clever trick to run this somewhat efficiently on a classical machine. For a quantum killer application, we likely need to search in a fairly specific niche, right at the sweet spot where classical methods struggle while a quantum computer excels. It is not entirely clear how large this niche is, and it is an active research area to identify more systems where classical methods fall short. To illustrate, a recent review article states3: . [Classical methods struggle with] multi-metal systems, where multiple metal ions are in similar electronic environments and interactions. These appear in some biological systems, including enzymes (for example, FeMoco and P450), but it is unclear how common they are or what the added value of an accurate system description would be. Still, there seems to be realistic hope that quantum computers can, at the very least, make tangible contributions within niche areas. The first end-users will most likely be scientists who study the fundamentals of quantum systems, as is already done in physics experiments today. We wouldn’t call these devices computers yet, but rather ‘analogue simulators’. One of the first actual ‘computer’ applications could be to study models of quantum materials, such as the famous Hubbard model4. Looking at more commercially relevant applications, the aforementioned multi-metal systems are often cited, which are relevant to calculating ligand binding affinities in drugs and understanding the mechanism behind the biological production of ammonia. We study the latter example at the end of this chapter. Another potential area could be to understand and search for high-temperature, superconductors5. It is hard to say what the impact of quantum computers will be beyond these niche areas, as this will depend strongly on the usefulness of small polynomial speedups and unpredictable breakthroughs in quantum algorithms. We see a broad palette of applications that are proposed, such as water splitting (to efficiently produce hydrogen as fuel), carbon capture mechanisms6, the study of efficient solar cells and the development of higher capacity batteries. Algorithms for quantum chemistry . Three quantum methods deserve to be mentioned. The first is the Trotter-Suzuki method, sometimes called ‘Trotterization’, which simulates time evolution. In this case, we assume that the initial state is encoded in the qubits of the quantum computer. The Trotter-Suzuki method is guaranteed to return a good approximation of the state at a later time, again encoded in the qubit registers. The most common tool for finding a ground state is quantum phase estimation (QPE), which reports the energy of a certain quantum state. As a subroutine, it requires some evolution method, like Trotter-Suzuki. Unfortunately, it will only succeed in finding the ground state if it gets as input a state that is a reasonable approximation to the ground state. This shifts the problem to: how do we produce a good candidate for the ground state? . The most popular algorithm for creating states with certain properties is the variational quantum eigensolver (VQE). This is an example of a variational quantum circuit: a series of gates that can be gradually changed until the output matches certain requirements. Just like other variational approaches, it is a heuristic algorithm, lacking rigorous guarantees that it will produce the desired output in a reasonable time. Creating a good approximation to a ground state is, in general, NP-hard. This means that it is extremely unlikely that a rigorous algorithm for this task will be found. On the other hand, there is good hope that more heuristic methods (just like VQE) will be found that work well on certain subsets of problems (for example, the kind of molecules frequently encountered in the human body). In fact, such heuristic methods already form the workhorse of classical computational chemistry, with tools such as Density functional theory (DFT), Configuration Interaction (CI) and Quantum Monte Carlo (QMC), although these are too slow to address very large systems such as drugs7. A typical workflow for finding a ground state on a quantum computer could be as follows: We construct a quantum circuit that first performs a VQE, followed by QPE on the output of the variational circuit. This way, the output of the circuit represents the energy of the state produced by the VQE. We then allow a classical optimiser to run this circuit many times, gradually changing the parameters of the VQE until the lowest possible energy is found. Needless to say, there exist various more technical and more sophisticated methods, for which we refer to other more technical sources, such as: . TODO . A hype around quantum computing for climate changes . Some sources make spectacular (but extremely dubious) claims about how quantum computing could be a key ingredient to solve climate change, thanks to the boost to R&amp;D on batteries, carbon capture, and more efficient chemical processes. But McKinsey takes the biscuit with their report titled “Quantum computing just might save the planet”8. We’ll remain agnostic as to whether the impact of chemistry R&amp;D is really sufficient to save the planet, but the claim of quantum computing as a saviour is especially ridiculous because it will still take several years before these machines are sufficiently mature. To limit global warming to no more than 1.5° C, we need to take action much sooner. Imperial College London writes on their website9, referencing the 2014 IPCC report: . “Limiting warming to 1.5°C will only be possible if global emissions peak within the next few years, and then start to decline rapidly, halving by 2030.” . At the same time, our chapter on timelines shows that it is exceedingly unlikely that serious quantum utility in chemistry R&amp;D is possible anywhere before the 2030s. Nevertheless, it’s interesting to look at one of the ‘killer applications’ often mentioned in this context: a use case that has both 1) a convincing quantum speedup and 2) serious real-world impact. We’ll zoom in on the molecule FeMoco, which precisely has a multi-metal system that classical methods struggle with. To understand the relevance of this molecule, we need to dive into the world of food production. A case study of a promising enzyme: FeMoco . Chemical structure of the FeMo cofactor, taken from Wikimedia. Today’s agriculture relies heavily on the use of artificial fertilizer to grow our crops. Without large-scale use of supplementary nutrients, it would be problematic to feed our world’s huge population. In fact, about half of the nitrogen atoms in our body have previously passed a fertilizer factory! . Unfortunately, the production of fertilizer involves enormous energy consumptions and carbon emissions. The main culprit is the ingredient ammonia (NH3), of which we use as much as 230 Mton per year. Although our air consists mainly of molecular nitrogen (N2), plants cannot directly absorb this. Instead, they rely on bacteria (or, in the case of artificial fertilizer, humans) to perform so-called nitrogen fixation, breaking the strong triple bond of molecular nitrogen and converting this into ammonia. Microorganisms can convert this into further nitrogen-containing compounds that can be absorbed by the root system. Pretty much all of the world’s ammonia production follows the so-called Haber-Bosch process, where hydrogen gas (H2) and nitrogen gas (N2) react together to form ammonia. The process can be implemented in large, high-yield production lines, but also comes with a major disadvantage: its staggering energy consumption. This is mainly due to two essential steps: producing sufficiently pure hydrogen and nitrogen gasses, and later separating the H2 and N2 molecules into individual atoms. The latter is especially challenging for N2 due to its strong triple bond. To achieve this, factories operate at extreme conditions, with high temperatures (~400 degrees Celsius) and high pressure, largely driven by natural gas. As much as 1.8% of the world’s CO2 emission is caused by factories performing such reactions, consuming around 3-5% of the world’s natural gas production! . Can’t this be done more efficiently? In this case, we strongly suspect so. Certain bacteria are also capable of making ammonia, but in a much more efficient way: without high temperatures or high pressure. It would be extremely valuable to copy this trick. To imitate the bacteria, we need to better understand a particular substance, the FeMo cofactor (short: FeMoco), which acts as a catalytic active site during ammonia production. A perfect simulation of FeMoco is not possible on classical computers, as the structure of roughly 120 strongly reacting electrons rapidly becomes intractable. Researchers from ETH Zurich and Microsoft were the first to recognize that a quantum computer may aid in a more accurate study of FeMoco. A few years later, researchers at Google gave a more concrete prediction: with about 4 million qubits and 4 days of computing time, a single simulation could be accomplished. With FeMoco, we seem to finally have an example that confidently ticks all the boxes for quantum utility: classical methods are limited, we have well-understood quantum methods, and computational results have a significant commercial and societal impact. Unfortunately, there is yet another catch – innovation never comes so easily. A recent article10 quotes that industrial production of a ton of Ammonia costs around 26 GJ of energy, compared to at least 24 GJ (estimated) in bacteria. This is indeed not the massive reduction we were hoping for. The article concludes that perhaps the true value lies in a better understanding of this process: . “The chemical motivation to study nitrogenase is thus less to produce an energy-efficient replacement of the Haber-Bosch process but rather because it is an interesting system in its own right, and perhaps it may motivate how to understand and design other catalysts that can activate and break the nitrogen-nitrogen triple-triple bond under ambient conditions.” . As a final note, we want to stress that quantum computers do not magically spit out recipes for fertilizers, nor for medicines, batteries, or catalysts. For real breakthroughs, we need a team of chemists, engineers, and other experts, who spend several years to run experiments, have discussions, employ computer simulations, make mistakes, go back to the drawing board a few times, and slowly converge to practical solutions. We should not forget that quantum computers merely provide a new set of tools. The best we can hope for is that smart people will use them in the right way! . Further reading: . | (Scientific article) Toward the first quantum simulation with quantum speedup https://www.pnas.org/doi/10.1073/pnas.1801723115 . | (2022 review article) Prospects of quantum computing for molecular sciences https://link.springer.com/article/10.1186/s41313-021-00039-z . | (2019 review article) Quantum Chemistry in the Age of Quantum Computing https://pubs.acs.org/doi/10.1021/acs.chemrev.8b00803 . | . | Feynman, R.P. Simulating physics with computers. Int J Theor Phys 21, 467–488 (1982). https://doi.org/10.1007/BF02650179 &#8617; . | Lee, S., Lee, J., Zhai, H. et al. Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistry. Nat Commun 14, 1952 (2023). https://doi.org/10.1038/s41467-023-37587-6 &#8617; . | Santagati, R., Aspuru-Guzik, A., Babbush, R. et al. Drug design on quantum computers. Nat. Phys. 20, 549–557 (2024). https://doi.org/10.1038/s41567-024-02411-5 &#8617; . | Daley, A.J., Bloch, I., Kokail, C. et al. Practical quantum advantage in quantum simulation. Nature 607, 667–676 (2022). https://doi.org/10.1038/s41586-022-04940-6 &#8617; . | Garnet Kin-Lic Chan, Quantum chemistry, classical heuristics, and quantum advantage (preprint); https://arxiv.org/abs/2407.11235 &#8617; . | Von Burg et al., Quantum computing enhanced computational catalysis, Phys. Rev. Research 3, 033055. https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.033055 &#8617; . | “Current classical quantum-chemistry algorithms fail to describe quantum systems accurately and efficiently enough to be of practical use for drug design.”, Santagati, R., Aspuru-Guzik, A., Babbush, R. et al. Drug design on quantum computers. Nat. Phys. 20, 549–557 (2024). https://doi.org/10.1038/s41567-024-02411-5. &#8617; . | https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/quantum-computing-just-might-save-the-planet &#8617; . | https://www.imperial.ac.uk/grantham/publications/climate-change-faqs/how-and-when-do-we-need-to-act-on-climate-change-/ &#8617; . | Garnet Kin-Lic Chan, Quantum chemistry, classical heuristics, and quantum advantage (preprint); https://arxiv.org/abs/2407.11235 &#8617; . | . ",
    "url": "/part2/chapter_1/#applications-in-chemistry-and-material-science",
    
    "relUrl": "/part2/chapter_1/#applications-in-chemistry-and-material-science"
  },"26": {
    "doc": "1 Applications in chemistry and material science",
    "title": "1 Applications in chemistry and material science",
    "content": " ",
    "url": "/part2/chapter_1/",
    
    "relUrl": "/part2/chapter_1/"
  },"27": {
    "doc": "2 Applications of quantum networks",
    "title": "Applications of quantum networks",
    "content": "If we’re building computers that deal with qubits, superposition and entanglement, wouldn’t these computers also need some way to send qubits to each other? This is the dream of the quantum internet: a network that exchanges quantum-mechanical photons between devices all around the world, parallel to our well-known ‘classical’ internet.  . There is a bit of a paradox here. On the one hand, a full-blown quantum internet is very, very far away: a network that reliably transports actual qubits is arguably harder to realize than a large quantum computer, as it will build upon the error correction technology that we’re only just figuring out. On the other hand, it is often said that ‘quantum networks’ have a higher Technology Readiness Level than computing. That sounds like a contradiction, right? . The main reason is that there are some applications for ‘imperfect’ quantum networks, in particular in the context of cryptography.  . In that sense, quantum networking applications have always been ‘ahead’ of quantum computing. Already in 1984, long before quantum computers were seriously considered, researchers Benett and Brassard discovered a method to securely negotiate a secret key (think of a password) between two parties, based on sending individual photons. Their result is now famously known as the BB’84 protocol. Similarly, the commercialization of network technologies has long been ahead of computing. Early quantum startups like MagiQ Technologies and ID Quantique were founded around the new millennium, and brought their first commercial networking products to the market in 2003 and 2004. This technology, using a quantum network to establish a secret key between two endpoints, is called Quantum Key Distribution (QKD) – an application that we will address in much more detail below. The promises of the quantum internet . There is a long list of arguments why we should be excited about the quantum internet. Here is a list of the applications that we hear most frequently: . | Clustering quantum computers: By connecting multiple smaller computers, one might build a much larger computer which has more combined memory and can tackle larger problems.  . | Securing your classical communication. The main contender here is Quantum Key Distribution (QKD), sometimes dubbed the “unhackable” network. This allows two distant users to create a secret key (think of a password) that they can later use for further cryptographic applications. | . | “Blind computing”: Encrypting your data while still allowing someone else to process it. What if you hire an Amazon cloud computer to do calculations on your data, but you don’t want Amazon to actually see your data? It turns out that you can make quantum computers do their computations even while keeping data encrypted, with some caveats. Similarly, one could use ‘secret’ software to solve someone else’s problem without them discovering this algorithm. Such applications often go by the name of blind computing or private computing.  . | A scientific (hard!) overview of blind computing applications.  | . | . | Position verification: Can you prove that you are currently at a given location, in a way that cannot be spoofed?  | . | A short introductory video [3:23] | . | Complete protocols that require input from multiple parties, such as leader election or Byzantine agreement. You can find many more in the Quantum Protocol Zoo.  | . | Make quantum sensors more effective. There exist proposals to combine different telescopes or gravitational wave detectors, and plans to synchronize quantum clocks.  . | A scientific (hard!) overview of distributed quantum sensing | . | . Much more about the various applications can be found in this online Quantum Internet magazine by TU Delft, or as listed by the Quantum Internet Alliance. How useful is the quantum internet in practice?  . Admittedly, many applications of the quantum internet will depend on how much we will use quantum computers. If quantum computers become widespread in the future, then communication between them also seems to be extremely worthwhile. On the other hand, our current outlook of quantum computers focuses on special-purpose devices that are used to solve isolated problems. It is not immediately clear why they would benefit from a quantum internet. There is a clear road map to build a reliable quantum internet in the future (involving fascinating tricks like entanglement distillation and teleportation), but this would require multiple error-corrected quantum computers by itself! For that reason, in this guide, we’re not yet looking ahead at applications like clustering computers, multi-party computations, private computing, or making sensors more effective.  Regarding clustered quantum computers, we frequently hear arguments that we can make a ‘bigger’ quantum computer by connecting individual ones. I’m always wondering why those computers are not just simply built next to each other. Connecting those is much, much easier than connecting them over a quantum internet.  . In the foreseeable future, the first interesting applications are those that work over a “noisy” connection, and transport just one qubit at a time (or perhaps a handful of them). For practical interest, Quantum Key Distribution (QKD) is by far the most interesting application.   . The case for QKD . To fully understand QKD, we will need to have a bit more background about cryptography, especially the “key distribution problem”. For a full account, we recommend first reading the chapter on cryptography. In short: we’re wondering how Alice can agree on a secret key with her distant friend Bob, in a world where everyone can read plain data sent over the internet. Surely they can’t just generate a random key and send it over to each other, without having any encryption in the first place! This problem is commonly solved using public key cryptography (which we know will be revamped in the following years). If you really don’t trust public-key cryptography, the main alternative is to physically transport a usb stick by a trusted courier.  . Compared to conventional cryptography, the unique selling point of QKD is that it is fundamentally impossible for cybercriminals to obtain the secret key as it is being distributed. As long as our understanding of quantum mechanics is correct (arguably the most well-test theory in science), no amount of computational power or mathematical breakthroughs will let an attacker gain information about the key. Of course, this assumes that the protocol is executed precisely as prescribed, and there are no other vulnerabilities in the actual hardware or software used.  . This is fundamentally different from today’s approach of public key cryptography, which must rely on certain mathematical assumptions. We know for sure that, with sufficient computational power, these codes can be broken, but we argue that this takes such an incredibly long time that nobody will bother (except for bragging rights and prize money). Still, such statements about computation times are based on assumptions, and our trust is purely based on our experience that no brilliant cryptanalyst has broken it yet. In fact, well-regarded cryptosystems do get broken from time to time, such as SIKE, which was in the race to become a new NIST standard.  . That said, although QKD is “unhackable” in theory, the actual hardware and software are equally likely to contain vulnerabilities. Contrary to well-trusted public-key cryptography, no QKD system has received proper certification and accreditation, and a significant fraction of historical products have been hacked.  . QKD requires specialised hardware — although it is much less demanding than other quantum internet applications we mentioned. It can already prove useful on a basic point-to-point network with just two connected parties, of which one should send photons, and the other receive them. This is orders of magnitude less complex than building a fully-featured internet. Moreover, the qubits need only be sent and measured one at a time: no quantum memory or extensive computation are needed. There have already been several demonstrations that use standard telecom fiber (the stuff that’s already in the ground), or satellite-based systems that communicate through air. QKD hardware is fancy and expensive, but not completely out of reach.  . The major downside of QKD is that it has no way to confirm who the person on the other end of the line is. Some form of authentication is still needed – which is mostly done with secret keys that should already be present in the first place! This makes QKD just a partial solution to the key distribution problem.  .   . Further reading . | A video explanation of QKD for laymen or experts. | Companies like Toshiba and ID Quantique offer commercial QKD systems for distances of around 100 km.  . | Chinese scientists achieve QKD through satellites over 1000 km.  . | Nature commentary why practical long-range QKD is still out of reach.  . | . What do experts say?  . Cryptographers that have been working on securing classical computers are typically sceptical about QKD. In fact, all security authorities that we are aware of will advise against the use of QKD at this current point in time. They find the use of additional, uncertified hardware too large of a security risk, and stress that there is a better solution that works on conventional computers: post-quantum cryptography (PQC). From their perspective, PQC offers all the required functionalities, and is currently more practical to test, certify and implement.  . Be careful not to confuse the abbreviations PQC and QKD. QKD is the stuff that requires a quantum network. PQC is the stuff that runs on classical computers.  . A discussion between physicists and cryptographers. A fair argument in favor of QKD, stems from the ‘harvest now, decrypt later’ attacks that could be done over today’s internet. These would imply that even the privacy of today’s messages is compromised. This could be a convincing reason for organizations to rapidly jump into a first QKD testbed. Still, for those who can risk the expenses, it might be more worthwhile to look at more mature and readily available solutions. For example, there exist certified solutions that rely on symmetric encryption with trusted couriers.  . See also: . | Compumatica offers symmetric encryption with keys transported on SD-cards.   . | TNO designed a ‘quantum-safe proxy’ as add-on to existing cryptography.  . | StackOverflow question: “Why does the NSA find QKD impractical”? . | .   . What’s left is a very niche use-case for the most forward-thinking organizations that deal with extreme security requirements. It is somewhat of a pity that QKD is not so mature yet today, now that many organizations will start their security migrations. A widespread adoption of QKD would make it easier to expand to a large-scale quantum internet in the future. Nevertheless, since a quantum threat could be here as soon as the early 2030s, most companies are recommended to urgently migrate to post-quantum cryptography (PQC) first, and potentially consider QKD as an add-on for additional security later, if needed.  .   . Conclusion . In conclusion, most applications of a quantum internet will not be immediately relevant in the foreseeable future, with an exception for QKD. And even QKD might not be the killer applications that many investors are hoping for – it most definitely shouldn’t be called “unhackable”.  . Still, it seems unfair to us to dismiss a quantum internet because it would be ‘too technologically challenging’ or ‘too expensive’. These arguments are correct today, but perhaps naive on a scale of several decades. Would anyone from the 70’s have believed that today, more than half of the world population is streaming videos on a mobile phone for just a few dollars per month? Who knows what the quantum internet will look like 50 years from now.  . ",
    "url": "/part2/chapter_2/#applications-of-quantum-networks",
    
    "relUrl": "/part2/chapter_2/#applications-of-quantum-networks"
  },"28": {
    "doc": "2 Applications of quantum networks",
    "title": "2 Applications of quantum networks",
    "content": " ",
    "url": "/part2/chapter_2/",
    
    "relUrl": "/part2/chapter_2/"
  },"29": {
    "doc": "3 The impact on cybersecurity",
    "title": "The impact on cybersecurity",
    "content": "In the world of quantum computers, the most convincing exponential speedup lies in codebreaking. Anyone who wants to understand the impact of quantum computers, will need to know their basics of cryptography. Let’s start at the beginning.  . Cryptography is much more than just secrecy  . Why do we actually use cryptography? Pretty much everyone will immediately think of: . | Privacy / confidentiality: making sure that others cannot read your data (especially when messages are sent over a network). | . However, there are many more threats that cryptography protects us from. Most people wouldn’t normally worry about them, but when any of the following is missing, cybercriminals can cause a lot of harm:  . | Authentication: You want to verify that a message really came from the entity that claims to send the message. For example, during online banking, you want to be 100% sure that you are communicating with your bank and nobody else. Another example is when installing a new piece of software. When executing the latest Windows update, your computer makes sure to check that there is a ‘digital signature’ that belongs to Microsoft. Imagine how unsafe your computer would be if anyone could send fake updates! | . | Integrity: You want to verify that nobody changed the message during transit. Imagine what damage could be caused when your emails are maliciously altered before they arrive, or when the commands coming from an air traffic control tower are modified. Similarly, any software installer confirms that the software wasn’t changed by anyone but the original publisher, by verifying a digital signature. | . | Establishing secret keys: How do you negotiate a new secret key with a brand new webshop that you never visited before? This is a seemingly impossible task if bare internet traffic can be read by anyone, but modern cryptography has a solution. | . There are some others, like non-repudiation and availability, that we don’t discuss here. Remember the above bold-faced words, as we will come across them a lot more.  . We hope that this introduction makes the reader aware of the enormous importance of proper cryptography, and the sheer number of cryptographic checks that are required for proper functioning of our IT. You would be surprised how often you use cryptography on a daily basis, through your laptop, phone, car keys, or smart cards. The quantum threat is mainly to public-key cryptography.  . A common misconception, which we see a lot in popular literature, is that the quantum threat can be summarised as follows. (Both of the statements below are incorrect!)  . | A quantum computer will break all of today’s cryptography.  . | A quantum internet is needed to keep our cryptography safe again. | .   . To better understand this, let’s first look at what cryptography a quantum computer will break, and which it won’t. Later, we will look at the necessity of a quantum internet.  .   . In line with common cryptography jargon, we will have two parties, Alice and Bob, who want to communicate with each other. We distinguish two different types of cryptography: the symmetric and the asymmetric (public key) variants.  . In symmetric (or private key) cryptography, we assume that both Alice and Bob already know some secret key. This could be a password that they both know, or more commonly, a very long number represented by (say) 128 bits in their computer memory. Alice can use the key to encrypt any message using a protocol like AES. Bob can then use the same key to decrypt this message. The details of how encryption and decryption work are unimportant for our purposes. The only thing that’s relevant is that our computers can do this very efficiently, and that it’s considered extremely safe (without the key, nobody could reasonably break this encryption).  . In asymmetric cryptography, or more often called public-key cryptography (PKC), each participant has two keys: a public key and a private key. The public key can be shared with anyone, while the private key must be kept secret. That’s why we use the suggestive colours green (save to share) and red (be careful!). If Alice wants to send an encrypted message to Bob, she uses Bob’s public key to encrypt the message. If Bob wants to decrypt the message, he uses his private key.  . The setting with two keys offers more functionality. For example, using the previous encryption method, Alice could send a secret key to Bob, which they can then use for symmetric cryptography (which is often a lot faster). Furthermore, the protocol works in ‘reverse’. Alice can use her private key to encrypt a message, which then anyone in the world (including Bob) can decrypt using the corresponding public key. Bob should then be confident that Alice is the only person in the world who could have encrypted this message (indeed, something encrypted with the private key can only be decrypted with the public key, and vice versa). This forms the basis of digital signatures.   .   . This is precisely what’s used whenever you open a webpage. Your browser (here: Chrome) will display that the connection is secure, which means that (amongst others) it verified that the digital signature is valid. This ensures authenticity and integrity.  . It should come somewhat as a surprise that public-key cryptography is even possible at all! It’s kind of a small wonder that encryption and decryption with two totally different keys can be made to work, thanks to some powerful mathematics. (I don’t know of any physical locks that work this way). However, it turns out that the delicate relationship between the two keys is also a weak spot… . How good are quantum computers at cracking cryptography?  . In principle, symmetric-key cryptography is fairly safe against quantum hackers. The biggest problems are brute-force attacks, where an attacker effectively tries every possible secret key. Using a key of 128 bits, the total number of possible keys is 2128 — that’s an incomprehensibly large number (much more than the number of atoms in a human).  . We know that Grover’s algorithm speeds up brute-force search, by reducing the number of attempts from 2128 to its square root, which is 264. This is something that cryptographers are not happy about, but considering the slowness and extra overhead that comes with quantum computers, this doesn’t seem to be a problem in the foreseeable future. Still, to be on the safe side, it is recommended to double key lengths, hence to use the same algorithm with 256-bit keys. Changing this in existing IT infrastructure is relatively straightforward, although one shouldn’t underestimate the time and costs for such changes within large organisations. The situation is completely different with public-key cryptography. The most-used algorithms today, RSA and ECC, can be straightforwardly broken by a large quantum computer. We discussed the details of  Shor’s algorithm earlier. (To be precise: today’s best results estimate that 20 million qubits and around 8 hours are needed). Luckily, there exist PKC systems that are believed to be safe against quantum computers, and an obvious way forward is to start using these. We call such systems post-quantum cryptography, and despite the confusing name, they’re built to work on conventional computers. We discuss the rabbit hole of migrating to new cryptography in a different chapter, and similarly for estimating how many years we still have until a quantum computer can attack RSA and ECC.  . There is another threat that everyone should be aware of, called harvest now, decrypt later. Encrypted messages that are sent over a network today can be intercepted and stored for many years, until a quantum computer can efficiently decrypt the messages. In practice, we use public-key encryption mainly to establish temporary secret keys to be used with symmetric-key cryptography, but even these can, in principle, be found retroactively. In other words: the confidentiality of today’s communication is already threatened!  . Further reading: . | Redhat blog on Mosca’s theorem, which states when you should start upgrading if your communication needs to remain secret for at least X years. | .   . The following table summarizes how our cryptosystems are threatened: . Figure 3: News header by IFLScience. Source: https://www.iflscience.com/chinese-scientists-create-quantum-processor-60000-times-faster-than-current-supercomputers-61475 . |   | Symmetric | Public-key | . |   | Today (AES, … ) | Today (RSA, ECC) | PQC                       | . | Against classical computers | Safe | Safe | Safe | . | Against quantum computers | Safe* . *with double key lengths . | Unsafe | Safe | . Figure 3: News header by IFLScience. Source: https://www.iflscience.com/chinese-scientists-create-quantum-processor-60000-times-faster-than-current-supercomputers-61475 . ** Why don’t we switch to symmetric cryptography? ** . Public-key cryptography solves a very fundamental problem: how can Alice and Bob agree on a secret key before they have a means of encryption in the first place? They cannot just send a new key over the internet without any form of encryption, because anyone would be able to read this. This is the fundamental problem of key distribution. Let us look at the functionality offered by the two types of cryptography:  . |   | Symmetric | Public-key                    | . | Confidentiality (privacy) | Only with pre-shared keys | ✔ | . | Authentication / Integrity   | Only with pre-shared keys | ✔ | . | Establishing secret keys | ✗ | ✔ | . Figure 4: Aerial view of the strait of Gibraltar. Image source: https://commons.wikimedia.org/wiki/File:STS059-238-074_Strait_of_Gibraltar.jpg . If only we could somehow give Alice and Bob pre-shared keys in a secure way, we would resolve most of these problems. Without public-key cryptography, there are other options: . | Alice and Bob could meet every other week to exchange USB drives with secret codes. | Alice and Bob could both trust a large “key server”. If both share a secret key with the key server, they can securely ask the server to generate a new secret key that they can use together.  . | . Still, none of these form an attractive alternative to public-key cryptography, especially if one considers the sheer number integrity and authenticity checks we perform every day, and the incredible number of online entities we potentially want to communicate with.  . What solutions exist? . TODO: More on PQC, hopefully when new standards are released! . What about Quantum Key Distribution (QKD)? . As we saw in a different chapter, Quantum Key Distribution partially solves the key distribution problem. It requires a quantum network connection between Alice and Bob, and somewhat expensive quantum devices to generate and measure photons. When used, Alice and Bob will obtain a secret key (that can be as long as they like), in a very safe way (in the sense that nobody listening in on their classical or quantum communication can possibly find this key). Unlike PKC, this method still requires pre-shared keys for authentication (because otherwise you can’t be sure with whom you will share your new secret key). Therefore, it won’t completely solve the key distribution problem.  . There is even more reason to be careful: many security authorities warn against adopting QKD today. Although the theory is very sound, today’s hardware is still in an early stage. The time to generate secret keys is still relatively long, and it is very likely that the actual software and hardware contain mistakes that make them vulnerable to attacks.  . It is somewhat of a pity that QKD is not so mature yet, because it would be a viable weapon against Harvest Now, Decrypt Later (since harvesting the pre-shared authentication keys is not a problem). Moreover, widespread adoption of QKD would make it easier to expand to a large-scale quantum internet. Nevertheless, since a quantum threat could be here as soon as the early 2030s, most companies are recommended to urgently fix their post-quantum cryptography (PQC) first, and potentially consider QKD as an add-on for additional security later, if needed.  .   . What about Quantum Random Number Generators (QRNG)? . Good random number generators are extremely important in cryptography, and QRNGs could provide a good alternative to the hardware random number generators that are widely used today.  . However, all they do is generate random numbers – that doesn’t make any protocol safe in itself. As a general warning: **products with ‘quantum’ in the name do not automatically protect against Shor’s algorithm!  ** . What steps should a typical company or government take?  . We dedicate a separate chapter to that!  . Conclusion . It should be clear that cryptography is strongly intertwined with quantum computing, through Grover’s algorithm, Shor’s algorithm, and Quantum Key Distribution. Nevertheless, security experts recommend that there is an obvious way forward: . | Replace current public-key cryptography with new, quantum-safe protocols (PQC). | Double key lengths in symmetric cryptography.  . | . Especially the first bullet is a major challenge. There are many legacy systems around on the internet that may not be updated so easily. There are billions of devices that are all interconnected, so updating one device will surely cause incompatibilities somewhere else. What’s more, PQC protocols will surely require more CPU power and more memory than today’s trusted methods. Companies may need to update the core code of hundreds or even thousands of applications. And lastly, the new protocols haven’t been tested as extensively as our conventional methods, so it is not unlikely that new security issues will be found. Quantum computers, before they are even built, are already destined to make the next decade an incredibly complex period for anyone who deals with cryptography!  . ",
    "url": "/part2/chapter_3/#the-impact-on-cybersecurity",
    
    "relUrl": "/part2/chapter_3/#the-impact-on-cybersecurity"
  },"30": {
    "doc": "3 The impact on cybersecurity",
    "title": "3 The impact on cybersecurity",
    "content": " ",
    "url": "/part2/chapter_3/",
    
    "relUrl": "/part2/chapter_3/"
  },"31": {
    "doc": "4 Applications in optimisation and machine learning",
    "title": "Applications in optimisation and machine learning",
    "content": "(Please check back later!) . ",
    "url": "/part2/chapter_4/#applications-in-optimisation-and-machine-learning",
    
    "relUrl": "/part2/chapter_4/#applications-in-optimisation-and-machine-learning"
  },"32": {
    "doc": "4 Applications in optimisation and machine learning",
    "title": "4 Applications in optimisation and machine learning",
    "content": " ",
    "url": "/part2/chapter_4/",
    
    "relUrl": "/part2/chapter_4/"
  },"33": {
    "doc": "1 Quantum hardware",
    "title": "Quantum hardware",
    "content": "Conventional computer hardware is extremely well standardized. No matter what supplier you buy a computer from, you can be reasonably sure that you can run your favourite applications on them. We expect to have professional servers to run non-stop for years without the hardware ever failing. Thanks to such high reliability and clear compatibility, it is rather easy to compare different machines, by looking at speed (e.g. floating-point operations per second, FLOPS) and memory size. We will see that this is radically different for quantum computers: devices make mistakes, have limited functionalities, and depending on your application, you might like to use a completely different architecture. In this chapter, we take a high-level business perspective at quantum computing hardware. What should you know when starting your quantum journey?  . Different functionalities . The figure below shows different types of functionalities that quantum computers can have (top, brown), along with some examples of hardware that is available (below, yellow). This list is by no means complete! It should at best give an impression of the current state of the art. Let us start by taking a closer look at the functionalities. Our biggest dream is to have a ‘universal quantum computer’. The word ‘universal’ indicates that it is capable of executing any quantum algorithm (or technically: to approximate any algorithm’s output to arbitrary precision). For comparison: your laptop, phone, and even a smartwatch are universal classical computers, making them capable of running any classical application you can think of: spreadsheets, 3D games, data encryption, and so on. Similarly, a proper universal quantum computer is suitable for any quantum application, regardless of whether it is known today or invented in the future.  . The definition of ‘universal’ is blind to some details such as memory limitations (it assumes you will never run out of RAM), and omits tedious details about software compatibility (a Playstation game won’t run on an XBox). In our high level overview, such details are unimportant: the main point is that there also exist devices that can not run just any algorithm. Does a universal computer need to be “gate-based”? . No matter what architecture or qubit type you pick, our current technology will not allow you to run very long computations. This is due to the inherent imperfections in construction and control of quantum devices. The imperfections cause errors to accumulate during a computation, so that after some number of steps, the result is almost surely corrupted and unusable. For longer computations, it is essential to fix errors on the fly, using so-called error correction (also known as: fault-tolerance).   . Today, we are stuck in a so-called NISQ-era, with Noisy Intermediate-Scale Quantum devices. Many are in principle universal, except that they are limited both in the number of qubits, and in the number of steps that can be executed. Companies like IBM, IonQ, Quantinuum and Pasqal all have NISQ computers available to test your own programs on.  . Making a universal computer is challenging, and engineers can make special-purpose devices that improve in certain areas (like number of qubits or clock speed) by omitting certain functionalities. In a Quantum Simulator, the computer specialises in simulating a certain class of materials or molecules. The precise capabilities are often captured in a Hamiltonian that specifies which materials qualify. As an example, Harvard-spinoff QuEra has a quantum simulator available over the cloud that mimics a quantum Ising model. Today’s simulators (like QuEra’s) are fairly close to a universal computer (lacking only a few technical ingredients) and are similarly subject to accumulating errors. However, they are not designed to run conventional (gate-based) algorithms. There might be some confusion in jargon here, as the term ‘quantum simulation’ is also sometimes used when a classical computer tries to mimic the behaviour of a quantum computer. Others use the term ‘emulation’ for such classical approaches.  . See also: . | QuEra announces a 256 qubit simulator available over the Cloud.  . | Pasqal performs a material science simulation with 196 qubits and sells 100-qubit simulator in a EuroHPC tender. | . Another special-purpose device is the Quantum Annealer, for which the Canadian scale-up D-Wave is especially well known. These special-purpose devices can solve a certain class of optimization problems that goes by the name of QUBO: quadratic unconstrained binary optimization. There is a well-developed theory of mapping various problems into the QUBO formalism, making annealers fairly versatile machines. However, quantum annealers will never be able to take advantage of the various other quantum algorithms out there: even with enough qubits, we don’t see them cracking codes with Shor’s algorithm.  . See also: . | D-Wave’s introduction to its quantum annealing platform | . Different building blocks . There is another question about what material the qubits are actually made of. Scientists have cooked up several competing approaches, such as superconducting materials, photons, or individual atoms, or ions, each with their own strong and weak spots. The methodology or material used to make a physical qubit is often called the qubit implementation, the qubit type, or (we prefer) qubit platform.  . For conventional computer electronics, we converged to a single choice of material and broadly a single manufacturing process, based on silicon wafers and lithography. For quantum computers, there is still a fierce race between the different platforms, and it is totally unclear which will eventually be the winner — or whether we will converge to a single winner at all.  . There is an interesting story behind the different hardware types, but we won’t delve into that in this non-technical guide (would you otherwise care what material your classical CPU is made of?). However, anyone who wants to test quantum programs on actual hardware should definitely know the details. Interested readers can start here: . Further reading: . | Different types of qubits explained by Sifted.eu . | Different types of qubits at IQC Waterloo . | Different types of qubits on Wikipedia . | A MOOC about different hardware types by TU Delft . | . It is interesting to note that all these different machines (universal, annealers, simulators) can in principle be built using any type of qubit. If you go back to the figure at the top, you can see that there exist different qubit types with different functionalities. It’s not unthinkable that the empty squares will also be filled, so that we have access to superconducting-based simulators, or annealers that use ultracold atom qubits (or perhaps the authors have missed this!). In fact, we have already seen many academic showcases of superconducting simulators in academic literature. TODO: ADD OVERVIEW TABLE OF WHAT SUPPLIERS HAVE. ",
    "url": "/part3/chapter_1/#quantum-hardware",
    
    "relUrl": "/part3/chapter_1/#quantum-hardware"
  },"34": {
    "doc": "1 Quantum hardware",
    "title": "1 Quantum hardware",
    "content": " ",
    "url": "/part3/chapter_1/",
    
    "relUrl": "/part3/chapter_1/"
  },"35": {
    "doc": "2 Error correction",
    "title": "Error correction",
    "content": "At a glance . | To run long computations, we need to dramatically reduce the likelihood of error of each elementary step – not just a little bit, but by a factor of millions.  . | Error correction is the most effective method to achieve extremely low error probabilities. It combines a small number of ‘physical’ qubits (think of several hundreds) into a single ‘logical’ qubit that suppresses errors exponentially.  . | Logical qubits are still not perfect qubits: the ‘number of steps’ that they can survive is an important specification that determines whether they can run your application. | . Around 2024, we’re seeing a major shift in the road maps of quantum computer manufacturers. Several companies no longer put their bare qubits in the spotlight, but instead focus logical qubits. Error correction seems to be an essential component of large-scale quantum computing, adding yet another facet in which these devices differ from their classical counterparts. As with many aspects of quantum computing, error correction can be rather confusing. A statement that we often hear is the following (which is incorrect!) . “Logical qubits (or: error-corrected qubits) are resilient to errors that occur during a computation. Once we have logical qubits, we can increase the length of our computations indefinitely. “ . What’s the problem here? Well, not every logical qubit is created equally. We expect to soon see logical qubits that are perhaps 2x more accurate than today’s bare hardware qubits, and later 10x, and in the future perhaps 1000x. Error correction is a trick to reduce the probability of errors, but it will not eliminate errors completely. In the following decade, we expect to see gradual improvements, hopefully down to error rates of 10-10 and below. What is error correction? . In quantum error correction, we combine some number (think of hundreds or thousands) of ‘physical’ hardware qubits into a virtual ‘logical’ qubit. The logical qubits are the information carriers used in an algorithm or application. Error correction methods can detect whenever tiny errors occur in the logical qubit, which can then be ‘repaired’ with straightforward operations. Under the assumption that the probability of hardware errors is sufficiently low (below a certain error threshold), the overall accuracy improves exponentially when we combine more physical qubits per logical qubit. Hence, we obtain a very favourable trade-off between the number of qubits, and the accuracy of the qubits. Doesn’t measuring a quantum state destroy the information in the qubits? . Indeed, if we naively measure all the physical qubits, then that would destroy the computation. However, quantum error correction uses an ingenious way to measure only whether or not an error occurred. It learns nothing about the actual information content of the qubit. It turns out that this way, the data stored in the logical qubit is not affected.  . Why are errors so much of a problem? How do errors screw up our computations?  . In short: even tiny errors are a problem because we want to perform incredible numbers of quantum operations — think of billions or trillions of them.  . Let’s make this more concrete. A computer program is essentially a sequence of ‘steps’, each of which a computer knows how to perform. We say that a program or algorithm has a width, which is the number of qubits it requires. It also has a depth, which is the number of consecutive steps that need to be performed. In early hardware, you may interpret one step as a single quantum gate.  . The concept of  ‘width’ is pretty straightforward: if the computer doesn’t have enough memory, it will not be able to run the program. Dealing with ‘depths’ is harder. According to the laws of statistics, to run a program of 109 steps, we need to limit errors to roughly the inverse: say, 10-9 per step. If the error is larger, it becomes extremely unlikely to find a correct outcome of the computation. These are not hard numbers: a computer with 10-10 error would be a significant improvement (resulting in much fewer mistakes), and a computer with 10-8 error might be pushed to also find the correct answer after many tries. However, as the imbalance between depth and error grows, the probability to find a correct outcome is reduced exponentially. We illustrate this in more detail in the box below.  . To illustrate, why do we need such small error rates?** Let’s look at a very simple model of a computer, which is not unlike what happens inside a quantum computer or a modern (classical) CPU. As above, the computer is supposed to work through a list of instructions. We can consider various specifications of a computer: .   . | The available memory, measured in bits (or perhaps megabytes or gigabytes, if you like).  | . | The speed at which the computer operates, measured in steps per second.  . | The “probability of error”, the chance for each computational step to introduce some mistake. This is given as a number between 0 and 1 (or a percentage between 0 and 100%). Many sources use the word ‘fidelity’ instead, which can be roughly interpreted as the opposite (fidelity ≈ 1 – probability of error). In this text, we sometimes just say “error”. | .   . In this simple model, the time taken to complete the computation equals: “depth” x “speed”. You can make a computer program faster by increasing the speed of the computer, or by writing a ‘better’ program that takes fewer steps.  . The influence of errors is harder to track. For contemporary computers, we typically don’t worry about hardware mistakes at all: every step has essentially 100% certainty to output the correct result. Unfortunately, in our little model, this is not the case.  . Assume that each step has a 1% (= 10-2) chance of error. What will the impact on the final computation be? Here, we compute the chance to finish the computation without any errors, for various numbers of total steps:  . | Error probability: 1% |   | . | Number of steps | P(success)   | . | 1  | ( 0.99 )1 = 99% | . | 100 | ( 0.99 )100 = 37% | . | 1000 | ( 0.99 )1000 = 0.004 % | . | 10,000 | ( 0.99 )10,000 =   10-44  | . Figure 5. The number of qubits in the most mature quantum computers from a selection of different manufacturers, as of 2024. In this simple model, we assume that any error is catastrophic. This is quite accurate for most programs. You might argue that there is a miniscule chance that two errors cancel, or that the error has very little effect on the final result, but it turns out that such effects are statistically irrelevant in large computations.  . Now, assume we improve our hardware, towards an error rate of 0.1% (=10-3), then we find: . | Error probability: 0.1% |   | . | Number of steps | P(success)   | . | 1  | ( 0.999 )1 = 99.9% | . | 100 | ( 0.999 )10 = 90% | . | 1000 | ( 0.999 )1000 = 37% | . | 10,000 | ( 0.999 )10,000 = 0.004  | . Figure 6: Results of an expert survey by Global Risk Institute (globalriskinstitute.org). A probability to succeed of 37% sounds bad, but for truly high-end computations we might actually be okay with that. If the program results in a recipe for a brand-new medicine, or if it tells us the perfect design for an aeroplane wing, then surely we don’t mind repeating the computation 10 or 100 times, after which we’re very likely to learn this breakthrough result. On the other hand, if the probability of success is 10-44, then we will never find the right result, even if the computer repeats the program billions of times.  . In the table above, we see a pattern: to reasonably perform 102 steps, we require errors of roughly 10-2 or better. To perform 103 steps, we need roughly a 10-3 chance of error. These are very rough order-of-magnitude estimates, but they have a very useful conclusion when dealing with very large circuits (or very small errors): if you want to execute 10n steps, you’d better make sure that your error probability is not much bigger than 10-n.  . This simplified model assumes that an operation either works correctly, or it fails, but nothing in between. In reality, quantum operations act on continuous parameters, and therefore they have an inherent, scalar-value accuracy. For example: a quantum gate might change a parameter from A to A+0.49, where it’s supposed to do A+0.5. For our discussion, this doesn’t really matter — for our qualitative conclusions, it suffices to see a “99% accurate” quantum gate as simply having a 99% chance of succeeding. We also overlook various other technical details, operations carried out in parallel, different types of errors, native gate sets, connectivity, and so forth — these make the story much more complicated, but will not change our qualitative conclusions. **Why don’t we just make the hardware more stable?** To some degree, we can still greatly reduce errors by creating more accurate hardware. However, quantum objects are so incredibly fragile that even getting down to 10-2 errors requires some of the world’s most astonishing engineering. We definitely hope to see two-qubit gate errors reduced to 10-3 and perhaps even 10-4, but achieving targets of 10-9 seems unlikely with incremental hardware engineering alone. On the other hand, quantum error correction is incredibly effective: the error drops dramatically at the cost of adding a modest number of qubits, which are assumed to be scalable anyway. That’s why experts agree that error correction is the right way forward.   . **Do we use error correction in classical computers too?** This might be a good moment to appreciate the incredible perfection of classical computer chips: while doing billions of steps per second, running for months in a row, sometimes with thousands of cores at a time, errors in CPUs practically never occur. I was hoping to find hard numbers on this, but companies like Intel and AMD seem to keep this under stringent non-disclosure agreements. However, some research shows that errors under 10-20 are easily attained as long as we don’t push processors to their limits (in terms of voltages and clock speeds). Memory (RAM) does often come with error correction for high-performance supercomputers, and some form of CPU error correction was sometimes used in older mainframes and (even today) in space probes.  . Longer computations need more qubits . As problems become ‘more complex’, they typically require more from our computers: both in terms of width (number of bits) and depth (number of steps). We could illustrate this as below. You can think of a number “N” as the difficulty or the size of the problem: for example, we might consider the problem of “factoring a number that can be written down using at most N bits”).  . Keep in mind: we’re talking about the requirements to solve a problem here, so that width indicates logical bits. If a computer does not have error correction, then 1 logical bit is simply the same as 1 physical bit – or its quantum equivalent.  . For ‘perfect’ classical computers, the situation is straightforward: if a problem gets bigger, then we need more memory, and we need to wait longer before we obtain the result. For (quantum) computers that make errors, the situation is more complex. With increasing depth, not only do we need to wait longer, we also need to lower the error probabilities, and hence, need more advanced error correction.  . Let’s consider two computers for which we show the width and depth that they can handle (where the available ‘depth’ is essentially 1 / probability of error). On the left is a computer without error correction (hence it has a small, fixed depth). The other is an error-corrected computer that can trade between depth and width (in certain discrete steps).  . The computer without error correction might have enough memory to solve a problem, but often lacks the depth. Even an error-corrected computer might not have a suitable trade-off to solve the hardest problems. Looking at the above example, it seems that both computers can solve the N=10 problem. Here, only the error-corrected computer can solve the N=20 problem, as depicted below. For the N=40 problem, the error-corrected computer might have sufficient depth OR sufficient width, but it doesn’t have both at the same time. Hence, neither computer solves the N=40 problem.  . Towards cracking the N=40 problem, our best bet is to upgrade the error-corrected computer to have more physical qubits. Using error-correction, these can then be traded to achieve sufficient depth (whilst also reserving just enough logical qubits to run the algorithm).  . We have found an interesting conclusion here. Larger problems not only require more memory (to store the calculation), but also more depth, which requires more qubits again! To summarise:  . ‘Harder’ problems -&gt; More depth -&gt; Better error correction -&gt; More physical qubits  . Effectively, once we reach an era of error correction, then increasing the number of physical qubits will still be among the top of our wishlist.  . What is the current state-of-the-art? . This is a more technical section that can be safely skipped. As of 2024, there have been several demonstrations of error correction (and the slightly less demanding cousin: error detection), but these have all been with limited numbers of qubits, and with very limited benefit to depth (if any at all). However, we seem to be at a stage where hardware is sufficiently mature that we can start exploring early error correction.  . Below are the three most popular approaches to error correction. Each of them can be considered a ‘family’ of different methods, based on similar ideas: . | Surface codes . | Color codes . | Low-Density Parity Check (LPDC) codes . | . The surface code (or toric code) has received a lot of scientific attention, as this seems to be on the roadmap of large tech companies like Google and IBM. Their superconducting qubits cannot interact with each other over long distances, and the surface code can deal with this limitation. Many estimates that we use in this Guide (such as the resources required to break RSA or to simulate FeMoco) are based on this code. It has already been tested experimentally on relatively small systems: . | A team from Hefei/Shanghai experiments with a 17-qubit surface code. | Google sees improvements when scaling the surface code from 17 to 49 qubits. | . Color codes are somewhat similar to surface code, but typically lack the property that only neighbouring qubits have to interact. This makes them less interesting for superconducting or spin qubits, but still they appear to work extremely well for trapped ions and ultracold atoms.  . | Startup QuEra demonstrates 48 logical qubits using a color code [Scientific presentation]  . | Already in 2014, an early experiment on a single logical qubit (color code) was performed in Innsbruck. | . LDPC codes are now rapidly gaining attention. They build on a large body of classical knowledge, and could have (theoretically) more favourable scaling properties over the surface code.  . | French startup Alice &amp; Bob are aiming for a unique combination of ‘cat qubits’ together with LDPC codes, which can theoretically match very elegantly.  | . Which code will eventually become the standard (if any), is still completely open.   . What are the main challenges? . Firstly, we would need just slightly more accurate hardware. We mentioned a certain accuracy threshold earlier: state-of-the-art hardware seems to be close to this threshold, but not comfortably over it. Secondly, error correction also requires significant classical computing power, which needs to solve a fairly complex ‘decoding’ problem within extremely small time bounds (within just a few clock cycles of a modern CPU). Classical decoding needs to become more mature, both at the hardware and the software level. It is not unlikely that purpose-built hardware will need to be developed, which for some platforms might be placed inside a cryogenic environment (placing stringent bounds on heat dissipation). Theoretical breakthroughs can still reduce the requirements of classical processing.  . Lastly, it turns out that ‘mid-circuit measurements’ are technically challenging: most experiments so far relied on destructively measuring all qubits at the end of an experiment. Without intermediate measurements, one might retroactively detect errors, but one cannot repair them. We should also warn that many related terms such as “error mitigation” and “error suppression” exist. They might be useful for incremental fidelity improvements, but they don’t facilitate an exponential increase in depth like proper error correction does.  . Conclusion . The bottom line is that one shouldn’t blindly take ‘logical qubits’ as perfect building blocks that will run indefinitely. A logical qubit is no guarantee that a computer has any capabilities, it merely indicates that some kind of error correction is applied (and it doesn’t say anything about whether this error correction works well at all). A much more interesting metric is the probability of error in a single step (in jargon: the fidelity of an operation), which gives a reasonable indication of the number of steps that a device can handle! . See also: . | The Quantum Threat Timeline Report asked several experts what they find the most likely approach to fault tolerance (section 4.5).  . | British startup Riverlane builts a hardware chip that ‘decodes’ which error occurred on logical qubits. (Technical report).  . | Craig Gidney (Google) has a more technical blog post on why adding physical qubits will remain relevant in the following decades.  . | [Technical!] Some scientific work speaks of ‘early fault-tolerant’ quantum computing, such as: . | “Early Fault-Tolerant Quantum Computing”, discussing how we can squeeze as much as possible out of limited devices. | “Assessing the Benefits and Risks of Quantum Computers” takes a similar width x depth approach as we do here, but uses it to assess what applications will be within reach first. | . | . ",
    "url": "/part3/chapter_2/#error-correction",
    
    "relUrl": "/part3/chapter_2/#error-correction"
  },"36": {
    "doc": "2 Error correction",
    "title": "2 Error correction",
    "content": " ",
    "url": "/part3/chapter_2/",
    
    "relUrl": "/part3/chapter_2/"
  },"37": {
    "doc": "3 How to read hyped reports",
    "title": "How to read hyped reports",
    "content": "TODO still write this article? . ",
    "url": "/part3/chapter_3/#how-to-read-hyped-reports",
    
    "relUrl": "/part3/chapter_3/#how-to-read-hyped-reports"
  },"38": {
    "doc": "3 How to read hyped reports",
    "title": "3 How to read hyped reports",
    "content": " ",
    "url": "/part3/chapter_3/",
    
    "relUrl": "/part3/chapter_3/"
  },"39": {
    "doc": "4 What steps should your organization take?",
    "title": "What steps should your organization take?",
    "content": "TODO: add . https://arxiv.org/abs/2310.15505 The Quantum Tortoise and the Classical Hare: A simple framework for understanding which problems quantum computing will accelerate (and which it will not) . In the previous parts, we discussed the use-cases, the threats, and the timelines of quantum technologies. We will now take a more strategic perspective: what concrete steps can we take today? How does one establish a successful road map? Details will greatly differ per organisation and per sector, but as we are in an early phase, we expect that most enterprises will take surprisingly similar approaches during the next couple of years.  . Several organisations (especially consultants that would love to support you) have extensive writings on how to get started. Some examples: . | Capgemini – Quantum technologies: How to prepare your organization for a quantum advantage now . | McKinsey – Quantum computing use cases are getting real—what you need to know . | BCG – Quantum Computing Is Becoming Business Ready  . | . We find most of these somewhat ‘hypey’, with a strong emphasis on the risks of missing out and the need to take actions quickly. However, they all mention reasonable strategic steps that organisation can take, which we will lay out below. Let’s break them up into 3 different stages: . 1. Start with no-regret moves . Most companies start with early steps aimed at better understanding the situation. These can be done with very little financial risk. Some must-do actions: . | Appoint a quantum lead, or a quantum working group . | Read up and learn. If you’ve come this far in this Guide, you’re already doing a fantastic job.  . | Create internal awareness . | . Optionally: . | Put quantum on the agenda with senior management.  . | Involve collaborators, suppliers and vendors, make your interest in quantum known. It is in your benefit if suppliers are well-prepared.  . | Participate in a workshop, hackathon, and other events. | . We have a list of interesting learning resources, and an overview of Netherlands-based education opportunities and events.  . Very soon, the quantum journey will split into two fairly different categories: .   . | Preparing for quantum applications, where the goal is to leverage quantum technologies to gain some competitive advantage (for example, by strengthening your R&amp;D, further optimising your logistics, improving a product, etc). For most companies, the main items of interest are the applications of quantum computing.  . | Migrating to quantum-safe cryptography, where the goal is to keep your IT secure against attackers with a quantum computer. | . It is important to make this distinction, because these categories have different goals and are conventionally addressed by different departments. Let us first look at quantum applications. Prepare to use quantum applications . 2a: Start exploring use-cases . For most businesses, the goal of early, low-regret moves is to be ready to leverage quantum technologies fairly soon after they start offering an advantage.  . Must do: . | Find the most impactful use-cases in your area. | Sketch a road map for the coming years.  . | . Optional but recommended:  . | Start concrete explorations, for example by testing and implementing trial use-cases. It is relatively easy (and fun!) to follow the tutorial of programming packages like Qiskit or Cirq.  . | Find a partner. Save costs by collaborating on early, pre-competitive exploration.  . | Create PR! We notice that many companies are very actively promoting their early results on quantum applications, even if these do not offer significant advantages yet.  . | Build a skilled workforce. BCG states: “The biggest challenge may be talent, given the supply constraints. […] Building in-house talent will take time, so it is best to start as soon as possible. | . A good exercise is to look at your current needs in high-performance computing. What do you currently spend your computing budgets on? Are there any areas where new tools in computation or modelling could provide serious business value (for example, by being faster, tackling bigger problems, or delivering higher accuracy)? Which quantities would you ideally have calculated, but are beyond the reach of current computers?  . Sometimes you rapidly identify use-cases that are not worth further addressing. For example, if today’s computational power is easily sufficient to meet your needs, it’s unlikely that you want to invest in quantum computing for that use case.  .   . **R&amp;D partnerships **At Quantum.Amsterdam, we have strong ties to Amsterdam-based research organisations. Offers you might be interested in include: . | Short-term applied research by the Quantum Application Lab, which could lead to a roadmap or a proof-of-concept implementation.  . | We invite collaborators to sponsor a PhD or Postdoc researcher at an academic institute, leading to a 2-4 year intensive investigation of a certain use case. There are often subsidies for such public-private partnerships. Our website shows several examples of the past collaborations, and information about starting a new project can be found at the local tech transfer office.  . | . Also see: . | Quantum.Amsterdam meetup   (Youtube) | . 3a: Implementing actual applications, whenever ready . From here onwards, we find it hard to give concrete advice: goals may depend on your area of business, and on the way that the field of quantum will progress. Other sources will simply tell you do “develop a long-term strategy” or similar.  . For inspiration, or a dot on the horizon, you may think towards a competence center for quantum computing, similar to how many companies have special departments for data science and/or AI. It may also be reasonable to combine these departments.  . A common advice that we will relay here: the business use of quantum computers is still very uncertain, so at best we can recommend to remain agile! . Migrating to post-quantum cryptography . 2b. Prepare for migration to post-quantum cryptography.  . Cryptography is a completely different beast, with a more concrete goal, and more urgent timelines for most organizations (here’s why). Luckily, pretty much everyone faces the same problem, making it easier to give concrete guidelines. In essence, anyone who uses modern cryptography should migrate to Post-Quantum Cryptography (PQC) in the next decade. Although standards and regulations are still under construction, there are some urgent steps should be taken well before the migration can start.  . We recommend the PQC Migration Handbook (April 2023), written by the Dutch secret service AIVD, and research organisations CWI and TNO.  . In general, as a first step, it recommends to: . | Determine the risk and urgency of PQC migration. This likely requires making an inventory of all cryptography in use, and determining how long your data should remain private.  . | Create a migration plan.  . | . “Certain organisations should already start working on mitigating measures now. [..] For instance, organisations handling data that will still be confidential 20 years from now, or organisations developing long-lived systems that will still be in use decades from now.” . THE PQC MIGRATION HANDBOOK . Judging from previous migrations, [implementing PQC] might take well over five years. THE PQC MIGRATION HANDBOOK . 3b. Migrate . This is a much more technical part, for which you will need a well prepared migration plan. We find it likely that quantum computers might break today’s RSA encryption standard somewhere in the 2030’s, so we stress that such migration should be completed well before that time — taking in to account that IT migrations can take significantly longer than originally planned! . FAQ . Should I have knowledge of quantum mechanics? Should I understand quantum hardware?  . When you are tasked with implementing early-stage quantum applications: then yes! . As a strategic manager, or when working on post-quantum cryptography: no.  . Do I really need to work on NISQ? . That’s a good question that is hard to answer. We find it unlikely that near-term quantum computers (in the next, say, 1-4 years) will create relevant advantages, although there are plenty of experts who are more optimistic. Future applications will likely differ strongly from the way we ‘program’ quantum computers today, but practising on today’s hardware can grow your understanding and give an edge in experience.  . What should I do first: focus on quantum applications or post-quantum cryptography? . For most organisations, the threat to security is more urgent, especially because the migration trajectory can take several years (and this must be completed well before we have large-scale quantum computers!).  . However, most organisations work on both trajectories in parallel. I’m looking for a collaboration – what should I do? . Typically, business developers meet each other at conferences and events, where you may find like-minded partners. We also encourage you to contact us for some sparring on what your organization can do, and to get connected to Netherlands-based knowledge institutes like QuSoft and Quantum Application Lab.  . ",
    "url": "/part3/chapter_4/#what-steps-should-your-organization-take",
    
    "relUrl": "/part3/chapter_4/#what-steps-should-your-organization-take"
  },"40": {
    "doc": "4 What steps should your organization take?",
    "title": "4 What steps should your organization take?",
    "content": " ",
    "url": "/part3/chapter_4/",
    
    "relUrl": "/part3/chapter_4/"
  },"41": {
    "doc": "1 Further reading",
    "title": "Further reading",
    "content": "Below, we give a selection of recommended sources to learn more about this fascinating topic. I want to learn the technical details . For (late) high school students: . (or those who followed high-school level mathematics): . | Quantum Quest [Book/website] is an intensive 5-week online course about the theory (mathematics) of quantum computing. Materials are freely available for self-study.  . | Quantum in Pictures (Cooke) [Book] teaches the theory (mathematics) of quantum computing using diagrams. | . Undergraduate (Bachelor’s) university level: . | Quantum.Country [Website] – the “Duolingo of Quantum Computing”, a very well-written introduction for those with late high-school or early university level math background.  . | Quantum Computation and Quantum Information (Nielsen, Chuang) [Book] – the “bible of quantum computing”. Perhaps not the most up-to-date, but definitely the most well-known resource in our field. Sets the standards for jargon and notation.  . | . | Quantum Computer Science: An Introduction (Mermin) [Book] – a well-written introduction, with quite some focus on manipulating quantum circuits. | Quantum Computing Since Democritus (Aaronson) [Book] – Aaronson is an authority in the field. His book touched upon many topics such as the foundations of computer science, black holes and consciousness, making it a good read for those looking for something much more broad than just quantum computing. | . Graduate (Master’s) level: . These assume no prior knowledge about quantum physics, but require a strong background in mathematics (i.e. linear algebra, calculus, advanced inequality bounds and approximations, etc.). In exchange, they go into much more detail.  . | Lecture Notes for UvA course “Quantum Computing” by Ronald de Wolf, which is frequently updated and features some cutting-edge algorithms. Via the course website, you can find the link and password to view all the recorded lectures too.  | . | Lecture Notes for Caltech course “Quantum Computing” by John Preskil  | . Scientific overview papers . The papers below are aimed at scientists from fields other than quantum computing itself. All papers we mention are open-access and peer-reviewed, making them very suitable for citation.  . | Quantum algorithms: an overview (Ashley Montanaro) . | The Potential Impact of Quantum Computers on Society (Ronald de Wolf) [video lecture] . | . Scientific opinions and discussions . | Scott Aaronson’s blog. Although written from a theoretical computer science perspective, this blog addresses a very broad range of quantum computing topics. Prof. Aaronson has a strong authority in the field, and his posts attract readership and comments from a broad range of prominent scientists.  | . I want to learn to program a quantum computer . Several programming packages for quantum computers exist, mostly maintained by major hardware providers. All of them offer great introductory tutorials. The ones we recommend below are all in Python.  . | Qiskit, the language by IBM, probably features the largest catalogue of learning materials. To start from scratch, we recommend following the “Basics of Quantum Information”, which teaches both the mathematics behind qubits and the usage of the package itself. | Cirq is a very similar package developed by Google. As of 2024, they have a more focused tutorial to explain the programming package itself, without extensive theory of quantum mechanics. | QWorld Bronze offers tutorials in the form of Jupyter notebooks and hosts various training days around the world, mostly focused on Qiskit and sometimes ProjectQ. | PennyLane is a package by startup Xanadu with a strong focus on machine learning applications. | . I want to stay up-to-date with the latest developments . Major conferences . | Q2B (organized by QCWare) . | IQT (Inside Quantum Technology) . | Quantum.Tech (organized by Alpha Events) . | Commercializing Quantum (organized by The Economist) . | . Business News . | Quantum Computing Report - don’t be fooled by the basic look on the website. The content is written with a very critical eye and with very relevant contextual information, making them our favourite source for quantum-related news.   | . | The Quantum Insider | . Scientific news . None of these focus exclusively on Quantum Technology, but all offer high-quality news (and surely none would miss any important quantum breakthroughs).  . | Quanta Magazine . | Phys.org . | . ### . ### . I want to learn more about business implications . Several sources cover similar topics as this book. Most of these come from consultants of hardware providers who have a financial interest in making others get started with quantum. In our opinion, the articles are sometimes too optimistic and predict that quantum applications will come much sooner than the typical expert would anticipate. On the other hand, they collect insightful details about financial matters.  . | McKinsey publishes yearly “Quantum Technology Monitor” reports, focusing on the economic impact the quantum computers will have. Here is the 2024 edition. | Cloudlfare’s support pages contain an incredibly complete bible of Post-Quantum Cryptography. | . Workshops and trainings . These might not be the most relevant if you completed this book, by can be particularly useful to inspire your colleagues. | The Workshop General Awareness Quantum Computing follows the same philosophy as this book: an introduction to business opportunities that should be understandable for everyone. | Qureca is a british startup that offers several trainings, such as “Quantum for everyone” and “Quantum Training for Business”. | . ",
    "url": "/part4/chapter_1/#further-reading",
    
    "relUrl": "/part4/chapter_1/#further-reading"
  },"42": {
    "doc": "1 Further reading",
    "title": "1 Further reading",
    "content": " ",
    "url": "/part4/chapter_1/",
    
    "relUrl": "/part4/chapter_1/"
  },"43": {
    "doc": "2 Quantum Hype Bingo",
    "title": "Quantum Hype Bingo",
    "content": "| Harness the commercial potential | Game-changing | Transformative potential | . | Unprecedented capabilities | “Our algorithm solves …” (without benchmark comparison) | Solve (NP-)hard optimization problems | . | | | . ",
    "url": "/part4/chapter_2/#quantum-hype-bingo",
    
    "relUrl": "/part4/chapter_2/#quantum-hype-bingo"
  },"44": {
    "doc": "2 Quantum Hype Bingo",
    "title": "2 Quantum Hype Bingo",
    "content": " ",
    "url": "/part4/chapter_2/",
    
    "relUrl": "/part4/chapter_2/"
  }
}
