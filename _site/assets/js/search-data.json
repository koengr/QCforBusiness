{"0": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Audience",
    "content": "This book targets anyone who encounters quantum technologies in their professional lives, but who donâ€™t not need a full physics background. This includes: . | Managers and strategic decision makers . | Consultants . | Policy makers . | CIO or CISO departments . | Investors . | . | Â  | This book is aboutâ€¦ | Â  | Â  | This book does not containâ€¦ | . | âœ“ | The impact that quantum technology has on business and society | Â  | âœ— | Essential math or physics | . | âœ“ | Opportunities and threats | Â  | âœ— | Quantum programming | . | âœ“ | Timelines | Â  | âœ— | Exhaustive information about every possible detail | . | âœ“ | Links to other great resources | Â  | Â  | Â  | . ",
    "url": "/#audience",
    
    "relUrl": "/#audience"
  },"1": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Get the softcover",
    "content": "Prefer to read a printed edition? A physical edition will be released in Q4 2024. ",
    "url": "/#get-the-softcover",
    
    "relUrl": "/#get-the-softcover"
  },"2": {
    "doc": "Introduction to Quantum Computing for Business",
    "title": "Introduction to Quantum Computing for Business",
    "content": ". This free book contains everything you should know about quantum computers, without going into tedious technical details. It answers questions such as: . | What are the applications of quantum computers and quantum networks? . | How long will it take before quantum computing becomes competetive? . | What are the consequences for cybersecurity? . | How can an organisation effectively prepare? . | What is the status of todayâ€™s hardware? . | . Start reading . ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "The applications: what problems will we solve?",
    "title": "The applications: What problems will we solve with quantum computers?",
    "content": "At a glance The most important application areas are . | the simulation of chemistry and materials, . | cracking cryptography, . | using quantum networks to distribute cryptographic keys, and . | solving large-scale optimisation and AI problems.Â  . | . Getting utility out of a quantum computer is not straightforward. It requires an algorithm that beats all other known methods (even those that run on much faster classical computers), and it must tackle a problem with real-world relevance. Especially in optimisation and AI, we have not found a convincing â€˜killer applicationâ€™ yet. In the previous chapter, we saw that quantum computers are extremely slow computers, but they happen to solve some problems more efficiently, that is, in fewer steps. The most important question in this field is: what advantage do quantum computers have on which problems? . The Quantum Algorithm Zoo1 lists pretty much all known quantum algorithms. It has grown into an impressive list that cites over 400 papers. Unfortunately, upon closer inspection, itâ€™s hard to extract precisely the useful business applications, for a few reasons. Some algorithms solve highly artificial problems for which no real business use cases are known. Others may make unrealistic assumptions or may only offer a speedup when a problem grows to outrageously large problems (that we never encounter in the real world). Nevertheless, itâ€™s definitely recommended to scroll through. For this book, we take a different approach. We focus specifically on algorithms with plausible business applications. To assess their advantage, we split our main question into two parts: . | What are projected applications with a quantum speedup? . | How large is the practical advantage these speedups? . | . ",
    "url": "/essentials/applications-overview/#the-applications-what-problems-will-we-solve-with-quantum-computers",
    
    "relUrl": "/essentials/applications-overview/#the-applications-what-problems-will-we-solve-with-quantum-computers"
  },"4": {
    "doc": "The applications: what problems will we solve?",
    "title": "What are projected applications with a quantum speedup?",
    "content": "We foresee four major families of use cases where quantum computing can make a real impact on society. We briefly discuss each of them here and link to a later chapter that discusses each application in more depth.Â  . 1. Simulation of other quantum systems: molecules, materials, and chemical processes . Most materials can be accurately simulated on classical computers. However, in some specific situations, the locations of atoms and electrons become notoriously hard to describe, sometimes requiring quantum mechanics to make useful predictions. Such problems are the prototypical examples of where a quantum computer can offer a great advantage. Realistic applications could be in designing new chemical processes (leading to cheaper and more energy-efficient factories), estimating the effects of new medicine, or working towards materials with desirable properties (like superconductors or semiconductors). Of course, scientists will also be excited to simulate the physics that occur in exotic circumstances, like at the Large Hadron Collider or in black holes. Simulation is, however, not a silver bullet, and quantum computers will not be spitting out recipes for new pharmaceuticals by themselves. Breakthroughs in chemistry and material science will still require a mix of theory, lab testing, computation, and, most of all, the hard work of smart scientists and engineers. From this perspective, quantum computers have the potential to become a valued new tool for R&amp;D departments. Read more: What are the main applications in chemistry and material science? . See also: . | Startup PhaseCraft studies the famous Fermi-Hubbard model using a quantum computer . | Startup Zapata reduces the runtime and error rate of famous chemistry algorithm . | IBM and Daimler research next-gen batteries . | Roche started a project to find medicines for Alzheimerâ€™s . | An overview of various simulation software packages for quantum computers . | . 2. Cracking a certain type of cryptography . The security of todayâ€™s internet communication relies heavily on a cryptographic protocol invented by Rivest, Shamit and Adleman (RSA) in the late 70s. The protocol helps distribute secret encryption keys (so that nobody else can read messages in transit) and guarantees the origin of files and webpages (so that you know that the latest Windows update actually came from Microsoft, and not from some evil cybercriminal). RSA works thanks to an ingenious mathematical trick: honest users can set up their encryption using relatively few computational steps, whereas â€˜spyingâ€™ on others would require one to solve an extremely hard problem. For the RSA cryptosystem, that problem isÂ prime factorisation,Â where the goal is to decompose a very large number (for illustration purposes, letâ€™s think of 15) into its prime factors (here: 3 and 5). As far as we know, for sufficiently large numbers, this task takes such an incredibly long time that nobody would ever succeed in breaking a relevant code â€“ at least on a classical computer. This all changed in 1994 when computer scientist Peter Shor discovered that quantum computers happen to be quite good at factoring. The quantum algorithm by Shor can crack RSA (and also its cousin calledÂ elliptic curve cryptography) in a relatively efficient way using a quantum computer. To be more concrete, according toÂ a recentÂ paper, a plausible quantum computer could factor the required 2048-bit number in roughly 8 hours (and using approximately 20 million imperfect qubits). Note that future breakthroughs will likely further reduce the stated time and qubit requirements. Luckily, not all cryptography is broken as easily by a quantum computer. RSA falls in the category ofÂ public key cryptography,Â which delivers a certain range of functionalities. A different class of protocols isÂ symmetric key cryptography,Â which is reasonably safe against quantum computers but doesnâ€™t provide the same rich functionality asÂ public keyÂ crypto. The most sensible approach is replacing RSA with so-calledÂ post-quantum cryptographyÂ (PQC): public-key cryptosystems resilient to attackers with a large-scale quantum computer. Interestingly, PQC doesÂ notÂ require honest users (thatâ€™s you) to have a quantum computer: it will work perfectly fine on todayâ€™s PCs, laptops and servers. Read more: How will quantum computers impact cybersecurity?Â  . See also: . | (Youtube, technical) MinutePhysics explains Shorâ€™s algorithm.Â  . | Nature feature article:Â The race to save the Internet from quantum hackers . | . Â  . In the following years, every large organisation will have to worry about updating to post-quantum cryptography â€“ a complex migration that comes in addition to the many existing cybersecurity threats. The foundations have been laid: thanks to the American National Institute of Standards and Technology (NIST), cryptographers from around the globe came together to select the best quantum-safe alternatives, culminating in the publication of the first standards in August 2024. These are the new algorithms that the vast majority of users will adopt. Unfortunately, many organisations run a vast amount of legacy software that is hard to update, making this a complex IT migration that can easily take 5-15 years, depending on the organisation. Thereâ€™s a serious threat that quantum computers will be able to run Shorâ€™s algorithm within such a timeframe, so organisations are encouraged to start migrating as early as possible.Â  . A new type of cryptography comes with its own additional risks: the new standards have not yet been tested as thoroughly as the nearly 50-year-old RSA algorithm. Ideally, new implementations will beÂ hybrid, meaning that they combine the security of a conventional and a post-quantum algorithm. On top of that, organisations are encouraged to adoptÂ cryptographic agility, meaning that cryptosystems can be easily changed or updated if the need arises.Â  . Read more: What steps should your organisation take? . Other great sources are: . | The PQC Migration Handbook, written by the Dutch secret service AIVD and research organisations CWI and TNO, goes into many more details on how to update to quantum-safe cryptography. | CloudflareÂ tracks the adoption of post-quantum cryptographyÂ and explains many technical details extremely well.Â  . | UK National Cyber Security Center:Â Preparing for Quantum-Safe Cryptography . | BSI (German secret service):Â Quantum-safe cryptography â€“ fundamentals, current developments and recommendations . | NISTâ€™s webpage on standardization of PQC. | . 3. Quantum Key Distribution to strengthen cryptography . Out of all the applications for quantum networks, Quantum Key Distribution (QKD) is the one to watch. It allows two parties to generate secure cryptographic keys together, which can then be used for everyday needs like encryption and authentication. It requires a quantum network connection that transports photons in fragile quantum states. Such connections can currently reach a few hundred kilometres, and there is a clear roadmap to expand to a much wider internet. The most likely usage will be as an â€œadd-onâ€ for high-security purposes (such as military communication or data exchange between data centres), in addition to standard post-quantum cryptography.Â  . Unfortunately, we often see media articles suggesting that QKD is a solution to the threat of Shorâ€™s algorithm and that it would form an â€˜unbreakable internetâ€™. Both claims are highly inaccurate. Firstly, QKD does not offer the wide range of functionality that public-key cryptography offers, so it is not a complete replacement for the cryptosystems broken by Shor. Secondly, there will almost certainly be ways to hack a QKD system (just like with any other security system). Then why bother with QKD? The advantage of QKD is based on one important selling point: contrary to most other forms of cryptography, it does not rely on assumptions about the computational power of a hacker. This can be an essential factor when someone is highly paranoid about their cryptography or when data has to remain confidential for an extremely long period of time.Â  . At this time, pretty much every national security agency discourages the use of QKD simply because the available products are far from mature (and because PQC should be prioritised). It is unclear how successful QKD could be in the futureâ€”we will discuss this in depth in another chapter. We firmly warn that other security products with the word â€œquantumâ€ in the name do not necessarily offer protection against Shorâ€™s algorithm. In particular, â€œquantum random number generatorsâ€ (QRNGs) are sometimes promoted as a saviour against the quantum threat, which is nonsense. These devices serve a completely different purpose: they compete with existing hardware to generate unpredictable secret keys, which find a use (for example) inÂ hardware security modulesÂ in data centres.Â  . Read more: What are the use cases of quantum networks? . See also:Â  . | A short video explainer about how QKD works. | The NCSC states that it â€œdoes not endorse the use of QKD for any government or military applicationsâ€ . | The French ANSSI, German BSI, Dutch NLNCSA and Swedish SNCSA published a critical position paper on QKD in 2024.Â  . | Samsung builds QRNGs into certain phones on the South Korean market.Â  . | . 4. Optimisation and machine-learning . This is the part where most enterprises get excited. Can we combine the success of AI and machine learning with the radically new capabilities of quantum computers? Can we create a superpowered version of ChatGPT or DALL-E, or at least speed up the demanding training process? . In this section, weâ€™ll take a closer look at the known applications for quantum computers on â€œnon-quantum problemsâ€ other than cryptography. We focus specifically on the harder optimisation problems that currently take up large amounts of classical resources. Under the hood, all such applications are based on concrete mathematical problems such as binary optimisation, differential equations, classification, optimal planning, and so forth. For conciseness, we will use the word â€˜optimisationâ€™ as a catch-all term for all these problems, including things like machine learning and AI. Unfortunately, the amount of value that â€˜quantumâ€™ can add to optimisation tasks is very much a disputed topic. The situation here is very subtle: there exist many promising quantum algorithms, but as weâ€™ll see, each comes with important caveats that might limit their practical usefulness. To start, we can classify the known algorithms in the following three categories. Rigorous but slow algorithms . Many quantum optimisation algorithms have a well-provenÂ quantum speedup:Â there is no dispute that these requireÂ fewerÂ computational stepsÂ than any classical algorithm. For instance, a famous quantum algorithm invented byÂ Lov GroverÂ (with extensions byÂ Durr and Hoyer) finds the maximum of a function in fewer stepsÂ than conventional brute-force search. Similarly, quantum speedups were found for popular computational methods such asÂ backtracking,Â gradient descent,Â semidefinite programming,Â lasso, andÂ interior point methods for solving differential equations.Â  . The main question is whether this also means that the quantum computer requires lessÂ time! All of the above optimisation algorithms offer a so-calledÂ polynomial speedupÂ (in the case of Grover, this is sometimes further specified to be aÂ quadratic speedup). As we will soon see, it is not entirely clear if these speedups are enough to compensate for the slowness of a realistic quantum computer â€“ at least in the foreseeable future.Â  . Heuristic algorithms . Some algorithms claim much larger speedups, but there is no undisputed evidence to back this up. Often, these algorithms are tested on small datasets using the limited quantum computers available today â€“ which are still so tiny that not much can be concluded about larger-scale problems. Nonetheless, these â€˜high risk, high rewardâ€™ approaches typically make the bold claims that receive media attention. The most noteworthy variants are: . | Variational quantum circuitsÂ (VQC) are relatively short quantum programs that a classical computer can incrementally change. In jargon, these are quantum circuits that rely on a set of free parameters. The classical computer will run these programs many times, trying different parameters until the quantum program behaves as desired (for example, it might output very good train schedules or accurately describe a complex molecule). The philosophy is that we squeeze as much as possible out of small quantum computers with short-lived qubits: the (fast) classical computer takes care of most of the computation, whereas the quantum computer runs just long enough to sprinkle some quantum magic into the solution. Although its usefulness is disputed, this algorithm is highly flexible, leading to quantum variants of classifiers, neural networks, and support vector machines. Variants of this algorithm may be found under different names, such as Quantum Approximate optimisation Algorithm (QAOA), Variational Quantum Eigensolver (VQE), and quantum neural networks.Â  . | Quantum annealingÂ solves a particular subclass of optimisation problems. Instead of using the conventional â€˜quantum gatesâ€™, it uses the native physical forces that act on a set of qubits in a more analog way. Annealing itself is a mature classical algorithm. The advantage of a â€˜quantumâ€™ approach is not immediately apparent, although there are claims that hard-to-find solutions are more easily reached thanks to â€˜quantum fluctuationsâ€™ or â€˜tunnellingâ€™. Quantum annealing was popularised by the Canadian companyÂ D-Wave, which builds dedicated hardware with up to 5000 qubits and offers a cloud service that handles relatively large optimisation problems.Â  . | . Fast solutions in search of a suitable problem . Finally, there exist algorithms with large speedups, for which we are still looking for use-cases with any scientific or economic relevance. The most notable example is the quantum algorithm that solves systems of linear equations2 with an exponential advantage. This problem is ubiquitous in engineering and optimization, but unfortunately, there are so many caveats that no convincing practical uses have been found3. Recently, much attention has gone to the algorithm for topological data analysisÂ (a method to assess certain global features of a dataset), which promises an exponential advantage under certain assumptions. Again, scientists are still searching for a convincing application. Similarly, a quantum version of a classical machine learning algorithm called Support Vector Machines was found to have an exponential advantage over classical methods4. Unfortunately, this only works with a very specific dataset based on the factoring problem that Shorâ€™s algorithm is well known for. No rigorous advantage is known for more general datasets. A fourth class: quantum-inspired algorithms . Some impressive speedups that were recently found have been â€˜dequantizedâ€™: these algorithms were found to work on classical computers too! Thereâ€™s a beautiful story behind this process, where Ewin Tang, a Masterâ€™s student at the time, made one of the largest algorithmic breakthroughs of the decade. A great report by Robert Davis can be found on Medium5.Â  . Whatâ€™s left? . Unfortunately, there does not yet exist an optimisation algorithm with undisputed economic value: all of them come with serious caveats. This perspective is perhaps a bit disappointing, especially in a context where quantum computing is often presented as a disruptive innovation. Our main takeaway is that quantum optimisation (especially quantum machine learning!) is rather over-hyped.Â  . That doesnâ€™t mean that thereâ€™s no hope for quantum optimisation. Firstly, there are good reasons to believe that new algorithms and applications will be found. Secondly, the usefulness of the â€œslowerâ€ quantum optimization algorithms ultimately depends on the speed of a future quantum computer compared to the speed of a future classical computer. To better understand the differences in computational speeds, we will need to quantify the amount of â€˜quantum advantageâ€™ that different algorithms have. Â  . Further reading:Â  . | Volkswagen and ExxonMobil used annealing to optimise routes forÂ busesÂ andÂ transport ships. | Professor Scott Aaronson warns us to â€˜Read the fine printâ€™ of optimisation algorithms.Â [Appeared inÂ Nature Physics, with paywall]Â  . | Professor Sanker Das Sarma warns of hype within the field of quantum optimisation and machine learning. | . ",
    "url": "/essentials/applications-overview/#what-are-projected-applications-with-a-quantum-speedup",
    
    "relUrl": "/essentials/applications-overview/#what-are-projected-applications-with-a-quantum-speedup"
  },"5": {
    "doc": "The applications: what problems will we solve?",
    "title": "How can we compare different types of speedups?Â ",
    "content": "When looking at the applications of quantum computers, one should always keep in mind: Are these actual improvements over our current state-of-the-art? Anyone can claim that their algorithmÂ canÂ solve a problem, but what we really care about is whether it solves itÂ faster. Classical computers are already extremely fast, so quantum algorithms should offer a substantial speedup before they become competitive.Â  . The most fair way to compare algorithms is by running them on actual hardware in a setting similar to how you would use the algorithm in practice. In the future, we expect such benchmarks to be the main tool to compare quantum and classical approaches. However, mature quantum hardware is not available yet, so we resort to a more theoretical comparison tool: the asymptotic runtime of an algorithm. What does asymptotic runtime mean? An important figure of merit of an algorithm is its so-called asymptotic complexity, which describes how much longer a computation takes as the problem becomes â€˜biggerâ€™ or more complicated. The term â€˜asymptoticâ€™ refers to the problemâ€™s size, which gets (asymptotically) larger, theoretically all the way to infinity. Size turns out to be a very relevant parameter. For example, computing 54 x 12 is much easier than 231.423 x 971.321, even though in technical jargon, they are the same problem of â€˜multiplicationâ€™, and weâ€™d use the very sameÂ long multiplication algorithm that we learned in elementary school to tackle them. Similarly, creating a work schedule for a team of 5 is simpler than dealing with 10.000 employees. We typically use the letter \\(n\\) to denote the problem size. You can see \\(n\\) as the number of digits in a multiplication (like 2 or 6 above) or the number of employees involved in a schedule.Â  . For some very hard problems, the time to solution takes the form of an exponential, something like \\(T\\ \\sim\\ 2^{n}\\), where \\(T\\) is the time taken6. Exponential scaling is typically a bad thing, as the function \\(2^{n}\\) becomes incredibly large even for moderate values of \\(n\\). For example, the problem of factoring scalesÂ somewhat similarÂ to \\(T\\ \\sim\\ 2^{n}\\) on a classical computer. There are also problems for which the scaling looks like a polynomial, such as \\(T\\ \\sim\\ n^{3}\\) or \\(T\\ \\sim\\ n\\). Polynomials grow much slower than exponentials, making it easier to solve large problems in a reasonable amount of time. Quantum computers tackle factoring with roughly \\(T\\ \\sim\\ n^{3}\\) (thanks toÂ Shorâ€™s algorithm7). Because a quantum computer brought the exponential down to a polynomial, we call this an â€˜exponential speedupâ€™. Such speedups are a computer scientistâ€™s dream because they have a tremendous impact on practical runtimes.Â  . Often, we deal with â€˜merelyâ€™ aÂ polynomial speedup, which happens when we obtain a smaller polynomial (for example, going from \\(T\\ \\sim\\ n^{2}\\)Â towards \\(T\\ \\sim\\ n\\) or perhaps even a â€˜smallerâ€™ exponential function (like \\(T\\ \\sim\\ 2^{n}\\)Â towards \\(T\\ \\sim\\ 2^{n/2}\\)). Reducing the exponent by a factor of two (like \\(n^{2}\\ \\rightarrow n\\)) is also sometimes called aÂ quadratic speedup, which is precisely what Groverâ€™s algorithm gives us. Further reading: . | At a more coarse level, we can define differentÂ complexity classes like P, NP and BQP.Â  | . Here is a rough overview of quantum speedups as we understand them today, categorised by their type of asymptotic speedup: . Â  . ğŸŸ¢Â  Â The â€œexponentialâ€ box is the most interesting one, featuring applications where quantum computers seem to have a groundbreaking advantage over classical computers. It containsÂ Shorâ€™s algorithmÂ for factoring, explaining the towering advantage that quantum computers have in codebreaking. We also believe it contains some applications inÂ chemistry and material science, especially those relating to dynamics (studying how molecules and materials change over time).Â  . ğŸŸ¡Â  Â TheÂ â€œpolynomialâ€Â box is still interesting, but its applicability is unclear. Recall that a quantum computer would need much more timeÂ per stepÂ â€“ and on top of that, it will have considerable overhead due toÂ error correction. Does a polynomial reduction in the number of steps overcome this slowness? According to aÂ recent paper, small polynomial speedups (as achieved byÂ Groverâ€™s algorithm) will not cut it, at least not in the foreseeable future.Â  . ğŸ”´Â  Â For some computations, a quantum computer offersÂ no speedup.Â Examples include sorting a list or loading large amounts of data.Â  . If this were the complete story, then most people would agree that quantum computing is a bit disappointing. It would be a niche product for hackers and a tiny community of physicists and chemists who study quantum mechanics itself.Â  . âšªÂ  Â Luckily, there is yet another category: many of the most exciting claims come from theÂ heuristicÂ algorithms. This term is used when an algorithm might give a suboptimal solution (which could still be useful) or when we cannot rigorously quantify the runtime. Such algorithms are quite common on classical computers: neural networks fall in this category, and these caused a significant revolution in AI. Unfortunately, it is unclear what the impact of currently known heuristic quantum algorithms will be.Â  . In summary, we see that the utility of the mentioned quantum applications is unclear â€“ but some are more unclear than others. The following graph summarises this well: the most â€˜valuableâ€™ applications (like quantum machine learning) also come with the largest risks, whereas the most convincing speedups (like codebreaking) offer less value. The applications of chemistry and material science sit somewhere in between. Unfortunately, we donâ€™t dare to assign concrete numbers to this graph. Thatâ€™s something that we will need to empirically find out in the near future. See also: . | A quantitative analysis of Groverâ€™s runtime compared to todayâ€™s supercomputers.Â  . | (Technical) Amazon researchers lay out a comprehensive list of end-to-end complexities of nearly every known quantum algorithm. | . ",
    "url": "/essentials/applications-overview/#how-can-we-compare-different-types-of-speedups",
    
    "relUrl": "/essentials/applications-overview/#how-can-we-compare-different-types-of-speedups"
  },"6": {
    "doc": "The applications: what problems will we solve?",
    "title": "Where is the killer application?",
    "content": "Is there hope that weâ€™ll find new quantum algorithms with a large commercial or societal value? For a quantum algorithm to be truly impactful, we require two properties:Â  . | [Useful] The algorithm solves a problem with real-world significance (for example, because organisations can work more efficiently or because it helps answer a scientific question). | [Better/faster] Using this particular algorithm is the most sensible* choice from a technical perspective**, even when compared to all other possible methods. | . Iâ€™d propose the term quantum utility for a situation where both properties are convincingly satisfied. The precise definition can be a bit finicky, so before we start searching for utility, we need to get some technical details out of the way. * What is â€˜sensibleâ€™ (2) depends strongly on the context of the real-world problem (1). In most cases, we care about how fast a problem is solved, but one should also take into account the total cost of developing the software, the cost of leasing the hardware, the energy consumption, the probability of errors, and so forth. For example, a high-frequency trader might be happy with a 2% faster algorithm even if the costs are sky-high and thereâ€™s a decent chance of failure, whereas a hospital could dismiss a 200x faster quantum approach if the costs donâ€™t outweigh the benefits. Indeed, what is â€˜sensibleâ€™ is highly subjective. In practice, we can relax this requirement somewhat and focus primarily on speed, which is a sufficiently complex figure of merit. Ideally, the quantum algorithm should enjoy anÂ exponentialÂ speedup or at least a large polynomial speedup.Â  . ** We explicitly look for technical perspectives. Otherwise, one might also say that using a quantum algorithm is commercially the best option because it creates good PR or because it keeps the workforce excited. Then perhaps the first utility has already been reached! However, this is not the computational revolution that weâ€™re looking for, so I explicitly exclude such non-technical reasons in property (2). Similarly, I donâ€™t want to worry too much about legal issues (â€œit doesnâ€™t comply with regulationsâ€) because it feels somewhat artificial to dismiss a quantum algorithm for such reasons. Supremacy, advantage, utility . Around 2019 and 2020, the terms quantum supremacy and quantum advantage were popularly used when quantum computers did, for the first time, beat the best supercomputers in terms of speed (property 2)8 9. This involved an algorithm that was cherry-picked to perform well on a relatively small and noisy quantum computer whilst being as challenging as possible for a conventional supercomputer. Quantum advantage was mostly a man-on-the-moon-type scientific achievement, showcasing the rapid progress in hardware engineering and silencing the sceptics who still thought quantum computing wouldnâ€™t work. There was no attempt to have any practical value (1). As a natural next step, the race is on to be the first to run something useful whilst leaving classical supercomputers in the dust. This led IBM to coin the term quantum utility10, which we adapted above. In the following years, we can expect the leading hardware and software manufacturers to maximise the amount of â€˜utilityâ€™ that they could possible squeeze out of medium-sized quantum computers, whilst competitors will use their best classical simulations to dispute these claims. The first battles have already been fought: in June 2023, IBM claimed to simulate certain material science models better than classically possible11, quickly followed by two scientific responses that showed how easy it was to simulate the same experiment on a laptop12 13. It seems to me that a healthy competition is good for the field overall. It should lead to increasingly convincing and rigorous quantum utility, from which the end-users will eventually profit! . In parallel, there is a rapidly expanding number of press releases by startups and enterprises that claim to create business value by solving industrial problems on todayâ€™s hardware, often without sharing many details. These approaches typically start with a relevant problem in mind and hence score well on usefulness (1). However, it is questionable whether quantum algorithms were indeed the best option (2), and most reports Iâ€™ve seen hardly bother to show any argumentation in this direction. Such claims should only be taken seriously if a rigorous benchmark against state-of-the-art classical techniques is included. Do known algorithms provide utility? . With the quantum utility criteria in mind, we can revisit the algorithms that were discussed before. | | (1) Useful | (2) . Better than classical . | . | Optimisation: Rigorous but slow algorithms | âœ“ | ? | . | Optimisation: Solutions in search of a problem | ? | âœ“ | . | Optimisation: Heuristic algorithms | âœ“ | ? | . | Simulation of molecules and materials | âœ“ | ? | . | Breaking RSA | âœ“ | âœ“ | . Several algorithms, most notably Groverâ€™s algorithm, have an extensive range of industrial applicability. However, it seems that in practice, other (classical) approaches solve such problems faster. The quadratic speedup will be insufficient in the near term, and itâ€™s unclear if it will be in the future.Â  . Then, we have several exponential speedups, like the algorithm for topological data analysis, for which no practical uses have been found (despite many scientific and industrial efforts).Â  . Most optimistic outlooks focus on heuristic algorithms, for which the speed advantage will become clear with maturing hardware. Even for simulation of molecules and materials, it is hard to pinpoint precisely where we can find utility. Classical computers are already incredibly fast, and excellent classical algorithmic techniques have been developed. Scientist Garnet Chan even givesÂ talks which are suggestively titled â€œIs There Evidence of Exponential Quantum Advantage in Quantum Chemistry?â€.Â The case for chemistry and material science is quite subtle, and we discuss this further in the in-depth chapter on quantum simulation. To our best knowledge, codebreaking (Shorâ€™s algorithm) is the only impactful algorithm that has little competition from classical methods. Unfortunately, this is primarily aÂ negativeÂ application that helps criminals â€“ we are not aware of any positive uses of Shor, so it is not quite the killer application weâ€™re looking for.Â  . Could the nature of quantum mechanics be such that exponential speedups are only found in codebreaking, chemistry, and a bunch of highly artificial toy problems, but nowhere else in the broad spectrum of practically relevant challenges? Most people would argue that such a scenario is unlikely. There are still good hopes that either some of the caveats with existing algorithms will be addressed or that new breakthrough algorithms will be discovered. How optimistic you are about quantum computing should depend on (at least) the following questions: . -Â Â Â Â Â Â Â  How impactful will heuristic and to-be-discovered algorithms be compared to classical algorithms? In other words, what is the algorithmic potential of quantum computing? . -Â Â Â Â Â Â Â  How will quantum hardware develop relative to classical hardware? . Ultimately, the commercial success of quantum computers depends strongly on these questions. If we allow ourselves to do some more hypothetical dreaming, I picture that the following future scenarios could be possible, on a spectrum of optimism versus pessimism: . Starting on the pessimistic side, if one believes that optimisation algorithms turn out to be lacklustre, then quantum computing might remain a niche for academics. However, depending on the utility of more widely applicable algorithms, it could be that quantum computers will be installed in special-purpose computing facilities or, even more optimistically, that they become increasingly common additions to data centres (much like GPUs today). Where would you place yourself in this figure? . | https://quantumalgorithmzoo.org/Â &#8617; . | Harrow, Aram W; Hassidim, Avinatan; Lloyd, Seth (2008). â€œQuantum algorithm for linear systems of equationsâ€. Physical Review Letters. 103 (15) 150502. https://doi.org/10.1103/PhysRevLett.103.150502Â &#8617; . | Aaronson, S. Read the fine print. Nature Phys 11, 291â€“293 (2015). https://doi.org/10.1038/nphys3272Â &#8617; . | Liu, Y., Arunachalam, S. &amp; Temme, K. A rigorous and robust quantum speed-up in supervised machine learning. Nat. Phys. 17, 1013â€“1017 (2021). https://doi.org/10.1038/s41567-021-01287-zÂ &#8617; . | https://medium.com/qiskit/how-ewin-tangs-dequantized-algorithms-are-helping-quantum-algorithm-researchers-3821d3e29c65Â &#8617; . | With the symbol \\(\\sim\\) we mean â€˜roughly proportional toâ€™. It essentially allows us to write down an approximation of a function, making them easier to read, throwing away some details are not important here.Â &#8617; . | Â You may find even sources stating that Shorâ€™s algorithm takes a time proportional to n2Â log(n). Such scaling is theoretically possible but relies onÂ asymptotic optimizationsÂ that are unlikely to be used in practice.Â &#8617; . | Han-Sen Zhong et al., Quantum computational advantage using photons. Science 370, 1460-1463 (2020). https://doi.org/10.1126/science.abe8770Â &#8617; . | Arute, F., Arya, K., Babbush, R. et al. Quantum supremacy using a programmable superconducting processor. Nature 574, 505â€“510 (2019). https://doi.org/10.1038/s41586-019-1666-5Â &#8617; . | Technically, IBM has a subtly different interpretation. In a blog post (see https://www.ibm.com/quantum/blog/what-is-quantum-utlity), they define â€˜utilityâ€™ as: â€œQuantum computation that provides reliable, accurate solutions to problems that are beyond the reach of brute force classical computing methods, and which are otherwise only accessible to classical approximation methods.â€ In other words: a quantum computer doesnâ€™t have to outperform any classical algorithm, it merely has to compete with the silly approach of brute-force search â€“ which is almost never the best algorithm in practise. This definition seems heavily focused on claiming utility as soon as possible. Nevertheless, if we look at the big picture, we seem to have a similar notion of â€˜advantage for end-usersâ€™ in mind, so Iâ€™m happy to adopt the term â€˜utilityâ€™ anyway.Â &#8617; . | Kim, Y., Eddins, A., Anand, S. et al. Evidence for the utility of quantum computing before fault tolerance. Nature 618, 500â€“505 (2023). https://doi.org/10.1038/s41586-023-06096-3Â &#8617; . | Tomislav BeguÅ¡iÄ‡, Garnet Kin-Lic Chan, Fast classical simulation of evidence for the utility of quantum computing before fault tolerance (2023). https://doi.org/10.48550/arXiv.2306.16372Â &#8617; . | Joseph Tindall et al., Efficient Tensor Network Simulation of IBMâ€™s Eagle Kicked Ising Experiment, PRX Quantum 5, 010308 (2023), https://doi.org/10.48550/arXiv.2306.16372Â &#8617; . | . ",
    "url": "/essentials/applications-overview/#where-is-the-killer-application",
    
    "relUrl": "/essentials/applications-overview/#where-is-the-killer-application"
  },"7": {
    "doc": "The applications: what problems will we solve?",
    "title": "The applications: what problems will we solve?",
    "content": " ",
    "url": "/essentials/applications-overview/",
    
    "relUrl": "/essentials/applications-overview/"
  },"8": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "The background: why are we so enthusiastic about Quantum Technology?",
    "content": "At a glance Quantum technology is an umbrella term for devices that exploit quantum phenomena like superposition and entanglement. The most notable innovations are expected in computers, networks, sensors, and simulators. Quantum computers will be much slower than conventional computers. Their advantage comes from the ability to run quantum algorithms, which solve specific problems in much fewer steps. ",
    "url": "/essentials/background/#the-background-why-are-we-so-enthusiastic-about-quantum-technology",
    
    "relUrl": "/essentials/background/#the-background-why-are-we-so-enthusiastic-about-quantum-technology"
  },"9": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "What is quantum technology?",
    "content": "Quantum physics, the rules that dictate the behaviour of the tiniest particles, has already proven itself as an invaluable basis for new technologies. Without this scientific theory, many invaluable tools like LED lighting, MRI scanners and solar cells may not have been invented. And itâ€™s still relevant to push the limits of innovation, withÂ nano-size vehicles that consist of just a few atoms, orÂ ever-smaller transistors on computer chips on the horizon. Just ahead of us is a new paradigm, which weâ€™ll call quantum technology. The distinguishing factor is that it goes further than merely building stuff from small particles. Quantum technology is about devices that perform certain processes in a fundamentally different way. That is, the data (or operations) we work with can have special properties unique to quantum physics, such as superposition and entanglement. In our jargon, we will refer to â€˜classicalâ€™ technology for devices that donâ€™t carefully exploit the possibilities of quantum physics â€“ they are based on â€˜classicalâ€™ physics that weâ€™re used to from high school. Your laptop and phone are examples of classical computers, and theyâ€™re connected to the classical internet. The internal transistors and electrical circuits might be so tiny that quantum physics is relevant there, but the fundamental point is that the information that they process is purely classical. Whereas classical computers work with â€˜bitsâ€™, quantum technology will need a different type of information carrier that that itself can be controlled at a quantum-mechanical level. Weâ€™ll call these objects â€˜quantum bitsâ€™, or shortly â€˜qubitsâ€™. Generally, under the nomer ofÂ quantum technology, we distinguish four categories: . | Quantum computers are devices that use quantum physics to perform automatic calculations to solve a problem. Computing is considered the most impactful application for most organisations, and therefore, itâ€™s the main focus of this book.Â  . | Quantum networks are connections between quantum devices over whichÂ qubits (or similar forms of quantum data)Â can be transmitted. The most relevant use case is to strengthen cryptography used by classical computers, but there are many more applications. | Quantum sensors are devices that exploit the effects of quantum physics to accurately measure certain quantities, such as a magnetic field or the strength of the earthâ€™s gravity. Quantum clocks also fall into this category.Â  . | Quantum simulators are devices similar to quantum computers, except that they specialise in solving a limited set of problems. Typically, they are built to reproduce the behaviour of atoms and electrons in a molecule or a piece of material, allowing us to measure properties like energies and reaction rates. | . Each of these categories accomplishes a different goal or functionality. For now, weâ€™ll remain agnostic about how they are built â€“ it will be a task for hardware engineers to figure out how our desired functionality is best implemented. However, since all of these have to deal with quantum-mechanical processes under the hood, it is not uncommon that they use similar building blocks. In this book, we mainly focus onÂ computersÂ andÂ networks, because these seem to have the biggest impact on typical (business) users. ",
    "url": "/essentials/background/#what-is-quantum-technology",
    
    "relUrl": "/essentials/background/#what-is-quantum-technology"
  },"10": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "The importance of high-performance computing",
    "content": "The abundance of cheap computational power has given humanity incredible wealth. We automated the most tedious tasks to free up time for leisure and to solve other urgent problems. It allowed us to scale factories, supply chains and logistics to unprecedented sizes, allowing us to transport resources around the globe at minimal costs. Computer chips, aeroplane wings, heart monitors, and LCD screens have improved with every generation. Today, our computers are already incredibly fast. In fact, for many applications, there is little economic gain in making these computers even faster. Decades-old machines can successfully oversee factory operations, and writing a text document or scheduling a meeting with 8 busy colleagues is not limited by the speed of your computer in any way. However, this book specifically is about the applications where we are still hungry for more computational power. For example, by feeding more data into weather models, our forecasts can become more accurate. If staff rostering would take less time, we could take better last-minute changes into account. Accurate predictions of drug reactivity in the human body could save on costly medical trials and reduce the time to market. Machine learning models like ChatGPT are still demanding more training hours to produce more sophisticated results. It should be clear that weâ€™re not talking about computations that happen on your laptop. Weâ€™re thinking of problems where somehow thereâ€™s value in investing in the fastest possible computers on earth: high-performance computing (HPC), colloquially called supercomputers. Merely looking at the market, there seems to be incredible value in computing stuff: companies and academics spend tens of billions of dollars on them1, and hardware suppliers like Nvidia have rapidly grown to become among the most valuable companies on earth. The kinds of problems that are now being crunched in HPC data centres are also the ones for which a quantum treatment could have the largest impact. ",
    "url": "/essentials/background/#the-importance-of-high-performance-computing",
    
    "relUrl": "/essentials/background/#the-importance-of-high-performance-computing"
  },"11": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "Why can quantum computers have an advantage?Â ",
    "content": "A naive view of quantum computers is that theyâ€™re simplyÂ fasterÂ than their conventional cousins. Or perhaps one may naively point at Mooreâ€™s Law: with transistors reaching atomic scales, we run into quantum effects, so quantum physics may help us make better chips. However, none of these are our core motivations for looking at quantum computers.Â  . Firstly,Â it seems unlikely that quantum computers will catch up with classical machines in terms of raw clock speed any time soon, partially because the speed of a modern CPU is already spectacular. A modern desktop processor, or even the one in your phone, works at a rate of several GHz, that is, several billions of steps per second. In each of these steps, a broad palette of operations can be applied to astronomically large numbers â€“ modern chips work with 64-bit values, meaning that numbers up to 18,446,744,073,709,551,615 can be processed. Each of these elementary steps can be something like addition, multiplication, a comparison, etcetera, and we have powerful tools to weave these basic operations together to form efficient software. Now, quantum computers are supposed to be even faster, right? Well, itâ€™s not hard to find backing for that claim:Â  . Figure: News headers by Techradar and IFLScience. Sources: https://www.techradar.com/news/google-creates-quantum-chip-millions-of-times-faster-than-the-fastest-supercomputer ; https://www.iflscience.com/chinese-scientists-create-quantum-processor-60000-times-faster-than-current-supercomputers-61475 . You may be disappointed to hear that quantum computers, at this moment, cannot even add or multiply numbers of more than 3 or 4 bits. And even if they could, their rate of operation would by no means reach several GHz, but more likely several MHz (a few million operations per second) at best. In other words, theyâ€™re more than a thousand timesÂ slower.Â To make things worse, the information in quantum computers is extremely fragile and needs to be constantly checked and corrected using so-calledÂ error correction.Â This is a form of overhead that could make quantum computers another several orders of magnitude slower. Even in the far future, when quantum computers are more mature and more reliable, we still expect them to be much slower than the classical chips at that time.Â  . How does this rhyme with the news about ever-faster quantum computers? And why are we still interested in these slow machines? As we claimed before, we hope to do certain computations in aÂ fundamentally different way. Letâ€™s look at a beautiful analogy that Andy Matuschak and Michael Nielsen bring up in their online course Quantum.country.Â  . Imagine that youâ€™d like to travel from Morocco to Spain, which are separated by a small piece of sea called the Strait of Gibraltar. If your technology does not allow you to cross the sea (say, you have access to a car but not a boat), then youâ€™d need to take a large detour, all the way through North Africa, past the Arabian Peninsula, and through Europe, before you can reach your destination. This represents the steps taken by a classical computer. In the same analogy, a quantum computer grants you the ability to traverse both land and sea (much like a hovercraft) so that you can take a much more direct route. The beauty of quantum computation is that we have a fundamentally different way to travel (do computations), which can sometimes bring us to our destination using a shorter route (doing fewer computational steps). Even with a much slower vehicle (computer), one may arrive at the destination sooner. In fact, the quantum advantage often grows as problems become larger and more complicated. The analogy also shows that quantum computers do not always have an advantage: you would not want to travel from Amsterdam to Berlin by hovercraft. Unfortunately, in many cases, we donâ€™t yet know what the fastest means of transportation is. It is still an active area of research to completely map out the landscape over which quantum and classical computers can travel and to determine which problems can be sped up or not. For this reason, we donâ€™t expect that classical computers will be replaced any time soon. Instead, classical and quantum processors will live side by side, and programmers will pick whichever tool is better suited to solve a certain problem. The situation could be similar to how we use graphical processing units (GPUs) today, which offer tremendous speedups for the training of AI models but are not made to replace regular classical processors (CPUs). Perhaps we should even give quantum computers a similar name, like â€œQPUâ€ for Quantum Processing Unit. In the analogy with the Strait of Gibraltar, the precise route that you travel denotes the chosenÂ algorithm.Â In the field of computer science, an algorithm is aÂ step-by-step list of instructionsÂ that describes how a computational problem should be solved. TheÂ â€˜stepsâ€™Â here should be sufficiently simple so that it is completely unambiguously how to do them. They could be operations such as adding, multiplying, or comparing two numbers. Needless the say, the fewer steps the algorithm requires, the better. By exploiting quantum mechanics, a quantum computer introduces new basic steps (like crossing the sea) that are impossible to perform on a classical computer. For example, the previous chapter introduced quantum logic gates that generalise operations like AND and OR. Using these building blocks, we can formulate quantum algorithms that take fewer steps than the best classical algorithm ever could! . In the end, the time needed to solve a problem can be very roughly calculated as: \\(\\text{ }{\\text{Time to solve a problem} = \\text{time per step}\\ \\times \\text{number of steps required}}\\) . The â€˜time per stepâ€™ is a property of the hardware that you use. Clearly, a faster CPU will lead to faster solutions. The â€˜number of steps requiredâ€™ is dictated by the algorithm. The latter is precisely how quantum computers can offer spectacular speedups. As long as the improvement in the â€˜number of steps requiredâ€™ compensates for the disadvantage in â€˜time per stepâ€™, a quantum computer can help us solve problems in less time! . A recurring theme in this book is the search for industrially relevant quantum algorithms. This turns out to be more challenging than it seems at first sight. Quantum algorithms are built on deep and complex mathematics, rely on counter-intuitive quantum phenomena, and require inventive new methods to tackle a problem. Simple tweaks to existing classical algorithms are rarely sufficient. In fact, for most problems, no quantum speedups have been identified yet at all, despite the best attempts by scientists worldwide. We might go as far as to say that, even if we had a large-scale quantum computer today, its value would be limited. For this reason, the ongoing development of novel algorithms is exceedingly important. ",
    "url": "/essentials/background/#why-can-quantum-computers-have-an-advantage",
    
    "relUrl": "/essentials/background/#why-can-quantum-computers-have-an-advantage"
  },"12": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "From algorithm to software",
    "content": "In the end, simply finding a good algorithm is not enough: it has to be turned into software, a piece of language that explicitly tells a computer how to execute the step-by-step instructions. The difference between â€˜algorithmsâ€™ and â€˜softwareâ€™ is subtle. An algorithm is a purely mathematical description that describes precisely how numbers should be manipulated. It could tell which two numbers must be multiplied, what function must be evaluated, or how an image must be transformed. However, different computers can use different types of processors and memory, and an algorithm does not describe how these operations are done on a specific computer. This is where software comes into play. It describes precisely what hardware operation must be called, where each number is stored in memory, and how an image is represented in binary. As an analogy, you may think of the algorithm as a recipe to bake the perfect chocolate cookie. The algorithm should unambiguously describe what should happen to the ingredients: in what order they should be mixed, how long they should be heated at what temperature, etcetera. However, to build a factory that produces these cookies, you need to be even more specific: Where is the sugar stored? Out of what pipe does the dough flow? How are cookies laid next to each other in the oven? . Fundamentally, core scientific breakthroughs come from finding new algorithms. Once a new algorithm is found, it can be re-used many different times on any capable machine (assuming a good software developer will turn it into appropriate code!). In this book, we care less about quantum software and more about quantum algorithms. Firstly, the algorithms tell us precisely the functionality that quantum computers can offer. Moreover, we donâ€™t yet know how a mature quantum computer will be programmed or how quantum hardware and software will change in the following years. On the other hand, once a new algorithm is found, it can be cherished forever. Now that we have come to appreciate algorithms, it is natural to ask which quantum algorithms we know of. What problems do quantum computers solve well? And how do these algorithms compare to their classical equivalents? This will be the topic of the next chapter. ",
    "url": "/essentials/background/#from-algorithm-to-software",
    
    "relUrl": "/essentials/background/#from-algorithm-to-software"
  },"13": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "Further reading",
    "content": ". | The Map of Quantum Computing (Youtube)Â  â€“ A 30-minute video that forms a great supplement to this book.Â  . | Chris Ferrieâ€™s book â€˜What You Shouldnâ€™t Know About Quantum Computersâ€™ debunks several myths about quantum computers, presented in a very accessible way. | Are you looking for a much more extensive source that covers pretty much everything there is to know about quantum computers? French consultant Olivier Ezratti maintains a 1300+ page book â€œUnderstanding Quantum Technologiesâ€. | . | See e.g. https://www.marketsandmarkets.com/Market-Reports/Quantum-High-Performance-Computing-Market-631.html and https://www.mordorintelligence.com/industry-reports/cloud-high-performance-computing-hpc-marketÂ &#8617; . | . ",
    "url": "/essentials/background/#further-reading",
    
    "relUrl": "/essentials/background/#further-reading"
  },"14": {
    "doc": "The background: why are we so enthusiastic about quantum?",
    "title": "The background: why are we so enthusiastic about quantum?",
    "content": " ",
    "url": "/essentials/background/",
    
    "relUrl": "/essentials/background/"
  },"15": {
    "doc": "Four myths about quantum computing",
    "title": "Four myths about quantum computing",
    "content": " ",
    "url": "/essentials/myths/",
    
    "relUrl": "/essentials/myths/"
  },"16": {
    "doc": "Four myths about quantum computing",
    "title": "Contents",
    "content": ". | Four myths about quantum computing . | Myth 1: Quantum computers can try a huge number of solutions at once | Myth 2: Qubits can store much more data than the same number of classical bits. | Myth 3: Entanglement allows you to send information faster than light or to influence objects at a distance | Myth 4: Quantum computers are always ten years away. | Further reading | . | . ",
    "url": "/essentials/myths/#contents",
    
    "relUrl": "/essentials/myths/#contents"
  },"17": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 1: Quantum computers can try a huge number of solutions at once",
    "content": "This myth derives from the concept of superposition: if a qubit can represent the numbers 0 and 1 at the same time, then a mere 1000 qubits can represent \\(2^{1000}\\) unique numbers, all at the same time. Thatâ€™s an incomprehensibly large quantity of numbers that are represented at the same time, much more than there are atoms in the visible universe â€“ even the fastest computers in the world couldnâ€™t loop through all these numbers in a lifetime. Remember that all of these numbers, as stored in a quantum memory, can also be interpreted differently, perhaps as different Excel files, webpages, CAD drawings, or whatever kind of data we choose to work with. A smart computer scientist can devise a way to make 1000 bits represent â€˜solutionsâ€™ to some problem. For example, imagine that we want to find an optimal aeroplane wing that generates incredible lift while requiring as few materials as possible. Using quantum superposition, we might represent \\(2^{1000}\\) such wings all at once. We picked the example of aeroplane wings because it is clear that simulating its aerodynamic properties requires a pretty hefty computation. Letâ€™s assume that we have written such a computer program that accurately simulates any wing, and call that program \\(f\\). It will output 1 if the wing works well (according to whatever metric), and 0 otherwise. Surely, the program takes a very large number of computation steps, which weâ€™ll call N. The program will need some input, denoted by \\(x\\), which is a 1000-bit description of all the relevant properties of a hypothetical aeroplane wing. In other words, the compute program computes \\(f(x) = 1\\) if \\(x\\) is a fantastic wing, and \\(f(x) = 0\\) if itâ€™s rubbish. Now, a quantum computer should be able to execute any classical function, right? We should be able to run \\(f\\) on a quantum computer, but now we have the unique feature that the 1000-qubit input can actually represent a humongous number of potential aeroplane wings at the same time! By doing mere N computational steps, we can check the properties of \\(2^{1000}\\) solutions! . If this actually worked, quantum computers would have an astonishing power. They could straightforwardly find mathematical proofs that humans havenâ€™t been able to solve in centuries. They would rapidly produce the perfect train and bus schedules, discover new drugs and straightforwardly hack encryption systems. They would solve problems in the complexity class NP, which is widely believed to be impossible with machines in our universe, owing to the famous P â‰  NP conjecture. So whereâ€™s the catch? For those who read the introduction to quantum physics, we shouldnâ€™t forget about the postulate of quantum measurement. The output of the computation would be a superposition over \\(2^{1000}\\) outcomes. If we want to learn anything about this output, weâ€™d perform a quantum measurement that collapses this superposition. Instead of looking at \\(2^{1000}\\) different solutions simultaneously, we get to see only 1 outcome â€“ corresponding to the performance of just a random aeroplane wing. In this case, there is no advantage compared to a classical computer, because we couldâ€™ve just as well picked a random wing at first, and then spent N steps on a (much faster) classical machine. Although this â€˜quantum parallelismâ€™ is too good to be true, quantum computers can use the above idea to some lesser extent. Using Groverâ€™s algorithm, we can find desirable solutions (the \\(x\\) for which \\(f(x) = 1\\)) in roughly the square root of the number of values that \\(x\\) can take. In the above example, the number of required steps is reduced to \\(\\sqrt{2^{1000}}\\ N = 2^{500}\\ N\\). This is an incredible reduction, but weâ€™re still looking at a number of steps larger than the number of atoms in the universe â€“ which is far from â€˜efficientâ€™. ",
    "url": "/essentials/myths/#myth-1-quantum-computers-can-try-a-huge-number-of-solutions-at-once",
    
    "relUrl": "/essentials/myths/#myth-1-quantum-computers-can-try-a-huge-number-of-solutions-at-once"
  },"18": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 2: Qubits can store much more data than the same number of classical bits.",
    "content": "This myth is very similar to the previous one: canâ€™t N qubits represent \\(2^{N}\\)different numbers at the same time? Or arenâ€™t they perhaps even more powerful, because for each of the \\(2^{N}\\) different numbers, thereâ€™s a complex number, which can have as many decimal digits as we like? . Again, by the rules of quantum measurement, this is too good to be true. Itâ€™s impossible to store much information in a qubit because it collapses to a classical 0 or 1 when we measure it. The problem is really in retrieving the information, where we have very limited capabilities. For the same reason, when sending a classical message over a long distance, thereâ€™s little value in using qubits as information carriers. As a side note, there is a fascinating related protocol called â€˜superdense codingâ€™ that you may like to look up out of theoretical interest. Also, when your data itself represents something quantum (for example, the state of electrons in a molecule), then storing this data in qubits does have a potentially huge advantage. ",
    "url": "/essentials/myths/#myth-2-qubits-can-store-much-more-data-than-the-same-number-of-classical-bits",
    
    "relUrl": "/essentials/myths/#myth-2-qubits-can-store-much-more-data-than-the-same-number-of-classical-bits"
  },"19": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 3: Entanglement allows you to send information faster than light or to influence objects at a distance",
    "content": "Entanglement is an incredibly confusing phenomenon. In particular, our most common interpretation of quantum mechanics states that whenever we measure one qubit, the state of another distant qubit can drastically change. Whilst this picture is helpful for physicists when performing computations, it tricks our intuition. Imagine that, in the faraway future, we want to protect our solar system against an alien invasion. We installed sentinels on faraway outposts, who should signal Earth of any approaching dangers. Alice is one of these noble guards, stationed at a remote asteroid in the icy Kuiper Belt. She brought with her a qubit labeled A, which is entangled with another qubit B thatâ€™s safely kept on earth by her colleague Bob. Whilst it takes light signals around 5 hours to travel between them, isnâ€™t there a way for Alice to alarm Bob any faster, possibly by doing some special operations on her qubit? Perhaps she could even give some clues about the type of looming threat? . Unfortunately, Alice cannot remotely change any measurable quantity of Bobâ€™s qubit. Bobâ€™s measurements will always have the same outcome probabilities, no matter what Alice does to her qubit. Using more qubits, or employing different quantum objects wonâ€™t help either. Fundamentally, there is no way to signal any information faster than the speed of light. There is a subtle difference between â€œchanging measurable quantitiesâ€ and â€œknowing somethingâ€ about the state of a particle. To illustrate, assume that we start with a particular entangled state: measuring qubits A and B will result either in both qubits being â€œ0â€ or both qubits being â€œ1â€, letâ€™s say with 50% probability each. Measuring something like A= â€œ0â€ and B= â€œ1â€ is impossible. When Alice measures her qubit and reads the outcome â€œ0â€, she immediately knows the outcome of a future measurement made by Bob: she knows this will be â€œ0â€ with 100% probability. However, this knowledge is not accessible to Bob. He doesnâ€™t even know whether Alice measured or not! Even if they agreed in advance that Alice would measure at a set time, Bob doesnâ€™t know her outcome. From his perspective, â€œ0â€ or â€œ1â€ are still equally likely. Something interesting happens when Alice sends a message to Bob to inform him that her measurement returned â€œ0â€. With this updated knowledge, Bob suddenly knows precisely what the state of his qubit is: it must have collapsed to â€œ0â€, and he can perfectly predict the outcome of a subsequent measurement. In a way, this did indeed change the state of the qubit from Bobâ€™s perspective, but it was only possible after some (classical) communication took place between Alice to Bob, a process that is limited by the speed of light. What is quantum entanglement good for, then? Some potential applications include: . -Â Â Â Â Â Â Â Â Â Â Â Â Â  Creating certifiably secure encryption keys at remote locations. -Â Â Â Â Â Â Â Â Â Â Â Â Â  Creating certifiable randomness. -Â Â Â Â Â Â Â Â Â Â Â Â Â  Forming connections between separate quantum computers, allowing them to send quantum data to each other using teleportation. For this to work, devices also need to transfer some classical data, so qubit transmission is never faster than the speed of light. Teleportation is an intriguing method to scale up quantum computers when a limited number of qubits can fit on a single chip or within a single fridge. ",
    "url": "/essentials/myths/#myth-3-entanglement-allows-you-to-send-information-faster-than-light-or-to-influence-objects-at-a-distance",
    
    "relUrl": "/essentials/myths/#myth-3-entanglement-allows-you-to-send-information-faster-than-light-or-to-influence-objects-at-a-distance"
  },"20": {
    "doc": "Four myths about quantum computing",
    "title": "Myth 4: Quantum computers are always ten years away.",
    "content": "This statement is a playful reference to the situation around nuclear fusion, where predictions of its realisation being just 30 years in the future have repeatedly been postponed. Scientists have been working on fusion for decades, but itâ€™s still far from a mature energy source. Similarly, Iâ€™ve heard several overly optimistic claims about quantum computers being made in the past 10 years, often claiming that quantum computers are somewhere between 3 to 10 years away. An article by TechCrunch1 boldly paraphrases Dario Gil (IBM) and Chad Rigetti (founder of Rigetti Computing) saying that â€œthe moment that a quantum computer will be able to perform operations better than a classical computer is only three years awayâ€, whilst this article was published back in 2018. For reference, the 127-qubit Eagle chip was announced by IBM at the end of 2021, but even several years later, itâ€™s still primarily used for testing and education. In 2019, consulting firm Gartner published â€œThe CIOâ€™s Guide to Quantum Computingâ€ which indicates that 100â€”200 qubits are sufficient for â€œkey potential applicationsâ€ in chemistry. They also predicted that â€œby 2023, 20% of organisations will be budgeting for quantum computing projectsâ€. Clearly, these predictions were overly optimistic. Similarly, Microsoft made claims in 2018 that their cloud platform Azure would feature quantum computing in 5 years2, which is technically true. However, they have repeatedly hinted to do this with fault-tolerant topological qubits, which still remain elusive. Startup PsiQuantum famously claimed to have a million photonic qubits by 20253, and consultants at BCG advised that quantum computers â€œgenerate business valueâ€ in the same year4. Again, it remains to be seen if this holds true. Luckily, if youâ€™re reading this book, you must have noticed that not all experts share the same vision. Most scientists have warned for a long time that quantum computing is a long-term effort. Nevertheless, the thesis that â€˜quantum computing is always X years awayâ€™ is hard to defend, thanks to convincing evidence that we are steadily progressing towards a clear goal. Every year, quantum hardware sees major improvements in the number of qubits, their stability, and the level of control that is demonstrated. Most experts even expect an exponential scaling of the number of qubits, similar to Mooreâ€™s Law, and manufacturers have clear roadmaps that underline these predictions. Moreover, theorists have set clear targets for when the hardware is good enoughâ€”and weâ€™d sooner see the requirements drop with new breakthroughs rather than become more stringent. Building a quantum computer is a long marathon, and itâ€™s impossible to predict when they become commercially relevant, but the rapid rate of progress is undeniable. ",
    "url": "/essentials/myths/#myth-4-quantum-computers-are-always-ten-years-away",
    
    "relUrl": "/essentials/myths/#myth-4-quantum-computers-are-always-ten-years-away"
  },"21": {
    "doc": "Four myths about quantum computing",
    "title": "Further reading",
    "content": ". | Veritasium explains Entanglement . | (Technical) Minute Physics explains Teleportation . | Chris Ferrie debunks more myths in his free book â€œWhat you shouldnâ€™t know about Quantum Computersâ€ . | . | https://techcrunch.com/2018/09/07/the-reality-of-quantum-computing-could-be-just-three-years-away/Â &#8617; . | https://www.computerweekly.com/news/252440763/Microsoft-predicts-five-year-wait-for-quantum-computing-in-AzureÂ &#8617; . | https://www.ft.com/content/a5af3039-abbf-4b25-92e2-c40e5957c8cdÂ &#8617; . | https://www.bcg.com/publications/2023/enterprise-grade-quantum-computing-almost-readyÂ &#8617; . | . ",
    "url": "/essentials/myths/#further-reading",
    
    "relUrl": "/essentials/myths/#further-reading"
  },"22": {
    "doc": "Preface: why this book?",
    "title": "Preface: Why this book?",
    "content": ". â€œQuantum computing will change everything,â€ the man in front of me said. Standing tall and confident, he took another sip of his drink before continuing, â€œIt will be the biggest revolution since the invention of the transistor. Imagine a world where we can cure any disease with personalised medicine. A world where new energy sources will free us from our dependence on fossil fuels. Not to mention thatâ€¦â€ . â€œWellâ€”â€ I tried to interrupt, but the man passionately rattled on. â€œIt will finally enable us to build general Artificial Intelligence that can take over our tedious everyday jobs, so 95% of our population no longer has to work!â€ . â€œYou know that quantum computers are still quite some years away, right?â€, I countered. He leaned in, eyes still gleaming with excitement. â€œThatâ€™s what most people think. But the reality is, weâ€™re closer than ever. Quantum supremacy has already been achieved. Google did it in 2019; since then, progress has been exponential. Did you see the presentation by that guy from Goldman Sachs? Their investments already see higher returns than ever since their new Monte Carlo algorithm.â€ . The above conversation captures a feeling that many seasoned experts in quantum computing will have. And this is by no means exaggerated. Plentiful reputable sources report that â€˜quantumâ€™ is key in tackling climate change, revolutionising AI, and building unhackable networks. Experts who are actually building these quantum computers are much, much more reluctant. At an academic conference, you hear a completely different story. Scientists ridicule the absurd claims that some consultants and startups make. They will point out that the applications of quantum computers are still very much uncertain, and that weâ€™re still searching for convincing uses cases. The quantum scene seems to have two completely separate worlds. A business world, that reaches out to anybody who will hear them about the unprecedented capabilities of quantum computers. And the academic world, the community of experts that quietly bring this quantum computer to reality, sharing their results in technical papers that require a PhD to understand. I was grasped by this paradoxical situation. Who is right? How powerful are these quantum computers really, and how do they compare to existing technologies? In what year will we have a large-scale quantum computer, and what will it look like? These are billion-dollar questions, but the answers will wildly vary, depending on who you ask. After searching for these answers for a decade, Iâ€™ve finally found quite a unique position to answer most of these questions for myself. I have the luxury of being a native in both the academic and the commercial world. Iâ€™ve seen all the arguments from both sides and can cut through dishonest and deceptive claims. I have the privilege of being surrounded by the worldâ€™s most renowned experts from both worlds, and I have heard their unsalted opinions and predictions that they may not readily throw out publicly. And most of all, after training many new colleagues and setting up professional learning programs, I developed a good intuition about what newcomers want to know about quantum technology, and how to explain it in an accessible way. However, the definitive motivation to write this book came primarily from two external factors. First, like many others in this field, I feel uncomfortable with the many hyped and unbalanced articles that would otherwise populate the top entries in Google search results (or even the New York Times best-selling books1). Second, I see the need for an authoritative source of information that others can reference when disagreeing about facts or debunking myths. I am already very grateful for the many colleagues who frequently refer to an early version of this book. That doesnâ€™t mean that this book contains only confirmed facts â€“ not at all! Writing about a computer of the future comes with mountains of uncertainty. In 2005, nobody could have predicted that a mere 5 years ahead, everyone would be playing games and consuming the internet on their smartphones. In 2015, nobody could have predicted the impact of Large Language Models like ChatGPT. And indeed, todayâ€™s best predictions of a future quantum revolution wonâ€™t be quite so accurate either. Even worse, experts wildly disagree in several cases. For example, the usefulness of quantum AI and optimisation is vigorously disputed, and the rate at which hardware will progress depends on many yet-to-discover breakthroughs. The best I could do is describe various perspectives on these matters and highlight the best arguments from either side. Without plentiful discussions and disagreements, I wouldnâ€™t have been able to gather the facts and opinions in this book. And it shouldnâ€™t stop there. I keep welcoming criticism, opinions, and feedback about these complex topics, aiming to refine these texts even more in future updates. Even though much is still uncertain, I think that a reliable indication of the prospects of quantum computing is more important than ever. Quantum startups are acquiring huge investments, allowing them to hire managers, software developers, salesmen, and marketers. Governments need informed policymakers, and journalists should cover quantum breakthroughs. Pretty much every organisation that deals with IT will want to keep a close eye on the impact that â€˜quantumâ€™ will have on them. This book is for precisely these people who donâ€™t need to understand all the technical details but still need to talk, read, and write about quantum technologies. This is why I will focus on an accessible language everyone can understand. We donâ€™t care so much about the underlying math or physics, but rather about the functionality of a quantum computer: the opportunities, the threats, and the concrete actions organisations can take. How should you read this book? I chose to split the content into three parts. The first part contains the essentials that we recommend everyone read. This is an incredibly efficient way to learn all the background that you need â€“ you should be ready to understand other sources and have some depth in professional discussions or meetings. To get into more detail, parts two and three contain more information about the (software) applications and about the (hardware) devices, respectively. A final fourth part is reserved for additional resources that can be useful or fun when continuing your quantum journey. | Iâ€™m refering to Michio Kakuâ€™s book â€œQuantum Supremacyâ€, but before you even consider reading it, you might like to see the book review by a professor in quantum computer science (https://scottaaronson.blog/?p=7321).Â &#8617; . | . ",
    "url": "/essentials/preface/#preface-why-this-book",
    
    "relUrl": "/essentials/preface/#preface-why-this-book"
  },"23": {
    "doc": "Preface: why this book?",
    "title": "Preface: why this book?",
    "content": " ",
    "url": "/essentials/preface/",
    
    "relUrl": "/essentials/preface/"
  },"24": {
    "doc": "An introduction to the quantum world",
    "title": "An introduction to the quantum world",
    "content": "At a glance You donâ€™t need to understand quantum mechanics to understand the functionality of quantum computers. But if you insist, quantum mechanics describes the behaviour of the smallest particles. It leads to many counter-intuitive phenomena: computer memory can store multiple pieces of data at the same time, but upon measurement, nature selects just a single piece (and throws away all the others). If you want to drive a car, do you need to understand how its engine works? Of course you donâ€™t! In a similar vein, you donâ€™t need to know the details of quantum physics to read the rest of this book. So feel free to skip this chapter. Nevertheless, I know that most people want to have some conceptual intuition about what quantum mechanics really is. It is not natural to leave one of the most used word in this book as an abstract concept, and it might be hard for the human brain to proceed without at least seeing some examples. Here is my best attempt to explain quantum mechanics in accessible terms. Proceed with caution, as things will surely get confusing from here. ",
    "url": "/essentials/quantum/",
    
    "relUrl": "/essentials/quantum/"
  },"25": {
    "doc": "An introduction to the quantum world",
    "title": "What is quantum?Â Â ",
    "content": "Quantum physics or quantum mechanicsÂ is the theory that describes the tiniest particles, like electrons, atoms, and sometimes even small molecules. They are the laws of nature that dictate cause and effect at the scale of nanometers. You may contrast it to Newtonâ€™s classical physics that we teach at high schools, which works great for objects the size of a building or football but becomes inaccurate at much smaller scales. Quantum is, in a sense, aÂ refinementÂ of classical physics: the theories are effectively identical when applied to a coffee mug, but the more difficult quantum theory is needed to describe very small things.Â  . Some examples of systems where quantum could play a role are: . | Atoms, and the electrons that orbit around them. | Flows of electricity in microscopic (nano-scale) wires and chips . | Photons, the particles out of which â€˜lightâ€™ is made. | . To proceed, we need some physics jargon. We like to use the word â€˜stateâ€™, which is a complete description of all the physical properties of the world at one instance: the locations of all the different particle, their velocities, how much they rotate, etcetera. Usually, the entire universe is too big to study, so we often simplify our world to just a single isolated particle, or to a limited piece of computer memory. For example, for a bare particle, we might only imagine weâ€™re only interested in its location, which weâ€™ll call \\(x\\) (and we forget about any other structure in the world). For example, the world might look something the image below, which can be described by a very simple state: \\(x = 5\\) (the ruler is just virtual). In the spirit of computing, we might look at a â€˜bitâ€™ that stores information. You may think of it as a tiny magnet that can either point â€˜upâ€™ (0) or â€˜downâ€™ (1). The state of a piece of memory is easy to describe, simply by stating the bit values on by one. For example: 11010. Importantly, the state of the world can change over time. We will often care about the state of the world at a certain moment, for example at the beginning of computation, or at the end of it. ",
    "url": "/essentials/quantum/#what-is-quantum",
    
    "relUrl": "/essentials/quantum/#what-is-quantum"
  },"26": {
    "doc": "An introduction to the quantum world",
    "title": "Four surprising phenomena",
    "content": "The most iconic quantum phenomenon is superposition. Think about any property that we can (classically) measure, such as the position of a particle, or the value of a bit on a hard drive (0 or 1). In quantum mechanics, state of the world can be such that many different measurement outcomes are somewhat â€˜trueâ€™ at the same time: a particle can be at multiple positions at once, or a bit could be 0 and 1 simultaneously. When we say â€˜at the same timeâ€™ we mean that, to predict any cause and effect, we need to keep track of all these possibilities. To illustrate, I sometimes picture a quantum particle to split into many opaque copies of itself, spread out over space, where the degree of transparency determines how likely the particle is to be found there: the darker, the more likely. How can you possibly describe a state like that? For a single particle, the state is a long list, where for each possible position, we store a number called the amplitude, which is related to how likely the particle is to be found at that location. In other, the state describes precisely to what extent a particle is at position \\(x = 0\\), to what extent at position \\(x = 1\\), and so forth, for every possible location that the particle can be at. And indeed, this list could be infinitely long! Luckily, focusing on computers, we work with simpler objects. A quantum-bit needs just two amplitudes, which denote the extent to which the bit is â€˜0â€™ or â€˜1â€™ respectively. Because we will talk about quantum-mechanical bits a lot, we will give them a shorter name: qubits. If we have a bunch of qubits together, weâ€™ll call it a quantum memory. To throw in some more examples of weird quantum states, an electron can move at a velocity of 10 m/s and 100 m/s at the same time (which obviously also leads to a superposition in its location). More relevant for us: a quantum memory might use binary notation to store numbers 5 and 11 simultaneously, or even 46 different Microsoft Excel spreadsheets at once. These amplitudes feel somewhat analogous to probabilities, which can similarly describe the likelihood that a particle can be found at some location. However, there is a fundamental difference. Probabilities in the classical world help us deal with information we donâ€™t have: surely the particle is already at some location, but perhaps we just donâ€™t know which location yet. Quantum mechanics is different. We might know every tiny detail about to location of a particle, and still we need to describe it as a superposition. Fundamentally, the location is not determined yet. Hence, there is literally no better way to describe the particle than by tracking this convoluted superposition. Amplitudes are also slightly more finicky to deal with than probabilities, because these numbers can become negative (and for math experts, they can even be complex numbers). The second weird phenomenon is how quantum measurements work. Why do we never observe an electron at two places at the same time? Why do I never find a car both moving and standing still? In quantum mechanics, as soon as we measure the location of a particle, it instantly jumps to just a single location at random â€“ making its location fully determined. Similarly, when we measure a qubit, it jumps to either â€˜0â€™ or â€˜1â€™ with some probability. When we measure the data in a quantum memory, we may find any one of the 46 spreadsheets that were stored. This means that the world is intrinsically random (and hence, not deterministic!). But this doesnâ€™t imply that we cannot understand it. We can calculate the probabilities of measurement outcomes with incredible precision as long as we know the state before the measurement. It is important to note that we cannot learn anything about the world without measuring â€“ it is our only way to obtain data from the world, and any process that extracts data must involve a measurement. However, measurements are destructive in the sense that they change the state of the world. In fact, they delete all the rich data encoded in a superposition! If a particle was initially at position \\(x = 0\\),\\(\\ x = 3\\ \\)and \\(x = 10,\\) all simultaneously, then upon measurement, it jumps to one of these three options. In jargon, we call this instantaneous change a â€˜collapseâ€™. From then onwards, it is 100% at a fixed location: if at first we measure the particle to be at \\(x = 3\\), then any subsequence measurement will give the same result, until some other force moves it again. This means that, during a quantum computation, we should carefully choose when we to perform any measurements â€“ we cannot just peek at the data at any moment we like, or we risk disturbing a superposition. Importantly, this also means that a single piece of quantum memory cannot store an immense number of spreadsheets at the same time â€“ at least, you wouldnâ€™t be able to retrieve each of them. To store 15 MB worth of classical data, we need 15 MB worth of â€˜qubitsâ€™. Hence, quantum computers are not particularly useful for storing classical data. The fact that a measurement changes the state of the world poses a serious problem for the engineers who are building quantum computers. No matter what material we construct our qubits from, they will surely interact with other nearby particles, and some of these interactions could be destructive measurements. We call this effect decoherence, and we will later see that this forms one of the core challenges to large-scale quantum computation. At this point, quantum data doesnâ€™t seem particularly useful. Why would we want to deal with superpositions if they lead to all this uncertainty? Intuitively, think about the advantage of a quantum computer in this way: what if the quantum computer can perform certain actions that a classical computer could never do, such as creating certain superposition states? Can these actions somehow help us reach the final outcome of the computation more efficiently? . In jargon, weâ€™re talking about quantum operations. These are physical forces that change the state of a particle or a quantum memory. They can turn a classical-looking state into a quantum superposition or vice versa. They can act like logical operations, such as AND and OR, but also like new quantum â€˜logicâ€™ that has no classical counterpart. In this book, we mainly deal with operations that work on qubits. We will call such operations quantum gates or simply â€˜gatesâ€™. A quantum gate takes one or more qubits as input, changes their internal state, and then outputs the same number of qubits (with their altered states). In other words, the number of physical objects remains unchanged, but the overall state changes. As an example, you may think of our prototypical magnet that was initially pointing â€˜upâ€™, but a quantum gate might flip this to â€˜downâ€™. There are many such gates possible, each having a different effect on their input, we like to give them names in capital letters, such as X, Z, H, and CX. Importantly, a quantum gate is deterministic, meaning that itâ€™s input-output behaviour is always the same, as opposed to the quantum measurements we saw earlier. The canonical way to describe a quantum computer program is by defining a sequence of quantum gates, one after the other, each of which could act on a different set of qubits. At the end of the computation, we measure all qubits. Below, an example of such a sequence is given, using the standard Quantum Assembly (QASM) language. Together, these steps can be graphically displayed in a quantum circuit, as shown here on the right. Quantum circuits represent each qubit with a horizontal line and indicate time flowing from left to right. Whenever a box with a letter is displayed over a qubit line, then the corresponding gate should be applied. This isnâ€™t quite unlike the way we read sheet music! . Note that when we run a circuit on an actual quantum computer, the final measurements lead to probabilistic outcomes. We get to see a bunch of ones and zeroes: one classical bit for each qubit. If the circuit was a â€˜goodâ€™ quantum algorithm, then with high probability, these classical bits will tell us the answer we were looking for. But even then, we might need to redo the computation a few times and take (for example) the most common result as our final answer. If you are completely confused at this point, you are not alone. The whole business of quantum superposition and quantum operations is incredibly complex and is not something you could possibly master after reading a few book pages. Scientists who study the subject for many years are still frequently baffled by deceptive paradoxes and counter-intuitive phenomena. On the other hand, I hope that the functionality of quantum circuits makes a lot of sense: we define a list of instructions, and feed them into a machine that can execute them. We donâ€™t have to know precisely whatâ€™s going on under the hood! . There is one remaining quantum phenomenon to cover â€“ one that comes with a mysterious flair around itself. Weâ€™re talking about quantum entanglement. Imagine that we have two qubits, which we can transport independently from each other without disturbing the data they store. Together, the bits can represent the 00, 01, 10 or 11, or a superposition of these. According to quantum mechanics, we can create a very specific superposition, where the pair of qubits is simultaneously 00 and 11. Now, imagine that computer scientist Alice grabs one of the qubits, takes it on her rocket ship, and flies it all the way to dwarf planet Pluto. The other qubit remains on Earth, in the hands of physicist Bob. Upon arriving on Pluto, Alice measures her qubit and finds outcome â€˜1â€™. A deep question is: what do we now know about Bobâ€™s qubit? . Since the only possible measurement outcomes were 00 and 11, the other qubit can only be measured as â€˜1â€™ from now onwards. It essentially collapses to be 100% in the state â€˜1â€™. But how could the earth-based qubit possibly know that a measurement occurred on Pluto? According to Einsteinâ€™s theory of relativity, information cannot travel faster than the speed of light, which translates into a few hours between Earth and Pluto. Nevertheless, measuring the qubits two faraway locations will always give a consistent result, even when done within the hour. This paradox shows once again how confusing quantum mechanics can be. However, the story above is perfectly consistent with both quantum mechanics and relativity. The core principle is that no information can be sent faster than light between Alice and Bob. For example, can you see why Bob has no way of detecting when Alice performs her measurement, just by looking at his entangled qubit? In the most common interpretation of quantum mechanics, the Earth qubit does indeed change its state instantaneously when Alice measures, although there is no way to exploit this effect. Thereâ€™s a fascinating further discussion about the philosophy behind entanglement, but weâ€™ll leave that to other sources. What matters to us is that distant qubits can still share specific properties that would be impossible to mimic classically, leading to new functionalities we can exploit. We will discover what these functionalities are in the chapter on quantum networks. So there you have it: four surprising phenomena you may hear frequently in quantum technology conversations. To summarise: . | Superposition: the phenomenon where a qubit is both 0 and 1 at the same time. | Quantum measurement: measuring a quantum memory destroys superposition. The result we obtain is probabilistic. | Quantum operations/gates: deterministic changes to the state of qubits, which generalise classical logic gates like OR, AND, NOT. A list of several quantum gates forms a quantum circuit. | Entanglement: Qubits separated over a long distance can still share unique properties. | . ",
    "url": "/essentials/quantum/#four-surprising-phenomena",
    
    "relUrl": "/essentials/quantum/#four-surprising-phenomena"
  },"27": {
    "doc": "An introduction to the quantum world",
    "title": "What does a quantum computer look like?",
    "content": "Most large-scale computing today happens in data centres, where we donâ€™t care much about the specifics of the devices that do our calculations. We also expect that future quantum computers will mostly be tucked away in the â€˜cloudâ€™, making their appearance and inner workings largely irrelevant to most users. However, for this optional chapter, we can provide an idea of what a future quantum computer could look like. There are many different ways to build a quantum computer, each working in vastly different ways. Here, we choose the example of so-called superconducting qubits. We will see that only a tiny part of the computer is actually â€˜quantumâ€™, whereas most of the machine consists of bulky classical machinery thatâ€™s required to keep the computer working. The real quantum magic happens on a chip, not unlike the computer chips used in your laptop or phone. The qubits are formed by tiny electronic circuits where the flow of electrical current is restricted to just one out of two states: the â€˜bitâ€™ states 0 and 1. Since this is a quantum system, the current can also be in a superposition â€“ picture all the electrons in the wire participating both in flow â€˜0â€™ and flow â€˜1â€™ simultaneously! This only works when the chip is cooled down to unimaginably low temperatures, down to around 10 milliKelvin â€“ a hundredth of a degree above absolute zero. At these temperatures, the electronic circuits become superconducting, such that an initial current can flow indefinitely. This is important because any damping would cause unwanted disturbance to the qubit state. Thatâ€™s why the quantum chip is placed in a massive dilution refrigerator, a cylinder of about half a meter in diameter and over a meter tall, which specializes in keeping the quantum chip cool. In the future, larger quantum computers may need even bigger fridges or have several of these close together. Deeper parts of the fridge have different temperatures, allowing us to cool in stages. An example could be to cool a first environment to 35 Kelvin (-283 Â°Celsius or -396.7 Â°Fahrenheit), followed by subsequent stages to ~3K, 900mK, 100mK, until the final stage of ~10mK is reached. One often hangs the fridge on the ceiling so that the higher temperatures are on top and the ultracold quantum chip is placed at the very bottom. The internals are shaped accordingly: several layers of gold disks are suspended below one another, one disk for each temperature zone. A large number of wires run between the disks, transporting signals between the ceiling and the lowermost areas. The whole structure forms the iconic metal chandelier that you often see in images, although it would all be covered by a boring metal case when the fridge is in operation. To make the qubits do something useful, like executing a quantum gate or performing a measurement, we need to send signals into the chip. Just like in classical computers, these signals are technically voltage differences between various wires. Some voltage remain constant over time, others oscillate at microwave frequencies. The wiring itself becomes increasingly challenging for larger quantum computers, for two reasons. Firstly, we currently need around 2-4 wires to control a single qubit, which is problematic when we scale to millions of qubits â€“ itâ€™s impossible to connect that many wires to a tiny chip. Weâ€™ll need to find â€˜multiplexingâ€™ solutions, where a single wire can serve multiple qubits at once. Secondly, many wires connect the ultracold chip to other hardware that sits at room temperature, forming a channel for heat and noise to enter. The dilution fridge circumvents this by incrementally cooling and damping the signals as they travel through the different layers of the fridge, but it can only handle so many cables. Besides the large chandelier, an array of specialized control electronics is needed to produce the necessary electronic pulses and to carefully read out the tiny signals that qubits produce when we measure them. These devices sit in one or multiple electronics racks, each half a meter wide and nearly two meters tall, similar to the ones youâ€™ll find in a typical data centre. Ironically, the actual quantum software can be written on a simple laptop, from where the instructions are passed to the control electronics to run a quantum circuit. The whole situation is reminiscent of computers in the 1940s and 1950s, which similarly occupied a large room and required several engineers for all kinds of labourious manual maintenance tasks. On top of this, the dilution fridges are particularly noise â€“ to the extent that those who operate them ideally do this from a different room â€“ and are fairly power-hungry. The whole quantum computer consumes around 25 kW, comparable to driving an electric car. To summarize, a quantum computer based on superconducting qubits consists mostly of bulky classical stuff: a large cylindrical fridge and a rack full of classical electronic devices, all of which work together to keep the microscopic qubits in the coldest part of the fridge working. Further reading If youâ€™d like to know more about the physics and math behind qubits, we recommend the following sources: . | Quantum CountryÂ â€“ a great online textbook about Quantum Computing byÂ Andy MatuschakÂ andÂ Michael Nielsen. | QuTech Academyâ€™sÂ School of QuantumÂ â€“Â A broad range of topics explained using short videos.Â  . | A closer look at IBMâ€™s superconducting quantum computer on Youtube. | . ",
    "url": "/essentials/quantum/#what-does-a-quantum-computer-look-like",
    
    "relUrl": "/essentials/quantum/#what-does-a-quantum-computer-look-like"
  },"28": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "Timelines: When can we expect a useful Quantum Computer?",
    "content": "At a glance The earliest quantum applications will need several million qubits, according to the most rigorous studies. Assuming an exponential growth similar to Mooreâ€™s Law, we predict that the first applications could be within reach around 2035-2040. The billion-dollar question in our field is: . When will quantum computers outperform conventional computers on relevant problems? . In the previous chapter, we defined the requirements more precisely and coined the term â€˜utilityâ€™ for such an achievement. Unfortunately, nobody can confidently answer this question today, and past predictions often proved inaccurate. Moreover, a relevant quantum computer wonâ€™t just appear from one day to another: thereâ€™s a continuous evolution where these devices will become increasingly capable. In this chapter, we will show how we can make a rough prediction about future timelines and discuss what will happen on the path towards large-scale quantum computation. As an important disclaimer, this chapter is highly subjective. Itâ€™s not hard to arrive at different conclusions simply by choosing other sources and making different assumptions. I did my utmost best to rely on the most up-to-date information, combining the views of the most widely accepted papers and making assumptions that align with the view of most experts to present a balanced perspective. ",
    "url": "/essentials/timelines/#timelines-when-can-we-expect-a-useful-quantum-computer",
    
    "relUrl": "/essentials/timelines/#timelines-when-can-we-expect-a-useful-quantum-computer"
  },"29": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "What parameters are relevant?",
    "content": "Compared to currently available technology, weâ€™d require a fundamental improvement to these specifications: . | Number of qubits . | Accuracy of elementary operations (gates). This means that quantum computers have the ability to perform long computations without making mistakes. | . Quite a few other parameters matter, such as the connectivity, the available set of gates, the speed of operations, and so forth. In this chapter, we choose to simplify matters by assuming that all of these other parameters are not a bottleneck, allowing us to focus only on the number of qubits and gate accuracies. The relevance of accuracy is often overlooked, perhaps because this hardly plays a role for classical computers. The problem is as follows. A computation consists of many small, discrete steps called quantum gates. Unfortunately, even the most precisely engineered quantum computers are imperfect, and every gate has a slight chance of introducing an error. You can picture this intuitively as a qubit accidentally flipping from â€˜0â€™ to â€˜1â€™ or vice versa1. The probability that a gate introduces such an error on todayâ€™s hardware is around 0.1% to 1%. Sometimes the term â€˜accuracyâ€™ or â€˜fidelityâ€™ is used for the probability of not making an error, translating into numbers like 99.9%. Now, a serious computation can easily use billions of gates. You can hopefully see the issue here: for long calculations on current hardware, the output is almost certainly garbled by errors. In fact, given a certain error gate fidelity, there is a ballpark maximum number of steps that can be reasonably performed. With a 1:1000 probability of error, we can do roughly a thousand steps, and if the error is one in a million, we can do approximately a million gates. To solve increasingly complex problems, we do not only need to increase the number of qubits, but we also need to reduce the likelihood of errors. We should take a moment to appreciate the enormous challenge ahead of us. It took decades of engineering to minimize errors to about one in a thousand. Now, we should bring this rate down to one in billions. Thatâ€™s a huge gap that likely cannot be covered by hardware improvements alone â€“ even a breakthrough that reduces errors by 100x wouldnâ€™t cut it. Balancing qubits and accuracies . Luckily, there exists a technique that shrinks the probability of mistakes by any desired amount: error correction. It works roughly as follows. For every qubit that an algorithm requires, we donâ€™t just build a single hardware qubit, but instead, we dedicate a large number of qubits, like a hundred or a thousand. Weâ€™ll use the term physical qubits for the actual qubits present in the hardware, whereas the virtual error-mitigated ones are named logical qubits. For example, suppose we have a device with a million physical qubits. In that case, we might group a hundred of these to form a more error-resilient logical qubit, leaving a programmer with just 10.000 logical qubits to use. The image below shows a similar situation with a ratio of 1:12 between logical and physical qubits. For error correction to work, we need to make several assumptions. For example, depending on the precise error correction protocol, gate fidelities need to be quite good to start with â€“ numbers like 99.99% are often mentioned. This means that, as of 2024, the worldâ€™s best devices would still need to improve gate fidelities by more or less a factor of . | Moreover, qubits need to be routinely measured and reset, and large amounts of classical processing are available to deduce precisely how to repair a given error. These are significant engineering challenges, but experts are optimistic that this can be achieved. We discuss many more details in a separate chapter on error correction. | . For now, letâ€™s take for granted that we can somehow reach any desired accuracy (or any desired computation length) by simply adding sufficiently many physical qubits. Then, we can greatly simplify our analysis! For each application, we will forget about errors altogether and only count the number of physical qubits needed. This leads to an interesting situation. To solve larger, more complex problems, weâ€™ll need more qubits for two reasons: to store more data and to reduce the probability of errors so that the computation can run longer. Isnâ€™t the focus on just qubits a bit short-sighted? Doesnâ€™t this create a perverse incentive for manufacturers to focus only on qubit numbers, forgetting about all the other parameters? Well, I would indeed be worried that some companies can make headlines with unusable computers that happen to have a record qubit number. Luckily, most manufacturers seem dedicated to making the most â€˜usefulâ€™ computers, and customers will surely judge their products by the capabilities of their logical qubits. Weâ€™re obviously making a coarse simplification here, but making predictions about the future is hard enough as it is. Back to the main question: When can we expect a large quantum computer? Now that weâ€™re only counting qubits, we can break our billion-dollar question into two parts: . | How many qubits are needed? . | How long until we have million-qubit machines? . | . ",
    "url": "/essentials/timelines/#what-parameters-are-relevant",
    
    "relUrl": "/essentials/timelines/#what-parameters-are-relevant"
  },"30": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "How many qubits are needed?",
    "content": "In the previous chapter, we discussed the three main applications of quantum computers: quantum simulation, breaking cryptography, and optimisation. The most concrete numbers can be given forÂ Shorâ€™s algorithmÂ (breaking cryptography), where we have a very clear problem to tackle: obtain a private (secret) key from a widely-used cryptosystem, like the RSA-2048 protocol. This is the perfect benchmark because there can be no discussion about whether the problem is solved: one either obtains the correct key or one doesnâ€™t. Moreover, weâ€™re quite convinced that even the best classical computers canâ€™t solve the problemÂ (or else you shouldnâ€™t use internet banking or trust software updates).Â  . AÂ recent estimateÂ finds that a plausible quantum computer would require roughlyÂ 20 millionÂ â€˜reasonably goodâ€™ physical qubits to factor a 2048-bit number. The whole computation would take about 8 hours2. Such estimates require several assumptions on what a future quantum computer would look like. In this case, the authors assume qubits are built using superconducting circuits, which are laid out in a square grid. Error correction is assumed to be done using the so-called surface code, assuming the best-known methods for error correction in 2020. Note that future breakthroughs could reduce the required time and number of qubits even further. ForÂ chemistry and material simulation, itâ€™s a lot harder to make such esimates because there is not just a single problem to tackle here: one typically uses computers to gradually improve our understanding of a complex structure or chemical reaction. This should be combined with theoretical reasoning and practical experiments. Moreover, classical computers can often perform the same computations that the quantum computer would make at the cost of making certain assumptions or simplifications. Thereâ€™s a fuzzy region between â€˜classically tractableâ€™ and â€˜quantum advantageâ€™. The most concrete task is to compute the energy of certain molecular configurations. The benchmark is to provide energies more accurately than done in conventional experiments; one canonically takes the â€˜chemical accuracyâ€™ of roughly 1 kcal/mol as the precision to beat. Then, we should focus on molecules where classical computers cannot already achieve such accuracies. Â Â  . Note that the accuracy of a chemical energy should not be confused with the accuracy of a quantum gate, which is a whole different number. A highly promising and well-studied benchmark problem is the simulation of the so-called FeMo cofactor, in short, FeMoco. This molecule is relevant when bacteria produce Ammonia (NH3), a compound that is of great relevance to a plantâ€™s root system. A better understanding of this process could help us reduce theÂ ridiculously large carbon emissionsÂ now associated with the production of artificial fertiliser.Â We give more details in a separate chapter. Simulating FeMoco is believedÂ to require aroundÂ 4 million qubits3Â (and around 4 days of computation for a single simulation run). The hardware and error correction assumptions are similar to those of Shorâ€™s algorithm: the estimate is based on a square grid of superconducting qubits, using surface code to correct errors. For a different enzyme, namely cytochrome P450, itÂ has been estimated that aroundÂ 5 millionÂ qubits are needed4Â (again taking roughly 4 days of computation). Altogether, we conclude that a couple million qubits (of sufficiently high quality) can make quantum computers relevant for R&amp;D in chemistry.Â  . Some tasks that are mainly of interest for scientific purposes, such as simulating models of quantum magnets, can be achieved with fewer resources. Under similar assumptions, simulating a 2D transverse field Ising model is estimated to take just under 1 million qubits5. For manyÂ optimization problems, itâ€™s practically impossible to give reasonable estimates. As we saw previously, a true killer algorithm for optimisation problems is not known yet. The algorithms that are presented as the most promising are oftenÂ heuristic,Â meaning that scientists do not know how long an algorithm will take to find solutions. This can happen, for example, when an algorithm repeats a certain loop until a stopping criterion has been met. Our perspective starkly contrasts some other sources claiming that quantum computers are already solving practical problems today. But donâ€™t be fooled: these articles state that quantum computersÂ canÂ indeed solve relatively simple problems but often fail to mention that there existÂ differentÂ approaches by which classical computers can solve the same problems much, much faster.Â  . Moreover, many of these algorithms involve optimization problems that have a plethora of potential solutions, but the goal is to find theÂ optimalÂ solution (say, the one that incurs the least costs or gets you to your destination the fastest). The solution space is often so large that we donâ€™t even know if we hit this optimal solution, but weâ€™re okay with finding one thatâ€™sÂ pretty close.Â Several papers claim that a quantum computer already finds solutionsÂ faster, but in all cases, worrying sacrifices were made in the optimality of the solutions for more complex problems. What about D-Waveâ€™s quantum annealer? A particularly difficult case is the approach taken by D-Wave. This Canadian scale-up manufactures a quantum computer that is purpose-built to execute a specific optimisation algorithm called quantum annealing. With around 5000 qubits, it can handle reasonably large problems. The bare hardware alone doesnâ€™t seem to perform that well, but D-Wave cleverly combines it with classical high-performance computing in what they call a â€˜hybridâ€™ solver. Comparisons and benchmarks of the hybrid solution report results ranging from â€˜much worseâ€™ to â€˜very competitiveâ€™ relative to classical optimization solutions. Because it is unclear to what extent the hybrid solver actually exploits quantum phenomena, and since little is publicly available about future plans, I donâ€™t dare to make any future predictions about annealing. See also: . | D-Wave claims a scaling advantage when simulating quantum materials. | ETH Zurich researchers conclude that the 2015 version of D-Waveâ€™s annealer is comparable to a modern high-performance CPU. | Los Alamos researchers find annealing speedups in certain cases, but also note challenges to commercial adoption. | Researchers report that D-Waveâ€™s bare quantum hardware is quite slow, but the hybrid solution is very competetive with classical optimisation techniques. | . We can summarize our conclusions in the table below. | Application | How well can we estimate qubit requirements? | Use case example | Qubits needed? | Gate error assumed? | . | Breaking cryptography | Good | Cracking RSA-2048 | ~ 20 million | ~ 0.1% | . | Chemistry | Reasonable | Simulation of FeMoco | ~ 4 million | ~ 0.1% | . | Â  | Â  | Simulation of P450 | ~ 5 million | ~ 0.1 % | . | Optimization / AI | Bad | ? | ? | Â  | . What about future improvements? . It seems almost inevitable that the above methodologies will improve. Unfortunately, itâ€™s impossible to estimate by how much. Will we reduce the number of qubits required by a few per cent? Or by a factor of ten? By a factor of one thousand? . Some sources actually try to extrapolate the reduction in required qubits over time (like Youtube science educator Veritasium6 and a report by McKinsey7), but this is such a wonky extrapolation over a handful of data points that we will not follow this strategy. On the other hand, it would also be naive to stick to the numbers above without assuming some margin for improvements. In error correction techniques alone, there appears to be steady progress to improve the ratio between logical and physical qubits. Based on discussions with scientists, lowering the qubit requirements by a factor of 3 to 10 seems plausible. Hence, for optimistic readers, we can set another target at around 400.000 qubits. Interestingly, this number is similar to the qubit requirements for the simulation of models that are especially of scientific interest. | Application | How well can we estimate qubit requirements? | Use case example | Qubits needed? | Gate error assumed? | . | Chemistry (Optimistic) | Reasonable | Simulation of FeMoco (with 10x improved methods) | ~ 400.000 | ~ 0.1 % | . | Science | Reasonable | 2D Transverse field Ising model | ~ 900.000 | ~ 0.1 % | . ",
    "url": "/essentials/timelines/#how-many-qubits-are-needed",
    
    "relUrl": "/essentials/timelines/#how-many-qubits-are-needed"
  },"31": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "Can noisy algorithms be good enough?",
    "content": "Current quantum computers have a limited number of qubits and are not yet capable of large-scale error correction: they are Noisy Intermediate-Scale Quantum (NISQ) devices. An important question is: can we already achieve any utility with such noisy devices before the era of large-scale error correction? That is one of the most disputed topics in our field, and therefore it deserves some attention. A growing community of scientists, startups and enterprises are searching for such near-term applications. If successful, this would massively increase the overall usefulness of quantum computers. Some experts seem optimistic that this is possible, but a larger and more authoritative group remains highly sceptical about NISQâ€™s utility. In the past decades, when NISQ devices with just a handful of qubits were just on the horizon, several consultants made ridiculous claims about how such tiny machines would bring an exponential advantage over enormous supercomputers. Now that the field is coming of age, many are becoming more careful. To illustrate, when looking back at a 2021 report, BCG chivalrously admits8: . â€œOur assumptions for near-term value creation in the NISQ era, however, have proved optimistic and must be revised.â€ . The most serious recent claim about NISQ utility comes from the IBM team, in a paper titled â€œEvidence for the utility of quantum computing before fault-toleranceâ€9. However, their arguments were quickly refuted by further studies that simulated IBMâ€™s impressive quantum experiment on a conventional laptop10. Maryland-based professor Sankar Das Darma expresses the view of many academics in his opinion article â€œQuantum computing has a hype problemâ€11. He stresses about NISQ that â€œthe commercialization potential is far from clearâ€, pointing out that claims of speedups in finance, machine learning and drug discovery have so far come with highly unsatisfying evidence. That certainly doesnâ€™t mean that NISQ utility is ruled out. Most experts seem to keep an eye on the developments of NISQ applications but will agree that no utility for NISQ machines has been found yet. To illustrate, an overview article about pharmaceutical applications12 has a careful yet suggestive message: . â€œMost NISQ algorithms [â€¦] rely heavily on classical optimization heuristics, and the actual run time is difficult to estimate. Furthermore, recent results suggest that in NISQ approaches, the number of measurements required to achieve a given error scales exponentially with the depth of the circuit. For these reasons, here we focus our discussion exclusively on fault-tolerant quantum computers.â€ . Similarly, a recent overview13 of quantum chemistry seems to remain agnostic with regard to NISQ advantage while pointing out that fault-tolerance has a higher chance of succeeding. â€œâ€¦ it is difficult to predict when or if algorithms on near-term noisy intermediate-scale quantum devices will outperform classical computers for useful tasks. But it is likely that, at some point, the achievement of large-scale quantum error correction will enable the deployment of a host of so-called error-corrected quantum algorithms.â€ . In this book, I choose to follow the view of most authorities and sticking to the well-understood use cases for early fault-tolerant quantum computers that we discussed previously. Nobody can rule out new breakthroughs that allow NISQ utility, but it seems unwise to count on these. A potential scientific leap could completely stir up our fragile prediction â€“ but so would unexpected backlashes in hardware development or even unforeseen funding stops. ",
    "url": "/essentials/timelines/#can-noisy-algorithms-be-good-enough",
    
    "relUrl": "/essentials/timelines/#can-noisy-algorithms-be-good-enough"
  },"32": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "How long until we have million-qubit machines?",
    "content": "Now that weâ€™ve set our target to roughly a million qubits, weâ€™d like to estimate when such hardware will be available. We highlight the following sources: . | Road maps and claims of hardware manufacturers . | Surveys to experts . | Extrapolation of Mooreâ€™s law . | . What do manufacturers say? . Below, we see the qubit numbers that several manufacturers have already realized (solid disks) and what they will produce in the future according to their public road maps (opaque plusses). Note that the vertical axis is logarithmic, displaying a very broad range from around 10 to 10.000 qubits. A lower number of qubits does by no means indicate that these computers are worse. In fact, the machines with the lower numbers of qubits on this graph have an important edge in other parameters, such as gate accuracies and qubit connectivity. Figure: The number of qubits in the most mature quantum computers from a selection of various manufacturers, as of 2024. Besides their road maps, companies sometimes make more daring claims in media interviews or at presentations at large events. Based on the application targets above, it should be no surprise that manufacturers aim for around a million qubits as a â€˜moonshotâ€™ accomplishment. Back in 2020, IBM claimed to reach the 1 million qubit target by 203014. Around the same time, Google was interpreted by journalists to do this even faster (around 202915). The start-up PsiQuantum, which made waves thanks to record-high investments of over a billion dollars for their photonic quantum chips, went as far as claiming to have a million qubits by 202516. It seems that these claims were a bit too ambitious. In 2024, with only a year to go and no publicly presented product progression, PsiQuantum shifted its 1 million qubit road map to 202717. IBM took an even more conservative step, where itâ€™s now claiming to have just 100.000 qubits in 203318 (although this machine should meet the error correction assumptions that we dreamingly assumed in the previous sections). Although this delay sounds disappointing, hardware manufacturers are still making impressive progress, as the number of available qubits grows faster than one would predict according to Mooreâ€™s Law for classical chips! . Companies working on trapped ion machines tend to have fewer qubits, but higher gate accuracies. Perhaps this is why IonQÂ displays its road map in a different format: they aim to have 1024Â so-called algorithmic qubitsÂ by 202819. This means that IonQ will haveÂ at leastÂ this number of qubits, but also guarantees sufficient gate accuracy to run reasonably long circuits. Itâ€™s unclear whether error correction will be used for this. If so, the actual number of physical qubits may be some orders of magnitude higher.Â QuantinuumÂ (previously working under the name Honeywell) makes less concrete predictions but expects fault-tolerant computing by 203020 (meaning that significant error correction should be in place). What does Mooreâ€™s law say? . One could assume that quantum computers will â€˜growâ€™ at a similar rate as classical computers. Mooreâ€™s law states that the number of transistors in a dense integrated circuit grows exponentially: the number doubles roughly every two years. This has been a surprisingly accurate predictor for the development of classical IT. If we apply Mooreâ€™s law to quantum, then boosting qubit numbers from around a thousand to one million would take around 20 years â€“ predicting that the one million qubit mark wonâ€™t be passed until 2044. Clearly, most hardware manufacturers are more optimistic. If we assume the number of qubits doubles each year, one would predict that one million qubits will be available in ten years. While doubling a quantum computerâ€™s size each year is already a daunting challenge, companies like IBM, Pasqal and QuEra set the bar even higher for themselves, hoping to double every 7-9 months. What do experts say? . The Global Risk Institute conducts yearlyÂ surveys asking experts to state theÂ likelihoodÂ that quantum computers will pose a significant threat to public-key cryptography 5 years from now. Similarly, respondents would also estimate the likeliness 10, 15, 20, and 30 years away. This essentially boils down to the question: when will a quantum computer run Shorâ€™s algorithm to crack RSA-2048?Â We previously saw that around 20 million qubits would be needed for this (although experts may take into account that this number can still be lowered). We consider this an important source because many important authorities in the field (like professors and corporate leaders) take part in this study. The results from December 202321, gathered from 37 participants, are displayed below. Figure: Results of an expert survey by Global Risk Institute (source: globalriskinstitute.org). How to read this graph? Letâ€™s look at the column labeled â€˜5 yearsâ€™. A total of 24 correspondents indicate that there is less than 1% chance that quantum computers pose a security threat in the next five years. A single person is quite pessimistic and assigns a &gt;70% chance that this will happen. On average, experts say that thereâ€™s a fairly small chance that quantum computers will pose a threat to cryptography in the next 5 years.Â  . Further to the right, the ratios shift: looking at 20 years from now, the majority of experts believe that quantum computers pose a serious threat: over half of them assign a likelihood of 70% or more. It appears that the majority of experts believe that the tipping point is between 10-20 years from now. Somewhere between 15 and 20 years away, thereâ€™s a point where the median participant assigned roughly 50% chance to see a quantum computer capable of breaking cryptographic codes.Â However, we should take into account a significant uncertainty: even experts make wildly varying estimates, so thereâ€™s no obvious conclusion from this data. These experts are likely aware of hardware manufacturerâ€™s road maps, as we shall see below. ",
    "url": "/essentials/timelines/#how-long-until-we-have-million-qubit-machines",
    
    "relUrl": "/essentials/timelines/#how-long-until-we-have-million-qubit-machines"
  },"33": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "Putting it all together",
    "content": "The infographic below sums up our earlier findings. Assuming that qubit numbers will grow exponentially (and that all other parameters will keep up accordingly), we can consider several scenarios. A pessimistic scenario would be that the number of qubits â€˜merelyâ€™ follows the classical version of Mooreâ€™s Law, and qubit numbers double only once every two years (dotted line). Then, weâ€™d have to wait well past 2040 to reach 100.000 qubits. An extremely optimistic outlook would follow the blue dashed line (which extrapolates the progress by IBM, doubling their qubits every ~9 months). If one also believes in practical applications with much less than a million qubits, then these could be available by 2030. An intermediate perspective is to assume that the number of qubits doubles annually. Interestingly, this seems to approximately align IBMâ€™s latest claims and the typical expert opinion. Depending on the application, it would mean that quantum chemistry simulation and codebreaking can be within reach between ~2033 and 2040. To conclude, our estimates strongly depend on the assumptions that youâ€™re willing to accept (who wouldâ€™ve thought!). Do you believe that improving algorithms and error correction techniques will allow for applications with much less than a million qubits? How quickly do you believe that the hardware will improve? If you force me to make a prediction, Iâ€™d say the first applications arise around 2035, with the understanding that thereâ€™s a considerable margin for error. As a final remark, a full utility-scale quantum computer requires much more than just some number of qubits. To reach the first useful applications, we likely require simultaneous progress in algorithmics, software, gate accuracies, error correction techniques, fridges, lasers, and many other important subfields of quantum computing. Hopefully, all these disciplines will find the required breakthroughs that will sustain the exponential growth of quantum computing hardware. Further reading: . | Scientist Samuel Jaques (Waterloo) makes insightful graphs that combine the number of qubits and the error rates, and puts them in the perspective of applications requirements. https://sam-jaques.appspot.com/quantum_landscape_2023 | . | Technically, quantum gates are continuous operations, so numbers like fidelity are defined slightly differently. Still, the picture of discrete bit flips is not too much off and will lead to the same conclusions, so I prefer this more accessible explanation.Â &#8617; . | How to factor 2048 bit RSA integers in 8 hours using 20 million noisy qubits; Craig Gidney, and Martin EkerÃ¥, Quantum 5, 433 (2021), https://quantum-journal.org/papers/q-2021-04-15-433/Â &#8617; . | Lee, J. et al. (2021) â€˜Even More Efficient Quantum Computations of Chemistry Through Tensor Hypercontractionâ€™, PRX Quantum, 2(3), p. 030305. Available at: https://doi.org/10.1103/PRXQuantum.2.030305.Â &#8617; . | Goings, J.J. et al. (2022) â€˜Reliably assessing the electronic structure of cytochrome P450 on todayâ€™s classical computers and tomorrowâ€™s quantum computersâ€™, Proceedings of the National Academy of Sciences, 119(38), p. e2203533119. Available at: https://doi.org/10.1073/pnas.2203533119.Â &#8617; . | Beverland, M.E. et al. (2022) â€˜Assessing requirements to scale to practical quantum advantageâ€™. arXiv. Available at: https://doi.org/10.48550/arXiv.2211.07629.Â &#8617; . | See https://www.youtube.com/watch?v=-UrdExQW0cs&amp;t=1024s, starting at 17:04.Â &#8617; . | McKinsey Quantum Technology Monitor 2024, https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/steady-progress-in-approaching-the-quantum-advantageÂ &#8617; . | https://www.bcg.com/publications/2024/long-term-forecast-for-quantum-computing-still-looks-brightÂ &#8617; . | https://www.nature.com/articles/s41586-023-06096-3Â &#8617; . | https://arxiv.org/abs/2306.16372Â &#8617; . | https://www.technologyreview.com/2022/03/28/1048355/quantum-computing-has-a-hype-problem/Â &#8617; . | Santagati, R., Aspuru-Guzik, A., Babbush, R. et al. Drug design on quantum computers. Nat. Phys. 20, 549â€“557 (2024). https://doi.org/10.1038/s41567-024-02411-5Â &#8617; . | Quantum Chemistry in the Age of Quantum Computing, Chem. Rev.Â 2019, 119, 19, 10856â€“10915, https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.8b00803Â &#8617; . | https://fortune.com/2020/09/15/ibm-quantum-computer-1-million-qubits-by-2030/Â &#8617; . | https://quantumcomputingreport.com/google-goal-error-corrected-computer-with-1-million-physical-qubits-by-the-end-of-the-decade/Â &#8617; . | https://www.nextbigfuture.com/2020/04/psiquantum-targets-million-silicon-photonic-qubits-by-2025.html; https://www.icvtank.com/newsinfo/629365.htmlÂ &#8617; . | https://quantumcomputingreport.com/psiquantum-receives-940-million-aud-620m-usd-to-install-a-1-million-qubit-machine-in-australia-by-2027/Â &#8617; . | https://www.iotworldtoday.com/industry/ibm-details-road-to-100-000-qubits-by-2033Â &#8617; . | https://ionq.com/posts/december-09-2020-scaling-quantum-computer-roadmapÂ &#8617; . | https://www.honeywell.com/us/en/news/2020/10/get-to-know-honeywell-s-latest-quantum-computer-system-model-h1Â &#8617; . | https://globalriskinstitute.org/publication/2023-quantum-threat-timeline-report/Â &#8617; . | . ",
    "url": "/essentials/timelines/#putting-it-all-together",
    
    "relUrl": "/essentials/timelines/#putting-it-all-together"
  },"34": {
    "doc": "The timelines: when can we expect useful quantum computers?",
    "title": "The timelines: when can we expect useful quantum computers?",
    "content": " ",
    "url": "/essentials/timelines/",
    
    "relUrl": "/essentials/timelines/"
  },"35": {
    "doc": "Applications in chemistry and material science",
    "title": "Applications in chemistry and material science",
    "content": " ",
    "url": "/applications/chemistry/",
    
    "relUrl": "/applications/chemistry/"
  },"36": {
    "doc": "Applications in chemistry and material science",
    "title": "Contents",
    "content": ". | Applications in chemistry and material science . | What problems in chemistry and material science will we solve? | Algorithms for quantum chemistry . | Further reading on simulation algorithms | . | A hype around quantum computing for climate change | A case study of potential killer application: FeMoco | Further reading: | . | . Perhaps the most credible application of quantum computers is to study quantum physics itself. This helps us deepen our understanding of microscopic systems like molecules, atoms, or even sub-atomic particles, ultimately leading to the discovery of new drugs, materials and chemical production methods. At first sight, there seems to be a significant advantage compared to conventional computers, which struggle to store the complex quantum state of systems with many particles. As far back as 1981, physicist Richard Feynman ended a conference talk with a famous quote, hinting at the opportunities of quantum computing1: . â€œIâ€™m not happy with all the analyses that go with just the classical theory, because nature isnâ€™t classical, dammit, and if you want to make a simulation of nature, youâ€™d better make it quantum mechanicalâ€. Since then, scientists have become increasingly adept at accurately controlling quantum systems. Today, universities boast a wide spectrum of analogue quantum experiments that help us understand nature under exotic circumstances. Weâ€™re now lining up our tools to take these simulations to the next level: studying nature with digital quantum machines. In this chapter, we will assess how quantum computers can impact the fields of chemistry and material science. That makes this chapter more technical, and weâ€™ll assume some (very) basic background in chemistry and physics. We discuss the most relevant algorithms, evaluate claims about quantum computingâ€™s benefits in the fight against climate change, and analyse why the enzyme FeMoco receives such widespread attention. ",
    "url": "/applications/chemistry/#contents",
    
    "relUrl": "/applications/chemistry/#contents"
  },"37": {
    "doc": "Applications in chemistry and material science",
    "title": "What problems in chemistry and material science will we solve?",
    "content": "The computational problems that chemists care about typically come in two flavours. The most studied problem is finding the arrangement of particles with the lowest possible energy, which we call the ground state. In nature, we often find a system in (or close to) its ground state. In the context of molecules, the atomic nuclei are relatively heavy, while the lightweight electrons move much faster and are more prone to be entangled or in a quantum superposition. Therefore, chemists tend to make approximations that allow them to focus primarily on the positions and spins of the electrons: the electronic structure problem. The other problem is about dynamics: given some initial configuration of particles, how do they reconfigure themselves after a certain amount of time? This is often referred to as a systemâ€™s (time) evolution. Both problems are often informally referred to as quantum simulation. We often receive the question of why itâ€™s so hard to simulate quantum mechanics on a classical computer. Intuitively, this hardness arises when we deal with many particles that exhibit large amounts of superposition and entanglement, such that the location of one particle is heavily dependent on the (undecided) position of many other particles. We call such states strongly correlated. Classical computers struggle because they need to keep track of all the possible locations that particle A can be, but also all the locations of particle B, and the same for particle C, etcetera, which quickly winds up to an exponential number of possible sets of conditional locations. In other words, the number of relevant amplitudes (see the chapter on quantum physics) that we need to keep track of grows very quickly. Even with a mere one hundred particles, brute-force simulation is far beyond the capabilities of the worldâ€™s best supercomputers. It is a common misconception that quantum computers straightforwardly offer an exponential advantage compared to classical computers for all chemistry problems. An influential recent paper reports2: . â€œ[â€¦] we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.â€ . Note that this comment is specifically about finding ground states, which is still arguably the most relevant problem in chemistry. There is still ample evidence that quantum computers offer an exponential speedup for time evolutions. There is more bad news for quantum computers. Over the years, computational chemists have found brilliant hacks and optimisations to work around the classical computerâ€™s bottlenecks, raising a high bar before a quantum computer can meaningfully compete. For nearly every problem in chemistry, there appears to be a clever trick to solve it somewhat efficiently on a classical machine. For a killer application, we likely need to search in a fairly specific niche, right at the sweet spot where classical methods struggle while a quantum computer excels. It is not entirely clear how large this niche is, and it is an active research area to identify more systems where classical methods fall short. To illustrate, a recent review article states3: . [Classical methods struggle with] multi-metal systems, where multiple metal ions are in similar electronic environments and interactions. These appear in some biological systems, including enzymes (for example, FeMoco and P450), but it is unclear how common they are or what the added value of an accurate system description would be. Still, there seems to be realistic hope that quantum computers can, at the very least, make tangible contributions within niche areas. The first end-users will most likely be scientists who study the fundamentals of quantum systems, as is already done in physics experiments today. We wouldnâ€™t call these devices computers yet, but rather â€˜analogue simulatorsâ€™. One of the first actual â€˜computerâ€™ applications could be to study models of quantum materials, such as the famous Hubbard model4. The first error-corrected quantum computers will hopefully find their place in industrial R&amp;D settings. One of the first application areas could be the aforementioned multi-metal systems, which are relevant in calculations of ligand binding affinities in drugs and in understanding the mechanism behind the biological production of ammonia. We address the latter example at the end of this chapter. Another exciting area could be to explore the mechanism behind Type-II superconductivity and to search for materials that become superconducting at even higher temperatures5. It is hard to say what the impact of quantum computers will be beyond such niche areas, as this will depend strongly on the usefulness of small polynomial speedups and unpredictable breakthroughs in quantum algorithms. We see a broad palette of other impactful applications that have been proposed, such as water splitting (to efficiently produce hydrogen fuel)6, carbon capture mechanisms7, the study of efficient solar cells8 and the development of higher capacity batteries9. ",
    "url": "/applications/chemistry/#what-problems-in-chemistry-and-material-science-will-we-solve",
    
    "relUrl": "/applications/chemistry/#what-problems-in-chemistry-and-material-science-will-we-solve"
  },"38": {
    "doc": "Applications in chemistry and material science",
    "title": "Algorithms for quantum chemistry",
    "content": "We describe three of the most important quantum simulation algorithms. The first is the Trotter-Suzuki method, sometimes called â€˜Trotterizationâ€™, which simulates time evolution. In this case, we assume that some correct initial state of the world is encoded in the qubits of some quantum computer. The Trotter-Suzuki method is guaranteed to return a good approximation of the state at a later time, again encoded in the qubit registers. The most common tool for finding a ground state is quantum phase estimation (QPE), which reports the energy of a certain quantum state. As a subroutine, it requires some evolution method, like Trotter-Suzuki. Unfortunately, it will only succeed in finding the ground state if it gets as input a state that is a reasonable approximation to the ground state. This shifts the problem to: how do we produce a good candidate for the ground state? . The most popular algorithm for creating states with certain properties (like very low energies) is the variational quantum eigensolver (VQE). This is an example of a variational quantum circuit: a series of gates that can be gradually changed until the output matches certain requirements. Just like other variational approaches, it is a heuristic algorithm, lacking rigorous guarantees that it will produce the desired output in a reasonable time. Creating a good approximation to a ground state is, in general, NP-hard. This means that it is extremely unlikely that a rigorous algorithm for this task will be found. On the other hand, there is good hope that more heuristic methods (just like VQE) will be found that work well on certain subsets of problems (for example, the kind of molecules frequently encountered in the human body). In fact, such heuristic methods already form the workhorse of classical computational chemistry, with tools such as Density functional theory (DFT), Configuration Interaction (CI) and Quantum Monte Carlo (QMC). These work for small systems but are often too slow to study large systems such as proteins or drugs10. A workaround is to apply these methods to just a small part of the target system, employing faster but less accurate methods to oversee the larger whole. An example of a basic workflow to find find a ground state on a quantum computer could be as follows. The first step is to train a VQE to output states with low energy11. These might not be the exact ground states, but they will hopefully be very similar (in jargon, they have a large overlap with the ground state). As a second step, we append a QPE circuit, that will not only report the energy of the VQE states, but also has a fair probability of changing these states into perfect ground states (in jargon: it projects onto the ground state). Running the VQE + QPE combination a few times will almost certainly give the lowest energy states, assuming the VQE produces proper approximations of it. Further reading on simulation algorithms . Various more technical and sophisticated methods exist, for which we refer to other more technical sources. These require expert knowledge of quantum chemistry. | Introduction to Quantum Algorithms for Physics and Chemistry (2012)12, a pedagogical book chapter. Open version: https://arxiv.org/abs/1203.1331. | Quantum Algorithms for Quantum Chemistry and Quantum Materials Science (2020)13, a scientific overview article. Open version: https://arxiv.org/abs/2001.03685. | . ",
    "url": "/applications/chemistry/#algorithms-for-quantum-chemistry",
    
    "relUrl": "/applications/chemistry/#algorithms-for-quantum-chemistry"
  },"39": {
    "doc": "Applications in chemistry and material science",
    "title": "A hype around quantum computing for climate change",
    "content": "Some businesses make spectacular claims about how quantum computing could be a cornerstone in solving climate change, thanks to the boost to R&amp;D on batteries, carbon capture, and more efficient chemical factories. However, rarely do we see any evidence â€“ most seem to assume that quantum computers simply spit out blueprints for revolutionary sustainable technologies. McKinsey takes the biscuit with their report titled â€œQuantum computing just might save the planetâ€14. The article rightfully selects some of the most impactful technologies to reduce CO2 emissions, like electrification of transport, improved solar panels, and even vaccines that reduce methane emissions by cattle (indeed, due to cow farts). The article concludes that the selected innovations could reduce global warming from 1.7-1.8 Â°C by 2050 down to just 1.5 Â°C. It is a mystery to me why they throw in quantum computing because there is no mention whatsoever about why specifically quantum algorithms would be the key enabling factor. This exemplifies what we see more frequently in popular articles: quantum computers are depicted simply as insanely fast computers that will magically solve the barriers to other new technologies on our wishlist. What are the true prospects for quantum computing in the context of climate change? Sceptics will point out that technological innovations alone will be sufficient to avert a climate disaster â€“ we will remain agnostic in this debate. A much more concrete issue is the mismatch in timelines. Climate experts agree that, to limit global warming to no more than 1.5Â° C, we need to take action relatively soon. Imperial College London concludes on their website15, referencing the 2014 IPCC report: . â€œLimiting warming to 1.5Â°C will only be possible if global emissions peak within the next few years, and then start to decline rapidly, halving by 2030.â€ . Our chapter on timelines shows that it is exceedingly unlikely that significant quantum utility is possible anywhere before the 2030s. Additionally, it will take several years before a computational discovery is sufficiently mature for large-scale deployment. For this reason, we donâ€™t see quantum computers as a good investment against climate change, but rather as a long-term development that can help us tackle other problems that humanity will face in the future. Do we really have no concrete applications in climate science? Well, we do have some concrete leads. In the search for a killer application in chemistry, perhaps the most-studied topic is the enzyme FeMoco. This is precisely a multi-metal system that classical methods struggle with, and as weâ€™ll soon see, it appears in reputable plans for decarbonization. To understand the relevance of this molecule, we need to dive into the world of food production. ",
    "url": "/applications/chemistry/#a-hype-around-quantum-computing-for-climate-change",
    
    "relUrl": "/applications/chemistry/#a-hype-around-quantum-computing-for-climate-change"
  },"40": {
    "doc": "Applications in chemistry and material science",
    "title": "A case study of potential killer application: FeMoco",
    "content": ". Figure: Chemical structure of the FeMo cofactor, taken from Wikimedia. Todayâ€™s agriculture relies heavily on the use of artificial fertilisers. Without large-scale use of supplementary nutrients, we would not be able to sustain intensive farming practices and feeding our worldâ€™s huge population would be problematic. In fact, aboutÂ half of the nitrogen atomsÂ in our body have previously passed a fertiliser factory! . Unfortunately, the production of fertiliser involves enormous energy consumption and carbon emissions. The main culprit is the ingredient ammonia (NH3), of which we use as much asÂ 230 Mton per year. Although our air consists mainly of molecular nitrogen (N2), plants cannot directly absorb this. Instead, they rely on bacteria (or, in the case of artificial fertiliser, humans) to perform so-called nitrogen fixation, breaking the strong triple bond of molecular nitrogen and converting this into ammonia. Microorganisms can convert this into further nitrogen-containing compounds that the root system can absorb. Pretty much all of the worldâ€™s ammonia production facilities follow the so-called Haber-Bosch process, where hydrogen gas (H2) and nitrogen gas (N2) react together to form ammonia. This method has the benefit that it can be implemented in large, high-yield production lines but comes with the disadvantage of its staggering energy consumption. The inefficiency stems from two essential steps: first, producing sufficiently pure hydrogen and nitrogen gasses, and later, separating the H2 and N2 molecules into individual atoms. Breaking N2 is especially challenging due to its strong triple bond. As an effect, factories operate at extreme conditions, with high temperatures (~400 degrees Celsius) and high pressure, driven mainly by natural gas. As much asÂ 1.8% of the worldâ€™s CO2 emissionÂ is caused by factories performing such reactions, consuming around 3-5% of the worldâ€™s natural gas production! . Canâ€™t this be done more efficiently? We strongly suspect so. Certain bacteria are also capable of making ammonia, but in a seemingly more efficient way, without high temperatures or high pressure. It would be extremely valuable to copy this trick. To imitate the bacteria, we need to better understand a particular substance, the FeMo cofactor (short: FeMoco), which acts as a catalytic active site during ammonia production. A perfect simulation of FeMoco is not possible on classical computers, as the structure of roughly 120 strongly reacting electrons rapidly becomes intractable. In 2016,Â researchers from ETH Zurich and MicrosoftÂ were the first to report that a moderately large quantum computer could come to the rescue. A few years later, Google researchers refined the prospects even further, describing how simulations could be accomplished with aboutÂ 4 million qubits and 4 days of computing time. With FeMoco, we seem to finally have an example that confidently ticks all the boxes for quantum utility: classical methods are limited, we have well-understood quantum methods, and computational outputs have a significant commercial and societal impact. Unfortunately, there is yet another catch - innovation never comes so easily. A recent article16 quotes that industrial production of a ton of Ammonia costs around 26 GJ of energy, compared to at least 24 GJ (estimated) in bacteria. This is indeed not the massive reduction we were hoping for. The article concludes that perhaps the true value lies in a better understanding of this process: . â€œThe chemical motivation to study nitrogenase is thus less to produce an energy-efficient replacement of the Haber-Bosch process but rather because it is an interesting system in its own right, and perhaps it may motivate how to understand and design other catalysts that can activate and break the nitrogen-nitrogen triple-triple bond under ambient conditions.â€ . As a final note, we want to stress that quantum computers do not magically spit out recipes for fertilisers, nor for medicines, batteries, or catalysts. For real breakthroughs, we need collaborations between chemists, engineers, and many other experts who spend several years running experiments, having discussions, employing computer simulations, making mistakes, going back to the drawing board a few times, and slowly converging to practical solutions. We should not forget that quantum computers merely provide a new set of tools. The best we can hope for is that smart people will use them in the right way! . ",
    "url": "/applications/chemistry/#a-case-study-of-potential-killer-application-femoco",
    
    "relUrl": "/applications/chemistry/#a-case-study-of-potential-killer-application-femoco"
  },"41": {
    "doc": "Applications in chemistry and material science",
    "title": "Further reading:",
    "content": ". | (Scientific article) Toward the first quantum simulation with quantum speedup https://www.pnas.org/doi/10.1073/pnas.1801723115 . | (2022 review article) Prospects of quantum computing for molecular sciences https://link.springer.com/article/10.1186/s41313-021-00039-z . | (2019 review article) Quantum Chemistry in the Age of Quantum Computing https://pubs.acs.org/doi/10.1021/acs.chemrev.8b00803 . | . | Feynman, R.P. Simulating physics with computers.Â Int J Theor PhysÂ 21, 467â€“488 (1982). https://doi.org/10.1007/BF02650179Â &#8617; . | Lee, S., Lee, J., Zhai, H. et al. Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistry. Nat Commun 14, 1952 (2023). https://doi.org/10.1038/s41467-023-37587-6Â &#8617; . | Santagati, R., Aspuru-Guzik, A., Babbush, R. et al. Drug design on quantum computers. Nat. Phys. 20, 549â€“557 (2024). https://doi.org/10.1038/s41567-024-02411-5Â &#8617; . | Daley, A.J., Bloch, I., Kokail, C. et al. Practical quantum advantage in quantum simulation. Nature 607, 667â€“676 (2022). https://doi.org/10.1038/s41586-022-04940-6Â &#8617; . | Garnet Kin-Lic Chan, Quantum chemistry, classical heuristics, and quantum advantage (preprint); https://arxiv.org/abs/2407.11235Â &#8617; . | https://quantumapplicationlab.com/2024/01/08/photocatalysis-for-water-splitting/Â &#8617; . | Von Burg et al., Quantum computing enhanced computational catalysis, Phys. Rev. Research 3, 033055. https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.033055Â &#8617; . | https://www.pv-magazine.com/2023/08/04/quantum-physics-supercomputers-and-solar-cell-efficiency/Â &#8617; . | https://spectrum.ieee.org/lithium-air-battery-quantum-computingÂ &#8617; . | Santagati, R., Aspuru-Guzik, A., Babbush, R.Â et al.Â Drug design on quantum computers.Â Nat. Phys.Â 20, 549â€“557 (2024). https://doi.org/10.1038/s41567-024-02411-5. Quote from this article: â€œCurrent classical quantum-chemistry algorithms fail to describe quantum systems accurately and efficiently enough to be of practical use for drug design.â€Â &#8617; . | An interesting subtlety is how we measure the energy that the VQE is supposed to optimise. Luckily, there exist very short circuits that we can append to measure the output states in different bases. By running the VQE a relatively small number of times, we can make good estimates of the energy of its output states. This avoids performing the more complex QPE during the optimisation phase.Â &#8617; . | Yung, M.-H. et al. (2014) â€˜Introduction to Quantum Algorithms for Physics and Chemistryâ€™, in Quantum Information and Computation for Chemistry. John Wiley &amp; Sons, Ltd, pp. 67â€“106. Available at: https://doi.org/10.1002/9781118742631.ch03.Â &#8617; . | Bauer, B. et al. (2020) â€˜Quantum Algorithms for Quantum Chemistry and Quantum Materials Scienceâ€™, Chemical Reviews, 120(22), pp. 12685â€“12717. Available at: https://doi.org/10.1021/acs.chemrev.9b00829.Â &#8617; . | https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/quantum-computing-just-might-save-the-planetÂ &#8617; . | https://www.imperial.ac.uk/grantham/publications/climate-change-faqs/how-and-when-do-we-need-to-act-on-climate-change-/Â &#8617; . | Garnet Kin-Lic Chan, Quantum chemistry, classical heuristics, and quantum advantage (preprint); https://arxiv.org/abs/2407.11235Â &#8617; . | . ",
    "url": "/applications/chemistry/#further-reading",
    
    "relUrl": "/applications/chemistry/#further-reading"
  },"42": {
    "doc": "The impact on cybersecurity",
    "title": "The impact on cybersecurity",
    "content": "In the world of quantum computers, the most convincing exponential speedup lies in codebreaking. Anyone who wants to understand the impact of quantum computers, will need to know their basics of cryptography. Letâ€™s start at the beginning.Â  . ",
    "url": "/applications/cybersecurity/",
    
    "relUrl": "/applications/cybersecurity/"
  },"43": {
    "doc": "The impact on cybersecurity",
    "title": "Cryptography is much more than just secrecyÂ ",
    "content": "Why do we actually use cryptography? Pretty much everyone will immediately think of: . | Privacy / confidentiality:Â making sure that others cannot read your data (especially when messages are sent over a network). | . However, there are many more threats that cryptography protects us from. Most people wouldnâ€™t normally worry about them, but when any of the following is missing, cybercriminals can cause a lot of harm:Â  . | Authentication:Â You want to verify that a message really came from the entity that claims to send the message. For example, during online banking, you want to be 100% sure that you are communicating with your bank and nobody else. Another example is when installing a new piece of software. When executing the latest Windows update, your computer makes sure to check that there is aÂ â€˜digital signatureâ€™Â that belongs to Microsoft. Imagine how unsafe your computer would be if anyone could send fake updates! | . | Integrity:Â You want to verify that nobody changed the message during transit. Imagine what damage could be caused when your emails are maliciously altered before they arrive, or when the commands coming from an air traffic control tower are modified. Similarly, any software installer confirms that the software wasnâ€™t changed by anyone but the original publisher, by verifying a digital signature. | . | Establishing secret keys:Â How do you negotiate a new secret key with a brand new webshop that you never visited before? This is a seemingly impossible task if bare internet traffic can be read by anyone, but modern cryptography has a solution. | . There are some others, likeÂ non-repudiationÂ andÂ availability,Â that we donâ€™t discuss here. Remember the above bold-faced words, as we will come across them a lot.Â  . We hope that this introduction makes the reader aware of the enormous importance of proper cryptography, and the sheer number of cryptographic checks that are required for proper functioning of our IT. You would be surprised how often you use cryptography on a daily basis, through your laptop, phone, car keys, or smart cards. ",
    "url": "/applications/cybersecurity/#cryptography-is-much-more-than-just-secrecy",
    
    "relUrl": "/applications/cybersecurity/#cryptography-is-much-more-than-just-secrecy"
  },"44": {
    "doc": "The impact on cybersecurity",
    "title": "The quantum threat is mainly to public-key cryptography.Â ",
    "content": "A common misconception, which we see a lot in popular literature, is that the quantum threat can be summarised as follows. (Both of the statements below areÂ incorrect!)Â  . | A quantum computer will break all of todayâ€™s cryptography.Â  . | A quantum internet is needed to keep our cryptography safe again. | . Â  . To better understand this, letâ€™s first look at what cryptography a quantum computer will break, and which it wonâ€™t. Later, we will look at the necessity of a quantum internet.Â  . In line with common cryptography jargon, we will typically have two parties, Alice and Bob, who want to communicate with each other. We distinguish two different types of cryptography: the symmetric and the asymmetric (public key) variants.Â  . InÂ symmetric (or private key) cryptography,Â we assume that both Alice and Bob already know some secret key. This could be a password that they both know, or more commonly, a very long number represented by (say) 128 bits in their computer memory. Alice can use the key to encrypt any message using a protocol likeÂ AES. Bob can then use the same key to decrypt this message. The details of how encryption and decryption work are unimportant for our purposes. The only thing thatâ€™s relevant is that our computers can do this very efficiently, and that itâ€™s considered sufficiently safe: without the key, nobody could reasonably break this encryption. In asymmetric cryptography, or more often calledÂ public-key cryptography (PKC), each participant has two keys: aÂ public keyÂ and aÂ private key. TheÂ public keyÂ can be shared with anyone, while theÂ private keyÂ must be kept secret. Thatâ€™s why we use the suggestive colours green (save to share) and red (keep private!). If Alice wants to send an encrypted message to Bob, she usesÂ Bobâ€™sÂ public keyÂ to encrypt the message. If Bob wants to decrypt the message, he uses hisÂ private key.Â  . The setting with two keys offers more functionality. For example, using the previous encryption method, Alice could send a secret key to Bob, which they can then use for symmetric cryptography, which is faster in practice. Furthermore, the protocol works in â€˜reverseâ€™. Alice can use herÂ private keyÂ to encrypt a message, which then anyone in the world (including Bob) can decrypt using the correspondingÂ public key. Bob should then be confident that Alice is the only person in the world who could have encrypted this message (indeed, something encrypted with theÂ private keyÂ canÂ onlyÂ be decrypted with theÂ public key, and vice versa). This forms the basis of digital signatures.Â Â  . Â  . This is precisely whatâ€™s used whenever you open a webpage. Your browser (like Chrome or Firefox) will display that the connection is secure, which means that, amongst other things, it verified that the digital signature is valid. This ensures authenticity and integrity.Â  . It should come somewhat as a surprise that public-key cryptography is even possible at all! Itâ€™s kind of a small wonder that encryption and decryption with two totally different keys can be made to work, thanks to some powerful mathematics. For example, I donâ€™t know of any physical locks that work this way. However, it turns out that the delicate relationship between the two keys is also a weak spotâ€¦ . How good are quantum computers at cracking cryptography?Â  . In principle,Â symmetric-key cryptographyÂ is fairly safe against quantum hackers. The biggest problems are brute-force attacks, where an attacker effectively tries every possible secret key. Using a key of 128 bits, the total number of possible keys is 2128Â â€” thatâ€™s an incomprehensibly large number, much more than the number of atoms in a human body.Â  . We know that Groverâ€™s algorithm speeds up brute-force search, by reducing the number of attempts fromÂ 2128 to its square root, which isÂ 264. This is something that cryptographers are not happy about, but considering the slowness and extra overhead that comes with quantum computers, this doesnâ€™t seem to be a problem in the foreseeable future. Still, to be on the safe side, it is recommended to double key lengths, hence to use the same algorithm with 256-bit keys. Changing this in existing IT infrastructure is relatively straightforward, although one shouldnâ€™t underestimate the time and costs for such changes within large organisations. The situation is completely different withÂ public-key cryptography.Â The most-used algorithms today,Â RSAÂ andÂ ECC, can be straightforwardly broken by a large quantum computer. We discussed the details ofÂ  Shorâ€™s algorithmÂ earlier, and saw that around 20 million qubits and around 8 hours are needed to retrieve a secret RSA key. Luckily, there exist PKC systems that are believed to be safe against quantum computers, and an obvious way forward is to start using these. We call such systemsÂ post-quantum cryptography, and despite the confusing name, theyâ€™re built to work on conventional computers. We discuss the rabbit hole of migrating to new cryptographyÂ in a different chapter. Unfortunately, todayâ€™s communication is already at risk due to a practice calledÂ harvest now, decrypt later.Â Encrypted messages that are sent over a network can be intercepted and stored for many years, until a quantum computer can efficiently decrypt the messages. Even though we use public-key encryption mainly to establish temporary keys for symmetric cryptography, a smart attacker could still derive the necessary intermediate keys. It is inclear at what scales such large-scale storage of sufficiently detailed internet data is actually happening, but it seems plausible that security agencies of larger nations are working on it. Further reading: . | Redhat blog on Moscaâ€™s theorem, which statesÂ whenÂ you should start upgrading if your communication needs to remain secret for at least X years. | . Â  . The following table summarizes how our cryptosystems are threatened: . | Â  | Symmetric | Public-key | . | Â  | Today (AES, â€¦ ) | Today (RSA, ECC) | PQCÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â  | . | Against classical computers | Safe | Safe | Safe | . | Against quantum computers | Safe* . *with double key lengths . | Unsafe | Safe | . Why donâ€™t we switch to symmetric cryptography?Â  . Public-key cryptography solves a very fundamental problem: how can Alice and Bob agree on a secret key before they have a means of encryption in the first place? They cannot just send a new key over the internet without any form of encryption, because anyone would be able to read this. This is the fundamental problem ofÂ key distribution. Let us look at the functionality offered by the two types of cryptography:Â  . | Â  | Symmetric | Public-keyÂ  Â  Â Â Â  Â  Â  Â  Â  Â  Â  | . | Confidentiality (privacy) | Only with pre-shared keys | âœ” | . | Authentication / IntegrityÂ Â  | Only with pre-shared keys | âœ” | . | Establishing secret keys | âœ— | âœ” | . If only we could somehow give Alice and Bob pre-shared keys in a secure way, we would resolve most of these problems. Without public-key cryptography, there are other options: . | Alice and Bob could meet every other week to exchange USB drives with secret codes. | Alice and Bob could both trust a large â€œkey serverâ€. If both share a secret key with the key server, they can securely ask the server to generate a new secret key that they can use together.Â  . | . Still, none of these form an attractive alternative to public-key cryptography, especially if one considers the sheer number integrity and authenticity checks we perform every day, and the incredible number of online entities we potentially want to communicate with.Â  . ",
    "url": "/applications/cybersecurity/#the-quantum-threat-is-mainly-to-public-key-cryptography",
    
    "relUrl": "/applications/cybersecurity/#the-quantum-threat-is-mainly-to-public-key-cryptography"
  },"45": {
    "doc": "The impact on cybersecurity",
    "title": "What solutions exist?",
    "content": "TODO: More on PQC, hopefully when new standards are released! . What about Quantum Key Distribution (QKD)? . As we saw in a different chapter, Quantum Key Distribution partially solves the key distribution problem. It requires a quantum network connection between Alice and Bob, and somewhat expensive quantum devices to generate and measure photons. When used, Alice and Bob will obtain a secret key (that can be as long as they like), in a very safe way (in the sense that nobody listening in on their classical or quantum communication can possibly find this key). Unlike PKC, this method still requires pre-shared keys for authentication (because otherwise you canâ€™t be sure with whom you will share your new secret key). Therefore, it wonâ€™t completely solve the key distribution problem.Â  . There is even more reason to be careful: many security authoritiesÂ warnÂ againstÂ adoptingÂ QKDÂ today. Although the theory is very sound, todayâ€™s hardware is still in an early stage. The time to generate secret keys is still relatively long, and it is very likely that the actual software and hardware contain mistakes that make them vulnerable to attacks.Â  . It is somewhat of a pity that QKD is not so mature yet, because it would be a viable weapon against Harvest Now, Decrypt Later (since harvesting the pre-shared authentication keys is not a problem). Moreover, widespread adoption of QKD would make it easier to expand to a large-scale quantum internet. Nevertheless, since a quantum threat could be here as soon as the early 2030s, most companies are recommended to urgently fix their post-quantum cryptography (PQC) first, and potentially consider QKD as an add-on for additional security later, if needed.Â  . Â  . What about Quantum Random Number Generators (QRNG)? . Good random number generators are extremely important in cryptography, and QRNGs could provide a good alternative to theÂ hardware random number generatorsÂ that are widely used today.Â  . However, all they do is generate random numbers â€“ that doesnâ€™t make any protocol safe in itself. As a general warning:Â **products with â€˜quantumâ€™ in the name do not automatically protect against Shorâ€™s algorithm!Â  ** . What steps should a typical company or government take?Â  . We dedicate aÂ separate chapterÂ to that!Â  . Conclusion . It should be clear that cryptography is strongly intertwined with quantum computing, through Groverâ€™s algorithm, Shorâ€™s algorithm, and Quantum Key Distribution. Nevertheless, security experts recommend that there is an obvious way forward: . | Replace current public-key cryptography with new, quantum-safe protocols (PQC). | Double key lengths in symmetric cryptography.Â  . | . Especially the first bullet is a major challenge. There are many legacy systems around on the internet that may not be updated so easily. There are billions of devices that are all interconnected, so updating one device will surely cause incompatibilities somewhere else. Whatâ€™s more, PQC protocols will surely require more CPU power and more memory than todayâ€™s trusted methods. Companies may need to update the core code of hundreds or even thousands of applications. And lastly, the new protocols havenâ€™t been tested as extensively as our conventional methods, so it is not unlikely that new security issues will be found. Quantum computers, before they are even built, are already destined to make the next decade an incredibly complex period for anyone who deals with cryptography!Â  . ",
    "url": "/applications/cybersecurity/#what-solutions-exist",
    
    "relUrl": "/applications/cybersecurity/#what-solutions-exist"
  },"46": {
    "doc": "The impact on cybersecurity",
    "title": "Further reading",
    "content": ". | The NSA publishes recommendations on what which cryptographic algorithms should be used, and sketches a concrete timeline about when national security systems should be updated. | . ",
    "url": "/applications/cybersecurity/#further-reading",
    
    "relUrl": "/applications/cybersecurity/#further-reading"
  },"47": {
    "doc": "Applications of quantum networks",
    "title": "Applications of quantum networks",
    "content": "If weâ€™re building computers that deal with qubits, superposition and entanglement, wouldnâ€™t these computers also need some way to send qubits to each other? This is the dream of the quantum internet: a network that exchanges quantum-mechanical photons between devices all around the world, parallel to our well-known â€˜classicalâ€™ internet.Â  . There is a bit of a paradox here. On the one hand, a full-blown quantum internet is very, very far away: a network that reliably transports actual qubits is arguably harder to realize than a large quantum computer, as it will build upon theÂ error correctionÂ technology that weâ€™re only just figuring out. On the other hand, it is often said that â€˜quantum networksâ€™ have a higher Technology Readiness Level than computing. That sounds like a contradiction, right? . The main explanation is that there are some applications for â€˜imperfectâ€™ quantum networks, in particular in the context of cryptography.Â  . In that sense, quantum networking applications have always been â€˜aheadâ€™ of quantum computing. Already in 1984, long before quantum computers were seriously considered, researchers Benett and Brassard discovered a method to securely negotiate a secret key (think of a password) between two distant parties, based on sending individual photons. Their result is now famously known as theÂ BBâ€™84 protocol. Similarly, the commercialization of network technologies has long been ahead of computing. Early quantum startups like MagiQ Technologies and ID Quantique were founded around the new millennium, and brought their first commercial networking products to the market in 2003 and 2004. This technology, where a quantum network is used to generate a secret key at two endpoints, is called Quantum Key Distribution (QKD) â€“ an application that we will address in much more detail below. ",
    "url": "/applications/networks/",
    
    "relUrl": "/applications/networks/"
  },"48": {
    "doc": "Applications of quantum networks",
    "title": "The promises of the quantum internet",
    "content": "There is a long list of arguments why we should be excited about the quantum internet. Here is a list of the applications that we hear most frequently: . | Clustering quantum computers:Â By connecting multiple smaller computers, one might build a much larger computer which has more combined memory and can tackle larger problems.Â  . | Securing your classical communication.Â The main contender here isÂ Quantum Key Distribution (QKD), sometimes dubbed the â€œunhackableâ€ network.Â This allows two distant users to create a secret key (think of a password) that they can later use for further cryptographic applications. | . | â€œBlind computingâ€: Encrypting your data while still allowing someone else to process it.Â What if you hire an Amazon cloud computer to do calculations on your data, but you donâ€™t want Amazon to actually see your data? It turns out that you can make quantum computers do their computations even while data remains encrypted, with some caveats. Similarly, one could use â€˜encryptedâ€™ software to solve someone elseâ€™s problem without them discovering this algorithm. Such applications often go by the name of blind computing or private computing.Â  . | A scientific (hard!) overview of blind computing applications.Â  | . | . | Position verification:Â Can you prove that you are currently at a given location, in a way that cannot be spoofed?Â  . | A short introductory video [3:23] | . | . | Protocols with multiple parties, where not every participant can be trusted,Â such asÂ leader electionÂ orÂ Byzantine agreement. You can find many more in theÂ Quantum Protocol Zoo.Â  | . | Make quantum sensors more effective. There exist proposals to combine different telescopes or gravitational wave detectors, and plans to synchronize quantum clocks.Â  . | A scientific (hard!) overview of distributed quantum sensing | . | . Much more about the various applications can be found in thisÂ online Quantum Internet magazineÂ by TU Delft, or asÂ listed by the Quantum Internet Alliance. ",
    "url": "/applications/networks/#the-promises-of-the-quantum-internet",
    
    "relUrl": "/applications/networks/#the-promises-of-the-quantum-internet"
  },"49": {
    "doc": "Applications of quantum networks",
    "title": "How useful is the quantum internet in practice?Â ",
    "content": "Admittedly, many applications of the quantum internet will depend on how much we will use quantum computers. If quantum computers become widespread in the future, then communication between them also seems to be extremely worthwhile. On the other hand, our current outlook of quantum computers focuses on special-purpose devices that are used to solve isolated problems. It is not immediately clear what the value of exchanging quantum data is. There is a clear road map to build a reliable quantum internet in the future (involving fascinating tricks likeÂ entanglement distillationÂ andÂ teleportation), but this would require multiple error-corrected quantum computers by itself! For that reason, in this guide, weâ€™re not yet looking ahead at applications like clustering computers, multi-party computations, private computing, or making sensors more effective. Regarding clustered quantum computers,Â we frequently hear arguments that one can make a â€˜biggerâ€™ quantum computer by connecting individual ones. It seems to me that building these computers right next to each other (and calling it a single computer) is much more effective than transporting fragile quantum data over large distances â€“ clustering seems mainly viable on extremely small networks. In the foreseeable future, the first interesting applications are those that work over a â€œnoisyâ€ connection, and transport just one qubit at a time (or perhaps a handful of them). For practical interest,Â Quantum Key Distribution (QKD)Â is by far the most interesting application. ",
    "url": "/applications/networks/#how-useful-is-the-quantum-internet-in-practice",
    
    "relUrl": "/applications/networks/#how-useful-is-the-quantum-internet-in-practice"
  },"50": {
    "doc": "Applications of quantum networks",
    "title": "Â The case for QKD",
    "content": "To fully understand QKD, we will need to have a bit more background about cryptography, especially the â€œkey distribution problemâ€. For a full account, we recommendÂ first reading the chapter on cryptography. In short: weâ€™re wondering how Alice can agree on a secret key with her distant friend Bob, in a world where everyone can read plain data sent over the internet. Surely they canâ€™t just send their secrets or passwords over to each other, without having any encryption in the first place! This problem is commonly solved usingÂ public key cryptographyÂ (which we know will be revamped in the following years). If you really donâ€™t trust public-key cryptography, the main alternative is to physically transport a usb stick by a trusted courier.Â  . Compared to conventional cryptography, the unique selling point of QKD is that it is fundamentally impossible for cybercriminals to obtain the secret key as it is being distributed. As long as our understanding of quantum mechanics is correct (and weâ€™re quite convinced it is, as itâ€™s arguably the most well-test theory in science), no amount of computational power or mathematical breakthroughs will let an attacker gain information about the key. Of course, this assumes that the protocol is executed precisely as prescribed, and there are no other vulnerabilities in the actual hardware or software used.Â  . This is fundamentally different from todayâ€™s approach of public key cryptography, which must rely on certain mathematical assumptions. We know for sure that, with sufficient computational power, these codes can be broken, but we argue that this takes such an awfully long time that nobody will bother (except for bragging rights andÂ prize money1). Still, such statements about computation times are based on assumptions, and our trust is purely based on our experience that cryptographers have not been able to break them yet. In fact, well-regarded cryptosystems do get broken from time to time, such asÂ SIKE, which was in the race to become a new NIST standard.Â  . That said, although QKD is â€œunhackableâ€ in theory, the actual hardwareÂ andÂ software are equally likely to contain vulnerabilities. Contrary to well-trusted public-key cryptography, no QKD system has received proper certification and accreditation, and a significant fraction of historical productsÂ haveÂ beenÂ hacked.Â  . QKD has the downside that it requires specialised hardware, although it is much less demanding than other quantum internet applications we mentioned. It can already prove useful on a basic point-to-point network with just two connected parties, of which one should send photons, and the other measure them. This is orders of magnitude less complex than building a fully-featured internet. Moreover, the qubits need only be sent and measured one at a time: no quantum memory or extensive quantum computations are needed. There have already been several demonstrations that use standard telecom fiber (the stuff thatâ€™s already in the ground), or satellite-based systems that communicate through air. QKD hardware is fancy and expensive, but not completely out of reach.Â  . The major downside of QKD is that it has no way to confirm who the person on the other end of the line is. Some form of authentication is still needed â€“ which is mostly done with secret keys that should already be present in the first place! This makes QKD just a partial solution to the key distribution problem.Â  . Â  . Further reading about QKD . | A video explanation of QKD forÂ laymenÂ orÂ experts. | Companies likeÂ ToshibaÂ andÂ ID QuantiqueÂ offer commercial QKD systems for distances of around 100 km.Â  . | Chinese scientists achieveÂ QKD through satellitesÂ over 1000 km.Â  . | Nature commentaryÂ why practical long-range QKD is still out of reach.Â  . | . What do experts say?Â  . Cryptographers that have been working on securing classical computers are typically sceptical about QKD. In fact, all security authorities that we are aware of will advise against the use of QKD at this current point in time. They find the use of additional, uncertified hardware too large of a security risk, and stress that there is a better solution that works on conventional computers:Â post-quantum cryptography (PQC). From their perspective, PQC offers all the required functionalities, and is currently more practical to test, certify and implement.Â  . Be careful not to confuse theÂ abbreviations PQC and QKD. QKD is about communication with a fancy quantum network. PQC runs on conventional hardware.Â You may call both of them â€˜quantum-safeâ€™ cryptography, as they should both resist attacks from a lage-scale quantum computer. A discussion between physicists and cryptographers. A fair argument in favour of QKD, stems from the â€˜harvest now, decrypt laterâ€™ attacks that could be done over todayâ€™s internet. These would imply that even the privacy of todayâ€™s messages is compromised. This could be a convincing reason for organisations to rapidly switch to QKD for their most sensitive data. Still, for those who are willing to go the extra mile for their privacy, it might be more worthwhile to look at more mature and readily available solutions. For example, there exist certified solutions that rely on symmetric encryption with trusted couriers.Â  . See also: . | Compumatica offers symmetric encryption with keys transported on SD-cards.Â Â  . | TNO designed a â€˜quantum-safe proxyâ€™ as add-on to existing cryptography.Â  . | StackOverflow question:Â â€œWhy does the NSA find QKD impracticalâ€? . | . Â  . Whatâ€™s left is a very niche use-case for the most forward-thinking organisations that deal with extreme security requirements. It is somewhat of a pity that QKD is not so mature yet today, now that many organisations will start a migration to quantum-safe cryptography. A widespread adoption of QKD would make it easier to expand to a large-scale quantum internet in the future. Nevertheless, since a quantum threat could be here as soon as the earlyÂ 2030s, most companies are recommended to urgently migrate to post-quantum cryptography (PQC) first, and potentially consider QKD as an add-on for additional security later, if needed.Â  . Â  . Conclusion . In conclusion, most applications of a quantum internet will not be immediately relevant in the foreseeable future, with an exception for QKD. And even QKD might not be the killer applications that many investors are hoping for â€“ it most definitely shouldnâ€™t be called â€œunhackableâ€.Â  . Still, it seems unfair to us to dismiss a quantum internet because it would be â€˜too technologically challengingâ€™ or â€˜too expensiveâ€™. These arguments are correct today, but perhaps naive on a scale of several decades. Would anyone from the 70â€™s have believed that today, more than half of the world population is streaming videos on a mobile phone for just a few dollars per month? Who knows what the quantum internet will look like 50 years from now.Â  . | See https://en.wikipedia.org/wiki/RSA_Factoring_Challenge.Â &#8617; . | . ",
    "url": "/applications/networks/#the-case-for-qkd",
    
    "relUrl": "/applications/networks/#the-case-for-qkd"
  },"51": {
    "doc": "Applications in optimization and machine learning",
    "title": "Applications in optimisation and machine learning",
    "content": "TODO: write section! ğŸ¡ª See separate document! . Where should we look for a killer application? . Well, we simply donâ€™t know! However, some useful technical hints may be: . | As described above, weâ€™d most likely require anÂ exponentialÂ or someÂ heuristicÂ speedup. This is much more likely achieved on problems where we donâ€™t already know very efficient classical algorithms.Â  . | When reading data is a limiting factor (as in â€œbig dataâ€ applications), quantum computers appear to be very slow. Getting the data into a quantum computer seems to take at least as long as processing the data on a much cheaper supercomputer. This holds, for example, when searching through a database, but also for data-intensive simulations like weather forecasting.Â  . | Similarly, if the desired output is a large amount of data (such as a very large list or table), then a quantum computer is likely not efficient. Most quantum algorithms look at a global property of a function or dataset that can be encoded in a very small output (like Deutsch-Jozsa or Shorâ€™s algorithm when interpreted as finding the period of a function).Â  . | Some people would say that if quantum computers are not â€œfasterâ€, perhaps they might solve a problem â€œmore accuratelyâ€ (for example, they might produce a more reliable forecast). However, when we look at speedups, then accuracy is already taken into account: we compare the number of stepsÂ asserting the output has a given accuracy.Â  . | Classical computers are already incredibly fast, and the bottleneck for many real-world computational problems is not in a computerâ€™s speed. If an application does require a supercomputer today, then itâ€™s unlikely that anyone will invest in a quantum computer soon. | . ",
    "url": "/applications/optimization_ai/#applications-in-optimisation-and-machine-learning",
    
    "relUrl": "/applications/optimization_ai/#applications-in-optimisation-and-machine-learning"
  },"52": {
    "doc": "Applications in optimization and machine learning",
    "title": "Applications in optimization and machine learning",
    "content": " ",
    "url": "/applications/optimization_ai/",
    
    "relUrl": "/applications/optimization_ai/"
  },"53": {
    "doc": "Error correction",
    "title": "Error correction",
    "content": "At a glance . | To run long computations, we need to dramatically reduce the likelihood of error of each elementary step â€“ not just a little bit, but by a factor of millions.Â  . | Error correction is the most effective method to achieve extremely low error probabilities. It combines a small number of â€˜physicalâ€™ qubits (think of several hundreds) into a single â€˜logicalâ€™ qubit that suppresses errorsÂ exponentially.Â  . | Logical qubits are still not perfect qubits: the â€˜number of stepsâ€™ that they can survive is an important specification that determines whether they can run your application. | . Around 2024, weâ€™re seeing a major shift in the road maps of quantum computer manufacturers. Several companies no longer put their bare qubits in the spotlight, but instead focus logical qubits. Error correction seems to be an essential component of large-scale quantum computing, adding yet another facet in which these devices differ from their classical counterparts. As with many aspects of quantum computing, error correction can be rather confusing. A statement that we often hear is the following (which is incorrect!) . â€œLogical qubits (or: error-corrected qubits) are resilient to errors that occur during a computation. Once we have logical qubits, we can increase the length of our computations indefinitely. â€œ . Whatâ€™s the problem here? Well, not every logical qubit is created equally. We expect to soon see logical qubits that are perhaps 2x more accurate than todayâ€™s bare hardware qubits, and later 10x, and in the future perhaps 1000x. Error correction is a trick toÂ reduceÂ the probability of errors, but it will not eliminate errors completely. In the following decade, we expect to see gradual improvements, hopefully down to error rates of 10-10Â and below. ",
    "url": "/advanced/error_correction/",
    
    "relUrl": "/advanced/error_correction/"
  },"54": {
    "doc": "Error correction",
    "title": "What is error correction?",
    "content": "In quantum error correction, we combine some number (think of hundreds or thousands) ofÂ â€˜physicalâ€™Â hardware qubits into a virtualÂ â€˜logicalâ€™Â qubit. The logical qubits are the information carriers used in an algorithm or application. Error correction methods can detect whenever tiny errors occur in the logical qubit, which can then be â€˜repairedâ€™ with straightforward operations. Under the assumption that the probability of hardware errors is sufficiently low (below a certain error threshold), the overall accuracy improves exponentially when we combine more physical qubits per logical qubit. Hence, we obtain a very favourable trade-off between the number of qubits, and the accuracy of the qubits. Doesnâ€™t measuring a quantum state destroy the information in the qubits? . Indeed, if we naively measure all the physical qubits, then that would destroy the computation. However, quantum error correction uses an ingenious way to measure only whether or not an error occurred. It learns nothing about the actual information content of the qubit. It turns out that this way, the data stored in the logical qubit is not affected.Â  . Why are errors so much of a problem?Â How do errors screw up our computations?Â  . In short: even tiny errors are a problem because we want to perform incredible numbers of quantum operations â€” think of billions or trillions of them.Â  . Letâ€™s make this more concrete. A computer program is essentially a sequence ofÂ â€˜stepsâ€™, each of which a computer knows how to perform. We say that a program or algorithm has aÂ width,Â which is the number of qubits it requires. It also has aÂ depth,Â which is the number of consecutive steps that need to be performed. In early hardware, you may interpret one step as a single quantum gate.Â  . The concept ofÂ  â€˜widthâ€™ is pretty straightforward: if the computer doesnâ€™t have enough memory, it will not be able to run the program. Dealing with â€˜depthsâ€™ is harder. According to the laws of statistics, to run a program of 109Â steps, we need to limit errors to roughly the inverse: say, 10-9Â per step. If the error is larger, it becomes extremely unlikely to find a correct outcome of the computation. These are not hard numbers: a computer with 10-10Â error would be a significant improvement (resulting in much fewer mistakes), and a computer with 10-8Â error might be pushed to also find the correct answer after many tries. However, as the imbalance between depth and error grows, the probability to find a correct outcome is reducedÂ exponentially. We illustrate this in more detail in the box below.Â  . To illustrate, why do we need such small error rates? Letâ€™s look at a very simple model of a computer, which is not unlike what happens inside a quantum computer or a modern (classical) CPU. As above, the computer is supposed to work through a list of instructions. We can consider various specifications of a computer: . Â  . | The available memory, measured in bits (or perhaps megabytes or gigabytes, if you like).Â  | . | The speed at which the computer operates, measured in steps per second.Â  . | The â€œprobability of errorâ€, the chance for each computational step to introduce some mistake. This is given as a number between 0 and 1 (or a percentage between 0 and 100%). Many sources use the word â€˜fidelityâ€™ instead, which can be roughly interpreted as the opposite (fidelity â‰ˆ 1 â€“ probability of error). In this text, we sometimes just say â€œerrorâ€. | . Â  . In this simple model, the time taken to complete the computation equals: â€œdepthâ€ x â€œspeedâ€. You can make a computer program faster by increasing the speed of the computer, or by writing a â€˜betterâ€™ program that takes fewer steps.Â  . The influence of errors is harder to track. For contemporary computers, we typically donâ€™t worry about hardware mistakes at all: every step has essentially 100% certainty to output the correct result. Unfortunately, in our little model, this is not the case.Â  . Assume that each step has a 1% (= 10-2) chance of error. What will the impact on the final computation be? Here, we compute the chance to finish the computation without any errors, for various numbers of total steps:Â  . | Error probability: 1% | Â  | . | Number of steps | P(success)Â Â  | . | 1Â  | ( 0.99 )1Â = 99% | . | 100 | ( 0.99 )100Â = 37% | . | 1000 | ( 0.99 )1000Â = 0.004 % | . | 10,000 | ( 0.99 )10,000Â = Â  10-44Â  | . In this simple model, we assume thatÂ anyÂ error is catastrophic. This is quite accurate for most programs. You might argue that there is a miniscule chance that two errors cancel, or that the error has very little effect on the final result, but it turns out that such effects are statistically irrelevant in large computations.Â  . Now, assume we improve our hardware, towards an error rate of 0.1% (=10-3), then we find: . | Error probability: 0.1% | Â  | . | Number of steps | P(success)Â Â  | . | 1Â  | ( 0.999 )1Â = 99.9% | . | 100 | ( 0.999 )10Â = 90% | . | 1000 | ( 0.999 )1000Â = 37% | . | 10,000 | ( 0.999 )10,000Â = 0.004Â  | . A probability to succeed of 37% sounds bad, but for truly high-end computations we might actually be okay with that. If the program results in a recipe for a brand-new medicine, or if it tells us the perfect design for an aeroplane wing, then surely we donâ€™t mind repeating the computation 10 or 100 times, after which weâ€™re very likely to learn this breakthrough result. On the other hand, if the probability of success is 10-44, then we willÂ neverÂ find the right result, even if the computer repeats the program billions of times.Â  . In the table above, we see a pattern: to reasonably perform 102Â steps, we require errors of roughly 10-2Â or better. To perform 103Â steps, we need roughly a 10-3Â chance of error. These are very rough order-of-magnitude estimates, but they have a very useful conclusion when dealing with very large circuits (or very small errors): if you want to execute 10nÂ steps, youâ€™d better make sure that your error probability is not much bigger than 10-n.Â  . This simplified model assumes that an operation either works correctly, or it fails, but nothing in between. In reality, quantum operations act on continuous parameters, and therefore they have an inherent, scalar-value accuracy. For example: a quantum gate might change a parameter from A to A+0.49, where itâ€™s supposed to do A+0.5. For our discussion, this doesnâ€™t really matter â€” for our qualitative conclusions, it suffices to see a â€œ99% accurateâ€ quantum gate as simply having a 99% chance of succeeding. We also overlook various other technical details, operations carried out in parallel, different types of errors, native gate sets, connectivity, and so forth â€” these make the story much more complicated, but will not change our qualitative conclusions. Why donâ€™t we just make the hardware more stable? To some degree, we can still greatly reduce errors by creating more accurate hardware. However, quantum objects are so incredibly fragile that even getting down to 10-2Â errors requires some of the worldâ€™s most astonishing engineering. We definitely hope to see two-qubit gate errors reduced to 10-3Â and perhaps even 10-4, but achieving targets of 10-9Â seems unlikely with incremental hardware engineering alone. On the other hand, quantum error correction is incredibly effective: the error drops dramatically at the cost of adding a modest number of qubits, which are assumed to be scalable anyway. Thatâ€™s why experts agree that error correction is the right way forward.Â Â  . Do we use error correction in classical computers too? This might be a good moment to appreciate the incredible perfection of classical computer chips: while doing billions of steps per second, running for months in a row, sometimes with thousands of cores at a time, errors in CPUs practically never occur. I was hoping to find hard numbers on this, but companies like Intel and AMD seem to keep this under stringent non-disclosure agreements. However, someÂ researchÂ shows that errors under 10-20Â are easily attained as long as we donâ€™t push processors to their limits (in terms of voltages and clock speeds). Memory (RAM) does often come with error correction for high-performance supercomputers, and some form ofÂ CPU error correctionÂ was sometimes used in older mainframes and (even today) inÂ space probes.Â  . ",
    "url": "/advanced/error_correction/#what-is-error-correction",
    
    "relUrl": "/advanced/error_correction/#what-is-error-correction"
  },"55": {
    "doc": "Error correction",
    "title": "Longer computations need more qubits",
    "content": "As problems become â€˜more complexâ€™, they typically require more from our computers: both in terms of width (number of bits) and depth (number of steps). We could illustrate this as below. You can think of a number â€œNâ€ as the difficulty or the size of the problem: for example, we might consider the problem of â€œfactoring a number that can be written down using at most N bitsâ€).Â  . Keep in mind: weâ€™re talking about the requirements to solve a problem here, so that width indicatesÂ logicalÂ bits. If a computer does not have error correction, then 1 logical bit is simply the same as 1 physical bit â€“ or its quantum equivalent.Â  . For â€˜perfectâ€™ classical computers, the situation is straightforward: if a problem gets bigger, then we need more memory, and we need to wait longer before we obtain the result. For (quantum) computers that make errors, the situation is more complex. With increasing depth, not only do we need to wait longer, we also need to lower the error probabilities, and hence, need more advanced error correction.Â  . Letâ€™s consider two computers for which we show the width and depth that they can handle (where the available â€˜depthâ€™ is essentially 1 / probability of error). On the left is a computer without error correction (hence it has a small, fixed depth). The other is an error-corrected computer that can trade between depth and width (in certain discrete steps).Â  . The computer without error correction might have enough memory to solve a problem, but often lacks the depth. Even an error-corrected computer might not have a suitable trade-off to solve the hardest problems. Looking at the above example, it seems that both computers can solve the N=10 problem. Here, only the error-corrected computer can solve the N=20 problem, as depicted below. For the N=40 problem, the error-corrected computer might have sufficient depth OR sufficient width, but it doesnâ€™t have both at the same time. Hence, neither computer solves the N=40 problem.Â  . Towards cracking the N=40 problem, our best bet is to upgrade the error-corrected computer to haveÂ more physical qubits. Using error-correction, these can then be traded to achieve sufficient depth (whilst also reserving just enoughÂ logical qubitsÂ to run the algorithm).Â  . We have found an interesting conclusion here. Larger problems not only require more memory (to store the calculation), but also more depth, which requires more qubits again! To summarise:Â  . â€˜Harderâ€™ problems -&gt; More depth -&gt; Better error correction -&gt; More physical qubitsÂ  . Effectively, once we reach an era of error correction, then increasing the number of physical qubits will still be among the top of our wishlist.Â  . ",
    "url": "/advanced/error_correction/#longer-computations-need-more-qubits",
    
    "relUrl": "/advanced/error_correction/#longer-computations-need-more-qubits"
  },"56": {
    "doc": "Error correction",
    "title": "What is the current state-of-the-art?",
    "content": "This is a more technical section that can be safely skipped. As of 2024, there have been several demonstrations of error correction (and the slightly less demanding cousin: errorÂ detection), but these have all been with limited numbers of qubits, and with very limited benefit to depth (if any at all). However, we seem to be at a stage where hardware is sufficiently mature that we can start exploring early error correction.Â  . Below are the three most popular approaches to error correction. Each of them can be considered a â€˜familyâ€™ of different methods, based on similar ideas: . | Surface codes . | Color codes . | Low-Density Parity Check (LPDC) codes . | . The surface code (or toric code) has received a lot of scientific attention, as this seems to be on the roadmap of large tech companies like Google and IBM. Their superconducting qubits cannot interact with each other over long distances, and the surface code can deal with this limitation. Many estimates that we use in this Guide (such as the resources required to break RSA or to simulate FeMoco) are based on this code. It has already been tested experimentally on relatively small systems: . | A team from Hefei/Shanghai experiments with a 17-qubit surface code. | Google sees improvements when scaling the surface code from 17 to 49 qubits. | . Color codes are somewhat similar to surface code, but typically lack the property that only neighbouring qubits have to interact. This makes them less interesting for superconducting or spin qubits, but still they appear to work extremely well for trapped ions and ultracold atoms.Â  . | Startup QuEra demonstrates 48 logical qubits using a color codeÂ [Scientific presentation]Â  . | Already in 2014, an early experiment on a single logical qubit (color code) was performed in Innsbruck. | . LDPC codes are now rapidly gaining attention. They build on a large body of classical knowledge, and could have (theoretically) more favourable scaling properties over the surface code.Â  . | French startup Alice &amp; Bob are aiming for a unique combination of â€˜cat qubitsâ€™ together with LDPC codes, which can theoretically match very elegantly.Â  | . Which code will eventually become the standard (if any), is still completely open.Â Â  . What are the main challenges? . Firstly, we would need justÂ slightlyÂ more accurate hardware. We mentioned a certain accuracyÂ thresholdÂ earlier: state-of-the-art hardware seems to be close to this threshold, but not comfortably over it. Secondly, error correction also requires significant classical computing power, which needs to solve a fairly complex â€˜decodingâ€™ problem within extremely small time bounds (within just a few clock cycles of a modern CPU). Classical decoding needs to become more mature, both at the hardware and the software level. It is not unlikely that purpose-built hardware will need to be developed, which for some platforms might be placed inside a cryogenic environment (placing stringent bounds on heat dissipation). Theoretical breakthroughs can still reduce the requirements of classical processing.Â  . Lastly, it turns out that â€˜mid-circuit measurementsâ€™ are technically challenging: most experiments so far relied on destructively measuringÂ allÂ qubits at the end of an experiment. Without intermediate measurements, one might retroactively detect errors, but one cannot repair them. We should also warn that manyÂ related termsÂ such as â€œerror mitigationâ€ and â€œerror suppressionâ€ exist. They might be useful for incremental fidelity improvements, but they donâ€™t facilitate an exponential increase in depth like proper error correction does.Â  . ",
    "url": "/advanced/error_correction/#what-is-the-current-state-of-the-art",
    
    "relUrl": "/advanced/error_correction/#what-is-the-current-state-of-the-art"
  },"57": {
    "doc": "Error correction",
    "title": "Conclusion",
    "content": "The bottom line is that one shouldnâ€™t blindly take â€˜logical qubitsâ€™ as perfect building blocks that will run indefinitely. A logical qubit is no guarantee that a computer has any capabilities, it merely indicates that some kind of error correction is applied (and it doesnâ€™t say anything about whether this error correction works well at all). A much more interesting metric is the probability of error in a single step (in jargon: the fidelity of an operation), which gives a reasonable indication of the number of steps that a device can handle! . ",
    "url": "/advanced/error_correction/#conclusion",
    
    "relUrl": "/advanced/error_correction/#conclusion"
  },"58": {
    "doc": "Error correction",
    "title": "See also:",
    "content": ". | The Quantum Threat Timeline ReportÂ asked several experts what they find the most likely approach to fault-tolerance (section 4.5).Â  . | British startupÂ Riverlane builts a hardware chipÂ that â€˜decodesâ€™ which error occurred on logical qubits. (Technical report).Â  . | Craig Gidney (Google) has aÂ more technical blog postÂ on why adding physical qubits will remain relevant in the following decades.Â  . | [Technical!] SomeÂ scientificÂ work speaks of â€˜early fault-tolerantâ€™ quantum computing, such as: . | â€œEarly Fault-Tolerant Quantum Computingâ€, discussing how we can squeeze as much as possible out of limited devices. | â€œAssessing the Benefits and Risks of Quantum Computersâ€ takes a similar width x depth approach as we do here, but uses it to assess what applications will be within reach first. | . | . ",
    "url": "/advanced/error_correction/#see-also",
    
    "relUrl": "/advanced/error_correction/#see-also"
  },"59": {
    "doc": "Quantum hardware",
    "title": "Quantum hardware",
    "content": "Conventional computer hardware is extremely well standardized. No matter what supplier you buy a computer from, you can be reasonably sure that you can run your favourite applications on them. We expect to have professional servers to run non-stop for years without the hardware ever failing. Thanks to such high reliability and clear compatibility, it is rather easy to compare different machines, by looking at speed (e.g. floating-point operations per second, FLOPS) and memory size. We will see that this is radically different for quantum computers: devices make mistakes, have limited functionalities, and depending on your application, you might like to use a completely different architecture.Â In this chapter, we take a high-level business perspective at quantum computing hardware. What should you know when starting your quantum journey?Â  . ",
    "url": "/advanced/hardware/",
    
    "relUrl": "/advanced/hardware/"
  },"60": {
    "doc": "Quantum hardware",
    "title": "Different functionalities",
    "content": "The figure below shows different types of functionalities that quantum computers can have (top, brown), along with some examples of hardware that is available (below, yellow). This list is by no means complete! It should at best give an impression of the current state of the art. Let us start by taking a closer look at the functionalities. Our biggest dream is to have aÂ â€˜universal quantum computerâ€™. The word â€˜universalâ€™ indicates that it is capable of executing any quantum algorithm (or technically: to approximate any algorithmâ€™s output to arbitrary precision). For comparison: your laptop, phone, and even a smartwatch are universal classical computers, making them capable of running any classical application you can think of: spreadsheets, 3D games, data encryption, and so on. Similarly, a proper universal quantum computer is suitable for any quantum application, regardless of whether it is known today or invented in the future.Â  . The definition of â€˜universalâ€™ is blind to some details such as memory limitations (it assumes you will never run out of RAM), and omits tedious details about software compatibility (a Playstation game wonâ€™t run on an XBox). In our high level overview, such details are unimportant: the main point is that there also exist devices that canÂ notÂ run just any algorithm. Does a universal computer need to be â€œgate-basedâ€? . No matter what architecture or qubit type you pick, our current technology will not allow you to run very long computations. This is due to the inherent imperfections in construction and control of quantum devices. The imperfections cause errors to accumulate during a computation, so that after some number of steps, the result is almost surely corrupted and unusable. For longer computations, it is essential to fix errors on the fly, using so-calledÂ error correctionÂ (also known as fault-tolerance).Â Â  . Today, we are stuck in a so-called NISQ-era, withÂ Noisy Intermediate-Scale QuantumÂ devices. Many are in principle universal, except that they are limited both in the number of qubits,Â andÂ in the number of steps that can be executed. Companies like IBM, IonQ, Quantinuum and Pasqal all have NISQ computers available to test your own programs on.Â  . Making a universal computer is challenging, and engineers can makeÂ special-purpose devicesÂ that improve in certain areas (like number of qubits or clock speed) by omitting certain functionalities. In aÂ Quantum Simulator, the computer specialises in simulating a certain class of materials or molecules. The precise capabilities are often captured in a Hamiltonian that specifies which materials qualify. As an example, Harvard-spinoff QuEra has a quantum simulator available over the cloud that mimics a quantum Ising model. Todayâ€™s simulators (like QuEraâ€™s) are fairly close to a universal computer (lacking only a few technical ingredients) and are similarly subject to accumulating errors. However, they are not designed to run conventional (gate-based) algorithms. There might be some confusion in jargon here, as the term â€˜quantum simulationâ€™ is also sometimes used when a classical computer tries to mimic the behaviour of a quantum computer. Others use the term â€˜emulationâ€™ for such classical approaches.Â  . See also: . | QuEra announces a 256 qubit simulatorÂ available over the Cloud.Â  . | Pasqal performs a material science simulation with 196 qubitsÂ andÂ sells 100-qubit simulator in a EuroHPC tender. | . Another special-purpose device is theÂ Quantum Annealer,Â for which the Canadian scale-up D-Wave is especially well known. These special-purpose devices can solve a certain class of optimization problems that goes by the name ofÂ QUBO: quadratic unconstrained binary optimization. There is a well-developed theory of mapping various problems into the QUBO formalism, making annealers fairly versatile machines. However, quantum annealers will never be able to take advantage of the various other quantum algorithms out there: even with enough qubits, we donâ€™t see them cracking codes with Shorâ€™s algorithm.Â  . See also: . | D-Waveâ€™s introduction to its quantum annealing platform | . ",
    "url": "/advanced/hardware/#different-functionalities",
    
    "relUrl": "/advanced/hardware/#different-functionalities"
  },"61": {
    "doc": "Quantum hardware",
    "title": "Different building blocks",
    "content": "There is another question about what material the qubits are actually made of. Scientists have cooked up several competing approaches, such as superconducting materials, photons, or individual atoms, or ions, each with their own strong and weak spots. The methodology or material used to make a physical qubit is often called the qubit implementation, the qubit type, or (we prefer) qubitÂ platform.Â  . For conventional computer electronics, we converged to a single choice of material and broadly a single manufacturing process, based on silicon wafers and lithography. For quantum computers, there is still a fierce race between the different platforms, and it is totally unclear which will eventually be the winner â€” or whether we will converge to a single winner at all.Â  . There is an interesting story behind the different hardware types, but we wonâ€™t delve into that in this non-technical guide (would you otherwise care what material your classical CPU is made of?). However, anyone who wants to test quantum programs on actual hardware should definitely know the details. Interested readers can start here: . Further reading: . | Different types of qubits explained by Sifted.eu . | Different types of qubits at IQC Waterloo . | Different types of qubits on Wikipedia . | A MOOC about different hardware types by TU Delft . | . It is interesting to note that all these different machines (universal, annealers, simulators) can in principle be built using any type of qubit. If you go back to the figure at the top, you can see that there exist different qubit types with different functionalities. Itâ€™s not unthinkable that the empty squares will also be filled, so that we have access to superconducting-based simulators, or annealers that use ultracold atom qubits (or perhaps the authors have missed this!). In fact, we have already seen many academic showcases of superconducting simulators inÂ academicÂ literature. TODO: ADD OVERVIEW TABLE OF WHAT SUPPLIERS HAVE. ",
    "url": "/advanced/hardware/#different-building-blocks",
    
    "relUrl": "/advanced/hardware/#different-building-blocks"
  },"62": {
    "doc": "What steps should your organization take?",
    "title": "What steps should your organisation take?",
    "content": "TODO: add . https://arxiv.org/abs/2310.15505 The Quantum Tortoise and the Classical Hare: A simple framework for understanding which problems quantum computing will accelerate (and which it will not) . In the previous parts, we discussed theÂ use-cases, theÂ threats,Â and theÂ timelinesÂ of quantum technologies. We will now take a more strategic perspective: what concrete steps can we take today? How does one establish a successful road map? Details will greatly differ per organisation and per sector, but as we are in an early phase, we expect that most enterprises will take surprisingly similar approaches during the next couple of years.Â  . Several organisations (especially consultants that would love to support you) have extensive writings on how to get started. Some examples: . | Capgemini â€“ Quantum technologies: How to prepare your organisation for a quantum advantage now . | McKinsey â€“Â Quantum computing use cases are getting realâ€”what you need to know . | BCG â€“Â Quantum Computing Is Becoming Business ReadyÂ  . | . We find most of these somewhat â€˜hypeyâ€™, with a strong emphasis on the risks of missing out and the need to take actions quickly. However, they all mention reasonable strategic steps that organisation can take, which we will lay out below. Letâ€™s break them up into 3 different stages: . 1. Start with no-regret moves . Most companies start with early steps aimed at better understanding the situation. These can be done with very little financial risk. Some must-do actions: . | Appoint a quantum lead, or a quantum working group . | Read up and learn.Â If youâ€™ve come this far in this Guide, youâ€™re already doing a fantastic job.Â  . | Create internal awareness . | . Optionally: . | Put quantum on the agenda with senior management.Â  . | Involve collaborators, suppliers and vendors, make your interest in quantum known. It is in your benefit if suppliers are well-prepared.Â  . | Participate in a workshop, hackathon, and other events. | . We have a list of interestingÂ learning resources, and an overview of Netherlands-basedÂ education opportunitiesÂ andÂ events.Â  . Very soon, the quantum journey will split into two fairly different categories: . Â  . | Preparing forÂ quantum applications,Â where the goal is to leverage quantum technologies to gain some competitive advantage (for example, by strengthening your R&amp;D, further optimising your logistics, improving a product, etc). For most companies, the main items of interest are theÂ applications of quantum computing.Â  . | Migrating toÂ quantum-safe cryptography,Â where the goal is to keep your IT secure against attackers with a quantum computer. | . It is important to make this distinction, because these categories have different goals and are conventionally addressed by different departments. Let us first look at quantum applications. Prepare to use quantum applications . 2a: Start exploring use-cases . For most businesses, the goal of early, low-regret moves is to be ready to leverage quantum technologies fairly soon after they start offering an advantage.Â  . Must do: . | Find the most impactful use-cases in your area. | Sketch a road map for the coming years.Â  . | . Optional but recommended:Â  . | Start concrete explorations, for example by testing and implementing trial use-cases. It is relatively easy (and fun!) to follow the tutorial of programming packages likeÂ QiskitÂ orÂ Cirq.Â  . | Find a partner. Save costs by collaborating on early, pre-competitive exploration.Â  . | Create PR! We notice that many companies are very actively promoting their early results on quantum applications, even if these do not offer significant advantages yet.Â  . | Build a skilled workforce.Â BCG states: â€œThe biggest challenge may be talent, given the supply constraints. [â€¦] Building in-house talent will take time, so it is best to start as soon as possible. | . A good exercise is to look at your current needs in high-performance computing.Â What do you currently spend your computing budgets on? Are there any areas where new tools in computation or modelling could provide serious business value (for example, by being faster, tackling bigger problems, or delivering higher accuracy)? Which quantities would you ideally have calculated, but are beyond the reach of current computers?Â  . Sometimes you rapidly identify use-cases that are not worth further addressing. For example, if todayâ€™s computational power is easily sufficient to meet your needs, itâ€™s unlikely that you want to invest in quantum computing for that use case.Â  . Â  . **R&amp;D partnerships **At Quantum.Amsterdam, we have strong ties to Amsterdam-based research organisations. Offers you might be interested in include: . | Short-term applied research by theÂ Quantum Application Lab, which could lead to a roadmap or a proof-of-concept implementation.Â  . | We invite collaborators to sponsor a PhD or Postdoc researcher at an academic institute, leading to a 2-4 year intensive investigation of a certain use case. There are oftenÂ subsidiesÂ for such public-private partnerships. Our website shows severalÂ examples of the past collaborations, and information about starting a new project can be found at the localÂ tech transfer office.Â  . | . Also see: . | Quantum.Amsterdam meetupÂ Â Â (Youtube) | . 3a: Implementing actual applications, whenever ready . From here onwards, we find it hard to give concrete advice: goals may depend on your area of business, and on the way that the field of quantum will progress. Other sources will simply tell you do â€œdevelop a long-term strategyâ€ or similar.Â  . For inspiration, or a dot on the horizon, you may think towards a competence center for quantum computing, similar to how many companies have special departments for data science and/or AI. It may also be reasonable to combine these departments.Â  . A common advice that we will relay here: the business use of quantum computers is still very uncertain, so at best we can recommend to remain agile! . Migrating to post-quantum cryptography . 2b. Prepare for migration to post-quantum cryptography.Â  . Cryptography is a completely different beast, with a more concrete goal, and more urgent timelines for most organisations (hereâ€™s why). Luckily, pretty much everyone faces the same problem, making it easier to give concrete guidelines. In essence, anyone who uses modern cryptography should migrate to Post-Quantum Cryptography (PQC) in the next decade. Although standards and regulations are still under construction, there are some urgent steps should be taken well before the migration can start.Â  . We recommend theÂ PQC Migration HandbookÂ (April 2023), written by the Dutch secret service AIVD, and research organisations CWI and TNO.Â  . In general, as a first step, it recommends to: . | Determine the risk and urgency of PQC migration. This likely requires making an inventory of all cryptography in use, and determining how long your data should remain private.Â  . | Create a migration plan.Â  . | . â€œCertain organisations should already start working on mitigating measures now. [..] For instance, organisations handling data that will still be confidential 20 years from now, or organisations developing long-lived systems that will still be in use decades from now.â€ . THE PQC MIGRATION HANDBOOK . Judging from previous migrations, [implementing PQC] might take well over five years. THE PQC MIGRATION HANDBOOK . 3b. Migrate . This is a much more technical part, for which you will need a well prepared migration plan. We find it likely that quantum computers might break todayâ€™s RSA encryption standardÂ somewhere in the 2030â€™s, so we stress that such migration should be completed well before that time â€” taking in to account that IT migrations can take significantly longer than originally planned! . ",
    "url": "/advanced/strategic-actions/#what-steps-should-your-organisation-take",
    
    "relUrl": "/advanced/strategic-actions/#what-steps-should-your-organisation-take"
  },"63": {
    "doc": "What steps should your organization take?",
    "title": "FAQ",
    "content": "Should I have knowledge of quantum mechanics? Should I understand quantum hardware?Â  . When you are tasked with implementing early-stage quantum applications: then yes! . As a strategic manager, or when working on post-quantum cryptography: no.Â  . Do I really need to work on NISQ? . Thatâ€™s a good question that is hard to answer. We find it unlikely that near-term quantum computers (in the next, say, 1-4 years) will create relevant advantages, although there are plenty of experts who are more optimistic. Future applications will likely differ strongly from the way we â€˜programâ€™ quantum computers today, but practising on todayâ€™s hardware can grow your understanding and give an edge in experience.Â  . What should I do first: focus on quantum applications or post-quantum cryptography? . For most organisations, the threat to security is more urgent, especially because the migration trajectory can take several years (and this must be completed well before we have large-scale quantum computers!).Â  . However, most organisations work on both trajectories in parallel. Iâ€™m looking for a collaboration â€“ what should I do? . Typically, business developers meet each other atÂ conferences and events, where you may find like-minded partners. We also encourage you toÂ contact usÂ for some sparring on what your organisation can do, and to get connected to Netherlands-based knowledge institutes likeÂ QuSoftÂ andÂ Quantum Application Lab.Â  . ",
    "url": "/advanced/strategic-actions/#faq",
    
    "relUrl": "/advanced/strategic-actions/#faq"
  },"64": {
    "doc": "What steps should your organization take?",
    "title": "What steps should your organization take?",
    "content": " ",
    "url": "/advanced/strategic-actions/",
    
    "relUrl": "/advanced/strategic-actions/"
  },"65": {
    "doc": "(none)",
    "title": "How to read hyped reports",
    "content": "TODO still write this article? . ",
    "url": "/advanced/tbd1/#how-to-read-hyped-reports",
    
    "relUrl": "/advanced/tbd1/#how-to-read-hyped-reports"
  },"66": {
    "doc": "(none)",
    "title": "(none)",
    "content": " ",
    "url": "/advanced/tbd1/",
    
    "relUrl": "/advanced/tbd1/"
  },"67": {
    "doc": "Further reading",
    "title": "Further reading",
    "content": "Below, we give a selection of recommended sources to learn more about this fascinating topic. ",
    "url": "/resources/further-reading/",
    
    "relUrl": "/resources/further-reading/"
  },"68": {
    "doc": "Further reading",
    "title": "I want to learn the technical details",
    "content": "For (late) high school students: . (or those who followed high-school level mathematics): . | Quantum QuestÂ [Book/website] is an intensive 5-week online course about the theoryÂ (mathematics) of quantum computing. Materials are freely available for self-study.Â  . | Quantum in Pictures (Cooke) [Book] teaches the theory (mathematics) of quantum computing using diagrams. | . Undergraduate (Bachelorâ€™s) university level: . | Quantum.CountryÂ [Website] â€“ the â€œDuolingo of Quantum Computingâ€, a very well-written introduction for those with late high-school or early university level math background.Â  . | Quantum Computation and Quantum InformationÂ (Nielsen, Chuang) [Book] â€“ the â€œbible of quantum computingâ€. Perhaps not the most up-to-date, but definitely the most well-known resource in our field. Sets the standards for jargon and notation.Â  . | . | Quantum Computer Science: An IntroductionÂ (Mermin)Â [Book] â€“ a well-written introduction, with quite some focus on manipulating quantum circuits. | Quantum Computing Since DemocritusÂ (Aaronson) [Book] â€“ Aaronson is an authority in the field. His book touched upon many topics such as the foundations of computer science, black holes and consciousness, making it a good read for those looking for something much more broad than just quantum computing. | . Graduate (Masterâ€™s) level: . These assume no prior knowledge about quantum physics, but require a strong background in mathematics (i.e. linear algebra, calculus, advanced inequality bounds and approximations, etc.). In exchange, they go into much more detail.Â  . | Lecture Notes for UvA course â€œQuantum Computingâ€ by Ronald de Wolf,Â which is frequently updated and features some cutting-edge algorithms. Via theÂ course website, you can find the link and password to view all the recorded lectures too.Â  | . | Lecture Notes for Caltech course â€œQuantum Computingâ€ by John PreskilÂ  | . Scientific overview papers . The papers below are aimed at scientists from fields other than quantum computing itself. All papers we mention are open-access and peer-reviewed, making them very suitable for citation.Â  . | Quantum algorithms: an overviewÂ (Ashley Montanaro) . | The Potential Impact of Quantum Computers on SocietyÂ (Ronald de Wolf) [video lecture] . | . Scientific opinions and discussions . | Scott Aaronsonâ€™s blog.Â Although written from a theoretical computer science perspective, this blog addresses a very broad range of quantum computing topics. Prof. Aaronson has a strong authority in the field, and his posts attract readership and comments from a broad range of prominent scientists.Â  | . ",
    "url": "/resources/further-reading/#i-want-to-learn-the-technical-details",
    
    "relUrl": "/resources/further-reading/#i-want-to-learn-the-technical-details"
  },"69": {
    "doc": "Further reading",
    "title": "I want to learn to program a quantum computer",
    "content": "Several programming packages for quantum computers exist, mostly maintained by major hardware providers. All of them offer great introductory tutorials. The ones we recommend below are all in Python.Â  . | Qiskit, the language by IBM, probably features the largest catalogue of learning materials. To start from scratch, we recommend following the â€œBasics of Quantum Informationâ€, which teaches both the mathematics behind qubits and the usage of the package itself. | Cirq is a very similar package developed by Google. As of 2024, they have a more focused tutorial to explain the programming package itself, without extensive theory of quantum mechanics. | QWorld BronzeÂ offers tutorials in the form of Jupyter notebooks and hosts various training days around the world, mostly focused on Qiskit and sometimes ProjectQ. | PennyLane is a package by startup Xanadu with a strong focus on machine learning applications. | . ",
    "url": "/resources/further-reading/#i-want-to-learn-to-program-a-quantum-computer",
    
    "relUrl": "/resources/further-reading/#i-want-to-learn-to-program-a-quantum-computer"
  },"70": {
    "doc": "Further reading",
    "title": "I want to stay up-to-date with the latest developments",
    "content": "Major conferences . | Q2B (organized by QCWare) . | IQT (Inside Quantum Technology) . | Quantum.Tech (organized by Alpha Events) . | Commercializing Quantum (organized by The Economist) . | . Business News . | Quantum Computing Report - donâ€™t be fooled by the basic look on the website. The content is written with a very critical eye and with very relevant contextual information, making them our favourite source for quantum-related news. Â  | . | The Quantum Insider | . Scientific news . None of these focus exclusively on Quantum Technology, but all offer high-quality news (and surely none would miss any important quantum breakthroughs).Â  . | Quanta Magazine . | Phys.org . | . ",
    "url": "/resources/further-reading/#i-want-to-stay-up-to-date-with-the-latest-developments",
    
    "relUrl": "/resources/further-reading/#i-want-to-stay-up-to-date-with-the-latest-developments"
  },"71": {
    "doc": "Further reading",
    "title": "I want to learn more about business implications",
    "content": "Several sources cover similar topics as this book. Most of these come from consultants of hardware providers who have a financial interest in making others get started with quantum. In our opinion, the articles are sometimes too optimistic and predict that quantum applications will come much sooner than the typical expert would anticipate. On the other hand, they collect insightful details about financial matters.Â  . | McKinsey publishes yearly â€œQuantum Technology Monitorâ€ reports, focusing on the economic impact the quantum computers will have. Here is the 2024 edition. | Cloudlfareâ€™s support pages contain an incredibly complete bible of Post-Quantum Cryptography. | . Workshops and trainings . These might not be the most relevant if you completed this book, by can be particularly useful to inspire your colleagues. | The Workshop General Awareness Quantum Computing follows the same philosophy as this book: an introduction to business opportunities that should be understandable for everyone. | Qureca is a british startup that offers several trainings, such as â€œQuantum for everyoneâ€ and â€œQuantum Training for Businessâ€. | . ",
    "url": "/resources/further-reading/#i-want-to-learn-more-about-business-implications",
    
    "relUrl": "/resources/further-reading/#i-want-to-learn-more-about-business-implications"
  },"72": {
    "doc": "Quantum hype bingo",
    "title": "Quantum Hype Bingo",
    "content": "| Unprecedented capabilities | â€œOur algorithm solves â€¦â€ (without comparison to classical computers) | Solve (NP-)hard optimization problems | . | Harness the commercial potential | Game-changing | Transformative | . | Unhackable | Solve climate change | â€œX times fasterâ€ (without fair benchmark) | . ",
    "url": "/resources/hype-bingo/#quantum-hype-bingo",
    
    "relUrl": "/resources/hype-bingo/#quantum-hype-bingo"
  },"73": {
    "doc": "Quantum hype bingo",
    "title": "Quantum hype bingo",
    "content": " ",
    "url": "/resources/hype-bingo/",
    
    "relUrl": "/resources/hype-bingo/"
  }
}
